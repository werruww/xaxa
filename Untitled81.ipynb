{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMWTfJ3uVk-k"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/blt\n",
        "%cd blt\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8A7eshIkV2Jq",
        "outputId": "9949e859-2c61-4783-be12-3b63c6ca5f3b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'blt'...\n",
            "remote: Enumerating objects: 1133, done.\u001b[K\n",
            "remote: Counting objects: 100% (305/305), done.\u001b[K\n",
            "remote: Compressing objects: 100% (90/90), done.\u001b[K\n",
            "remote: Total 1133 (delta 245), reused 215 (delta 215), pack-reused 828 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1133/1133), 611.73 KiB | 9.13 MiB/s, done.\n",
            "Resolving deltas: 100% (737/737), done.\n",
            "/content/blt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bash setup/create_env.sh\n",
        "# or if you have access to a SLURM cluster\n",
        "sbatch setup/create_env.sh"
      ],
      "metadata": {
        "id": "tLZ3fv_1V5Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAsWLOiFWJ9A",
        "outputId": "fca2968b-d288-4766-c038-791ca78b3e06"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Install specific packages (PyTorch, xFormers, Ninja)\n",
        "# Note: Check if cu121 matches the CUDA version shown by nvidia-smi. Colab T4 usually uses CUDA 12.x, so cu121 is likely correct.\n",
        "!pip install torch==2.5.0 xformers --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install ninja"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKMOCtgBWUZv",
        "outputId": "042d5f2e-2065-43ee-e62b-e0a952b563ae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.5.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.0%2Bcu121-cp311-cp311-linux_x86_64.whl (780.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.5/780.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xformers\n",
            "  Downloading https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.5.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.5.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.5.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m118.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.5.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.5.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.5.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.5.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.5.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0) (2.21.5)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.5.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.1.0 (from torch==2.5.0)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.5.0) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xformers) (2.0.2)\n",
            "INFO: pip is looking at multiple versions of xformers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting xformers\n",
            "  Downloading https://download.pytorch.org/whl/cu121/xformers-0.0.29-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/xformers-0.0.28.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/xformers-0.0.28.post2-cp311-cp311-manylinux_2_28_x86_64.whl (16.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.0) (3.0.2)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, xformers\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.5.0+cu121 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.5.0+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nvtx-cu12-12.1.105 torch-2.5.0+cu121 triton-3.1.0 xformers-0.0.28.post2\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.11.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IBmIVNHWvxI",
        "outputId": "bc137a9d-088b-406d-9df1-2f3c4877cea1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: read).\n",
            "The token `read` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `read`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4ozoQ7MNXYZY",
        "outputId": "3d4e3598-7f10-41da-daab-af45f0697dd4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.0.2)\n",
            "Collecting omegaconf (from -r requirements.txt (line 2))\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting msgspec (from -r requirements.txt (line 3))\n",
            "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting rouge-score (from -r requirements.txt (line 4))\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacrebleu (from -r requirements.txt (line 5))\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (0.2.0)\n",
            "Collecting tiktoken (from -r requirements.txt (line 7))\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (2025.3.2)\n",
            "Collecting blobfile (from -r requirements.txt (line 9))\n",
            "  Downloading blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (0.19.9)\n",
            "Collecting viztracer (from -r requirements.txt (line 11))\n",
            "  Downloading viztracer-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (26 kB)\n",
            "Collecting lm-eval (from -r requirements.txt (line 12))\n",
            "  Downloading lm_eval-0.4.8-py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (1.14.1)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (12.0.0)\n",
            "Collecting datatrove (from -r requirements.txt (line 15))\n",
            "  Downloading datatrove-0.4.0-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (3.10.16)\n",
            "Collecting luigi (from -r requirements.txt (line 17))\n",
            "  Downloading luigi-3.6.0.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (2.11.3)\n",
            "Requirement already satisfied: altair in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (5.5.0)\n",
            "Collecting submitit (from -r requirements.txt (line 20))\n",
            "  Downloading submitit-1.5.2-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 21)) (0.15.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 22)) (13.9.4)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->-r requirements.txt (line 2))\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from omegaconf->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score->-r requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score->-r requirements.txt (line 4)) (3.9.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score->-r requirements.txt (line 4)) (1.17.0)\n",
            "Collecting portalocker (from sacrebleu->-r requirements.txt (line 5))\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu->-r requirements.txt (line 5)) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu->-r requirements.txt (line 5)) (0.9.0)\n",
            "Collecting colorama (from sacrebleu->-r requirements.txt (line 5))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu->-r requirements.txt (line 5)) (5.3.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->-r requirements.txt (line 7)) (2.32.3)\n",
            "Collecting pycryptodomex>=3.8 (from blobfile->-r requirements.txt (line 9))\n",
            "  Downloading pycryptodomex-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.11/dist-packages (from blobfile->-r requirements.txt (line 9)) (2.3.0)\n",
            "Requirement already satisfied: filelock>=3.0 in /usr/local/lib/python3.11/dist-packages (from blobfile->-r requirements.txt (line 9)) (3.18.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (2.26.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 10)) (4.13.2)\n",
            "Collecting objprint>=0.3.0 (from viztracer->-r requirements.txt (line 11))\n",
            "  Downloading objprint-0.3.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from lm-eval->-r requirements.txt (line 12)) (1.5.2)\n",
            "Collecting evaluate (from lm-eval->-r requirements.txt (line 12))\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting datasets>=2.16.0 (from lm-eval->-r requirements.txt (line 12))\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting jsonlines (from lm-eval->-r requirements.txt (line 12))\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from lm-eval->-r requirements.txt (line 12)) (2.10.2)\n",
            "Requirement already satisfied: peft>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from lm-eval->-r requirements.txt (line 12)) (0.14.0)\n",
            "Collecting pybind11>=2.6.2 (from lm-eval->-r requirements.txt (line 12))\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting pytablewriter (from lm-eval->-r requirements.txt (line 12))\n",
            "  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from lm-eval->-r requirements.txt (line 12)) (1.6.1)\n",
            "Collecting sqlitedict (from lm-eval->-r requirements.txt (line 12))\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from lm-eval->-r requirements.txt (line 12)) (2.5.0+cu121)\n",
            "Collecting tqdm-multiprocess (from lm-eval->-r requirements.txt (line 12))\n",
            "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: transformers>=4.1 in /usr/local/lib/python3.11/dist-packages (from lm-eval->-r requirements.txt (line 12)) (4.51.3)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.11/dist-packages (from lm-eval->-r requirements.txt (line 12)) (0.23.0)\n",
            "Collecting dill (from lm-eval->-r requirements.txt (line 12))\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting word2number (from lm-eval->-r requirements.txt (line 12))\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from lm-eval->-r requirements.txt (line 12)) (10.6.0)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml->-r requirements.txt (line 14)) (12.570.86)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from datatrove->-r requirements.txt (line 15)) (0.30.2)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from datatrove->-r requirements.txt (line 15)) (4.12.2)\n",
            "Collecting loguru>=0.7.0 (from datatrove->-r requirements.txt (line 15))\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting multiprocess (from datatrove->-r requirements.txt (line 15))\n",
            "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from datatrove->-r requirements.txt (line 15)) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.5 in /usr/local/lib/python3.11/dist-packages (from luigi->-r requirements.txt (line 17)) (2.8.2)\n",
            "Collecting tenacity<9,>=8 (from luigi->-r requirements.txt (line 17))\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: tornado<7,>=5.0 in /usr/local/lib/python3.11/dist-packages (from luigi->-r requirements.txt (line 17)) (6.4.2)\n",
            "Collecting python-daemon (from luigi->-r requirements.txt (line 17))\n",
            "  Downloading python_daemon-3.1.2-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r requirements.txt (line 18)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r requirements.txt (line 18)) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r requirements.txt (line 18)) (0.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair->-r requirements.txt (line 19)) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair->-r requirements.txt (line 19)) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair->-r requirements.txt (line 19)) (1.35.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from altair->-r requirements.txt (line 19)) (24.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from submitit->-r requirements.txt (line 20)) (3.1.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer->-r requirements.txt (line 21)) (1.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->-r requirements.txt (line 22)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->-r requirements.txt (line 22)) (2.18.0)\n",
            "Collecting adlfs (from fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading adlfs-2024.12.0-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[full]->-r requirements.txt (line 23)) (3.11.15)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.11/dist-packages (from fsspec[full]->-r requirements.txt (line 23)) (2024.12.1)\n",
            "Requirement already satisfied: distributed in /usr/local/lib/python3.11/dist-packages (from fsspec[full]->-r requirements.txt (line 23)) (2024.12.1)\n",
            "Collecting dropbox (from fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading dropbox-12.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting dropboxdrivefs (from fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading dropboxdrivefs-1.4.1.tar.gz (7.4 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fusepy (from fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading fusepy-3.0.1.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.11/dist-packages (from fsspec[full]->-r requirements.txt (line 23)) (2025.3.2)\n",
            "Collecting libarchive-c (from fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading libarchive_c-5.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting ocifs (from fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading ocifs-1.3.2-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: panel in /usr/local/lib/python3.11/dist-packages (from fsspec[full]->-r requirements.txt (line 23)) (1.6.2)\n",
            "Collecting paramiko (from fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading paramiko-3.5.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: pyarrow>=1 in /usr/local/lib/python3.11/dist-packages (from fsspec[full]->-r requirements.txt (line 23)) (18.1.0)\n",
            "Requirement already satisfied: pygit2 in /usr/local/lib/python3.11/dist-packages (from fsspec[full]->-r requirements.txt (line 23)) (1.17.0)\n",
            "Collecting s3fs (from fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading s3fs-2025.3.2-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting smbprotocol (from fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading smbprotocol-1.15.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm-eval->-r requirements.txt (line 12)) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[full]->-r requirements.txt (line 23)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[full]->-r requirements.txt (line 23)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[full]->-r requirements.txt (line 23)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[full]->-r requirements.txt (line 23)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[full]->-r requirements.txt (line 23)) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[full]->-r requirements.txt (line 23)) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[full]->-r requirements.txt (line 23)) (1.19.0)\n",
            "Collecting dill (from lm-eval->-r requirements.txt (line 12))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm-eval->-r requirements.txt (line 12)) (2.2.2)\n",
            "Collecting xxhash (from datasets>=2.16.0->lm-eval->-r requirements.txt (line 12))\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datatrove->-r requirements.txt (line 15))\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "INFO: pip is looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting datasets>=2.16.0 (from lm-eval->-r requirements.txt (line 12))\n",
            "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-3.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-3.3.1-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-3.3.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "INFO: pip is still looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting pyarrow-hotfix (from datasets>=2.16.0->lm-eval->-r requirements.txt (line 12))\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting datasets>=2.16.0 (from lm-eval->-r requirements.txt (line 12))\n",
            "  Downloading datasets-2.19.2-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-2.17.1-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-2.17.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting dill (from lm-eval->-r requirements.txt (line 12))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting datasets>=2.16.0 (from lm-eval->-r requirements.txt (line 12))\n",
            "  Downloading datasets-2.16.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting fsspec[full] (from -r requirements.txt (line 23))\n",
            "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading fsspec-2025.2.0-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 10)) (4.0.12)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair->-r requirements.txt (line 19)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair->-r requirements.txt (line 19)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair->-r requirements.txt (line 19)) (0.24.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->-r requirements.txt (line 22)) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 7)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 7)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 7)) (2025.1.31)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm-eval->-r requirements.txt (line 12)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm-eval->-r requirements.txt (line 12)) (3.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 12)) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 12)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 12)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 12)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 12)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 12)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 12)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 12)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 12)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 12)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 12)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 12)) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 12)) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm-eval->-r requirements.txt (line 12)) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8->lm-eval->-r requirements.txt (line 12)) (12.5.82)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8->lm-eval->-r requirements.txt (line 12)) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.1->lm-eval->-r requirements.txt (line 12)) (0.21.1)\n",
            "Collecting azure-core<2.0.0,>=1.28.0 (from adlfs->fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading azure_core-1.33.0-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-datalake-store<0.1,>=0.0.53 (from adlfs->fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading azure_datalake_store-0.0.53-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Collecting azure-identity (from adlfs->fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading azure_identity-1.21.0-py3-none-any.whl.metadata (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-storage-blob>=12.17.0 (from adlfs->fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading azure_storage_blob-12.25.1-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask->fsspec[full]->-r requirements.txt (line 23)) (1.4.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask->fsspec[full]->-r requirements.txt (line 23)) (0.12.1)\n",
            "Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask->fsspec[full]->-r requirements.txt (line 23)) (8.6.1)\n",
            "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from distributed->fsspec[full]->-r requirements.txt (line 23)) (1.0.0)\n",
            "Requirement already satisfied: msgpack>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from distributed->fsspec[full]->-r requirements.txt (line 23)) (1.1.0)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.11/dist-packages (from distributed->fsspec[full]->-r requirements.txt (line 23)) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from distributed->fsspec[full]->-r requirements.txt (line 23)) (3.1.0)\n",
            "Requirement already satisfied: zict>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from distributed->fsspec[full]->-r requirements.txt (line 23)) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair->-r requirements.txt (line 19)) (3.0.2)\n",
            "Collecting stone<3.3.3,>=2 (from dropbox->fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading stone-3.3.1-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs->fsspec[full]->-r requirements.txt (line 23)) (4.4.2)\n",
            "INFO: pip is looking at multiple versions of gcsfs to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting gcsfs (from fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading gcsfs-2025.3.1-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading gcsfs-2025.3.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading gcsfs-2025.2.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading gcsfs-2024.12.0-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs->fsspec[full]->-r requirements.txt (line 23)) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (from gcsfs->fsspec[full]->-r requirements.txt (line 23)) (1.2.1)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (from gcsfs->fsspec[full]->-r requirements.txt (line 23)) (2.19.0)\n",
            "Collecting oci>=2.43.1 (from ocifs->fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading oci-2.150.2-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from panel->fsspec[full]->-r requirements.txt (line 23)) (6.2.0)\n",
            "Requirement already satisfied: bokeh<3.8.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from panel->fsspec[full]->-r requirements.txt (line 23)) (3.6.3)\n",
            "Requirement already satisfied: linkify-it-py in /usr/local/lib/python3.11/dist-packages (from panel->fsspec[full]->-r requirements.txt (line 23)) (2.0.3)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.11/dist-packages (from panel->fsspec[full]->-r requirements.txt (line 23)) (3.8)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.11/dist-packages (from panel->fsspec[full]->-r requirements.txt (line 23)) (0.4.2)\n",
            "Requirement already satisfied: param<3.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from panel->fsspec[full]->-r requirements.txt (line 23)) (2.2.0)\n",
            "Requirement already satisfied: pyviz-comms>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from panel->fsspec[full]->-r requirements.txt (line 23)) (3.0.4)\n",
            "Collecting bcrypt>=3.2 (from paramiko->fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.11/dist-packages (from paramiko->fsspec[full]->-r requirements.txt (line 23)) (43.0.3)\n",
            "Collecting pynacl>=1.5 (from paramiko->fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: cffi>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from pygit2->fsspec[full]->-r requirements.txt (line 23)) (1.17.1)\n",
            "Collecting DataProperty<2,>=1.1.0 (from pytablewriter->lm-eval->-r requirements.txt (line 12))\n",
            "  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm-eval->-r requirements.txt (line 12))\n",
            "  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm-eval->-r requirements.txt (line 12))\n",
            "  Downloading pathvalidate-3.2.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm-eval->-r requirements.txt (line 12))\n",
            "  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm-eval->-r requirements.txt (line 12))\n",
            "  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval->-r requirements.txt (line 12))\n",
            "  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting lockfile>=0.10 (from python-daemon->luigi->-r requirements.txt (line 17))\n",
            "  Downloading lockfile-0.12.2-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs->fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading aiobotocore-2.21.1-py3-none-any.whl.metadata (24 kB)\n",
            "INFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting s3fs (from fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading s3fs-2025.3.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading s3fs-2025.3.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading s3fs-2025.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading s3fs-2024.12.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting pyspnego (from smbprotocol->fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading pyspnego-0.11.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs->fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting botocore<1.37.2,>=1.37.0 (from aiobotocore<3.0.0,>=2.5.4->s3fs->fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading botocore-1.37.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs->fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->fsspec[full]->-r requirements.txt (line 23)) (1.17.2)\n",
            "Collecting msal<2,>=1.16.0 (from azure-datalake-store<0.1,>=0.0.53->adlfs->fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading msal-1.32.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting isodate>=0.6.1 (from azure-storage-blob>=12.17.0->adlfs->fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.11/dist-packages (from bokeh<3.8.0,>=3.5.0->panel->fsspec[full]->-r requirements.txt (line 23)) (1.3.2)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from bokeh<3.8.0,>=3.5.0->panel->fsspec[full]->-r requirements.txt (line 23)) (11.1.0)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.11/dist-packages (from bokeh<3.8.0,>=3.5.0->panel->fsspec[full]->-r requirements.txt (line 23)) (2025.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.17.0->pygit2->fsspec[full]->-r requirements.txt (line 23)) (2.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 10)) (5.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs->fsspec[full]->-r requirements.txt (line 23)) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs->fsspec[full]->-r requirements.txt (line 23)) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs->fsspec[full]->-r requirements.txt (line 23)) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.13.0->dask->fsspec[full]->-r requirements.txt (line 23)) (3.21.0)\n",
            "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.11/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm-eval->-r requirements.txt (line 12)) (5.2.0)\n",
            "Requirement already satisfied: pyOpenSSL<25.0.0,>=17.5.0 in /usr/local/lib/python3.11/dist-packages (from oci>=2.43.1->ocifs->fsspec[full]->-r requirements.txt (line 23)) (24.2.1)\n",
            "Requirement already satisfied: pytz>=2016.10 in /usr/local/lib/python3.11/dist-packages (from oci>=2.43.1->ocifs->fsspec[full]->-r requirements.txt (line 23)) (2025.2)\n",
            "Collecting circuitbreaker<3.0.0,>=1.3.1 (from oci>=2.43.1->ocifs->fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading circuitbreaker-2.1.3-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->lm-eval->-r requirements.txt (line 12)) (2025.2)\n",
            "Requirement already satisfied: ply>=3.4 in /usr/local/lib/python3.11/dist-packages (from stone<3.3.3,>=2->dropbox->fsspec[full]->-r requirements.txt (line 23)) (3.11)\n",
            "Collecting msal-extensions>=1.2.0 (from azure-identity->adlfs->fsspec[full]->-r requirements.txt (line 23))\n",
            "  Downloading msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->panel->fsspec[full]->-r requirements.txt (line 23)) (0.5.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib->gcsfs->fsspec[full]->-r requirements.txt (line 23)) (2.0.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs->fsspec[full]->-r requirements.txt (line 23)) (2.24.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs->fsspec[full]->-r requirements.txt (line 23)) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs->fsspec[full]->-r requirements.txt (line 23)) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs->fsspec[full]->-r requirements.txt (line 23)) (1.7.1)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.11/dist-packages (from linkify-it-py->panel->fsspec[full]->-r requirements.txt (line 23)) (1.0.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs->fsspec[full]->-r requirements.txt (line 23)) (1.70.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs->fsspec[full]->-r requirements.txt (line 23)) (1.26.1)\n",
            "Requirement already satisfied: PyJWT<3,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from PyJWT[crypto]<3,>=1.0.0->msal<2,>=1.16.0->azure-datalake-store<0.1,>=0.0.53->adlfs->fsspec[full]->-r requirements.txt (line 23)) (2.10.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs->fsspec[full]->-r requirements.txt (line 23)) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs->fsspec[full]->-r requirements.txt (line 23)) (3.2.2)\n",
            "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blobfile-3.0.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading viztracer-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lm_eval-0.4.8-py3-none-any.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datatrove-0.4.0-py3-none-any.whl (17.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.2/17.2 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading submitit-1.5.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.9/74.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading objprint-0.3.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodomex-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading adlfs-2024.12.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading dropbox-12.0.2-py3-none-any.whl (572 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.1/572.1 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gcsfs-2024.12.0-py2.py3-none-any.whl (35 kB)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading libarchive_c-5.2-py3-none-any.whl (15 kB)\n",
            "Downloading ocifs-1.3.2-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading paramiko-3.5.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Downloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_daemon-3.1.2-py3-none-any.whl (30 kB)\n",
            "Downloading s3fs-2024.12.0-py3-none-any.whl (30 kB)\n",
            "Downloading smbprotocol-1.15.0-py3-none-any.whl (126 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
            "Downloading aiobotocore-2.21.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_core-1.33.0-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.1/207.1 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_datalake_store-0.0.53-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_storage_blob-12.25.1-py3-none-any.whl (406 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.0/407.0 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\n",
            "Downloading lockfile-0.12.2-py2.py3-none-any.whl (13 kB)\n",
            "Downloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\n",
            "Downloading oci-2.150.2-py3-none-any.whl (29.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.8/29.8 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathvalidate-3.2.3-py3-none-any.whl (24 kB)\n",
            "Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stone-3.3.1-py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.3/162.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tabledata-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\n",
            "Downloading typepy-1.3.4-py3-none-any.whl (31 kB)\n",
            "Downloading azure_identity-1.21.0-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.2/189.2 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyspnego-0.11.2-py3-none-any.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
            "Downloading botocore-1.37.1-py3-none-any.whl (13.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading circuitbreaker-2.1.3-py3-none-any.whl (7.7 kB)\n",
            "Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading msal-1.32.0-py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.7/114.7 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
            "Building wheels for collected packages: antlr4-python3-runtime, rouge-score, luigi, dropboxdrivefs, fusepy, sqlitedict, word2number\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=697ac058954caf5b249a6f7afc113d1f734be3ca5f3c47cbc21f98911706cc55\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=7e3c7ecce3b1ebe60da3aaa2681623740fdb04bb50018930f434310ac17b8f76\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "  Building wheel for luigi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for luigi: filename=luigi-3.6.0-py3-none-any.whl size=1093756 sha256=0c348bdc2e897d4f76ea53fc43aa06c6fa7cd88a0c0498d463eeeeed9f65c01b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/22/3c/af6674483adf7af53c451451c24b1e330e00b6fe1cc70bb96a\n",
            "  Building wheel for dropboxdrivefs (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dropboxdrivefs: filename=dropboxdrivefs-1.4.1-py3-none-any.whl size=8240 sha256=185b62b2f353133accd5d9d10af21be1f783bb7e3c05bbbadec0f5453ecf24a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/fd/ae/739b6fd4bbedc3c90dd265e6c4719ccd861cd7519fc49c1f06\n",
            "  Building wheel for fusepy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fusepy: filename=fusepy-3.0.1-py3-none-any.whl size=10486 sha256=c06f74d1125a353022dfff15cb74f2fc7e2c867527696447afda9d23bed9d2e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/4a/86/fdda91f8b8ebb0a70e4181dc2423b1f70c3c2d3bd1158685b5\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16862 sha256=7541e5469612b6b9d3053adeed574c191d568e052aa46e62618da1411397f7ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/63/89/7210274f9b7fb033b8f22671f64c0e0b55083d30c3c046a3ff\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=a0428a08684d0733df205b5128fcb92a42a39b81eb97e217960084ec94dd9e74\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/ef/ae/073b491b14d25e2efafcffca9e16b2ee6d114ec5c643ba4f06\n",
            "Successfully built antlr4-python3-runtime rouge-score luigi dropboxdrivefs fusepy sqlitedict word2number\n",
            "Installing collected packages: word2number, sqlitedict, lockfile, libarchive-c, fusepy, circuitbreaker, antlr4-python3-runtime, xxhash, tenacity, tcolorpy, submitit, stone, python-daemon, pycryptodomex, pybind11, portalocker, pathvalidate, omegaconf, objprint, msgspec, mbstrdecoder, loguru, jsonlines, jmespath, isodate, fsspec, dill, colorama, bcrypt, aioitertools, viztracer, typepy, tqdm-multiprocess, tiktoken, sacrebleu, rouge-score, pynacl, multiprocess, luigi, dropbox, botocore, blobfile, azure-core, pyspnego, paramiko, dropboxdrivefs, datatrove, azure-storage-blob, aiobotocore, smbprotocol, s3fs, oci, msal, datasets, DataProperty, tabledata, ocifs, msal-extensions, evaluate, azure-datalake-store, pytablewriter, gcsfs, azure-identity, lm-eval, adlfs\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.1.2\n",
            "    Uninstalling tenacity-9.1.2:\n",
            "      Successfully uninstalled tenacity-9.1.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: gcsfs\n",
            "    Found existing installation: gcsfs 2025.3.2\n",
            "    Uninstalling gcsfs-2025.3.2:\n",
            "      Successfully uninstalled gcsfs-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.5.0+cu121 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.5.0+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed DataProperty-1.1.0 adlfs-2024.12.0 aiobotocore-2.21.1 aioitertools-0.12.0 antlr4-python3-runtime-4.9.3 azure-core-1.33.0 azure-datalake-store-0.0.53 azure-identity-1.21.0 azure-storage-blob-12.25.1 bcrypt-4.3.0 blobfile-3.0.0 botocore-1.37.1 circuitbreaker-2.1.3 colorama-0.4.6 datasets-3.5.0 datatrove-0.4.0 dill-0.3.8 dropbox-12.0.2 dropboxdrivefs-1.4.1 evaluate-0.4.3 fsspec-2024.12.0 fusepy-3.0.1 gcsfs-2024.12.0 isodate-0.7.2 jmespath-1.0.1 jsonlines-4.0.0 libarchive-c-5.2 lm-eval-0.4.8 lockfile-0.12.2 loguru-0.7.3 luigi-3.6.0 mbstrdecoder-1.1.4 msal-1.32.0 msal-extensions-1.3.1 msgspec-0.19.0 multiprocess-0.70.16 objprint-0.3.0 oci-2.150.2 ocifs-1.3.2 omegaconf-2.3.0 paramiko-3.5.1 pathvalidate-3.2.3 portalocker-3.1.1 pybind11-2.13.6 pycryptodomex-3.22.0 pynacl-1.5.0 pyspnego-0.11.2 pytablewriter-1.2.1 python-daemon-3.1.2 rouge-score-0.1.2 s3fs-2024.12.0 sacrebleu-2.5.1 smbprotocol-1.15.0 sqlitedict-2.1.0 stone-3.3.1 submitit-1.5.2 tabledata-1.3.4 tcolorpy-0.1.7 tenacity-8.5.0 tiktoken-0.9.0 tqdm-multiprocess-0.0.11 typepy-1.3.4 viztracer-1.0.3 word2number-1.1 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "92eba90249b74220bbe62145c3b79acd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!!pip install /content/blt/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fn-L3cUUXb6D",
        "outputId": "4bafbc1f-8d95-49d5-b7bc-ffb5d7bb1f05"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Processing ./blt',\n",
              " '  Installing build dependencies ... \\x1b[?25l\\x1b[?25hdone',\n",
              " '  Getting requirements to build wheel ... \\x1b[?25l\\x1b[?25hdone',\n",
              " '  Preparing metadata (pyproject.toml) ... \\x1b[?25l\\x1b[?25hdone',\n",
              " 'Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from bytelatent==0.1.0) (0.2.0)',\n",
              " 'Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from bytelatent==0.1.0) (0.9.0)',\n",
              " 'Requirement already satisfied: xformers in /usr/local/lib/python3.11/dist-packages (from bytelatent==0.1.0) (0.0.28.post2)',\n",
              " 'Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->bytelatent==0.1.0) (2024.11.6)',\n",
              " 'Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->bytelatent==0.1.0) (2.32.3)',\n",
              " 'Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xformers->bytelatent==0.1.0) (2.0.2)',\n",
              " 'Requirement already satisfied: torch==2.5.0 in /usr/local/lib/python3.11/dist-packages (from xformers->bytelatent==0.1.0) (2.5.0+cu121)',\n",
              " 'Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0->xformers->bytelatent==0.1.0) (3.18.0)',\n",
              " 'Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0->xformers->bytelatent==0.1.0) (4.13.2)',\n",
              " 'Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0->xformers->bytelatent==0.1.0) (3.4.2)',\n",
              " 'Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0->xformers->bytelatent==0.1.0) (3.1.6)',\n",
              " 'Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0->xformers->bytelatent==0.1.0) (2024.12.0)',\n",
              " 'Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0->xformers->bytelatent==0.1.0) (12.1.105)',\n",
              " 'Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0->xformers->bytelatent==0.1.0) (12.1.105)',\n",
              " 'Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0->xformers->bytelatent==0.1.0) (12.1.105)',\n",
              " 'Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0->xformers->bytelatent==0.1.0) (9.1.0.70)',\n",
              " 'Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0->xformers->bytelatent==0.1.0) (12.1.3.1)',\n",
              " 'Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0->xformers->bytelatent==0.1.0) (11.0.2.54)',\n",
              " 'Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0->xformers->bytelatent==0.1.0) (10.3.2.106)',\n",
              " 'Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0->xformers->bytelatent==0.1.0) (11.4.5.107)',\n",
              " 'Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0->xformers->bytelatent==0.1.0) (12.1.0.106)',\n",
              " 'Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0->xformers->bytelatent==0.1.0) (2.21.5)',\n",
              " 'Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0->xformers->bytelatent==0.1.0) (12.1.105)',\n",
              " 'Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0->xformers->bytelatent==0.1.0) (3.1.0)',\n",
              " 'Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.0->xformers->bytelatent==0.1.0) (1.13.1)',\n",
              " 'Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.5.0->xformers->bytelatent==0.1.0) (12.5.82)',\n",
              " 'Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.0->xformers->bytelatent==0.1.0) (1.3.0)',\n",
              " 'Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->bytelatent==0.1.0) (3.4.1)',\n",
              " 'Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->bytelatent==0.1.0) (3.10)',\n",
              " 'Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->bytelatent==0.1.0) (2.3.0)',\n",
              " 'Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->bytelatent==0.1.0) (2025.1.31)',\n",
              " 'Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.0->xformers->bytelatent==0.1.0) (3.0.2)',\n",
              " 'Building wheels for collected packages: bytelatent',\n",
              " '  Building wheel for bytelatent (pyproject.toml) ... \\x1b[?25l\\x1b[?25hdone',\n",
              " '  Created wheel for bytelatent: filename=bytelatent-0.1.0-py3-none-any.whl size=147313 sha256=36f249dc148cfbae2a862aa07b0c89327b97d6e8a6632b46cbe7beea63d5892e',\n",
              " '  Stored in directory: /tmp/pip-ephem-wheel-cache-th9f1gcn/wheels/10/13/27/f0c4bd462820770c83979f05849abafc28c4fe62dcba1c6585',\n",
              " 'Successfully built bytelatent',\n",
              " 'Installing collected packages: bytelatent',\n",
              " 'Successfully installed bytelatent-0.1.0']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show xformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqWL1WUnX3yb",
        "outputId": "22d03e6b-e11a-4e0f-8ccf-f990007bf561"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: xformers\n",
            "Version: 0.0.28.post2\n",
            "Summary: XFormers: A collection of composable Transformer building blocks.\n",
            "Home-page: https://facebookresearch.github.io/xformers/\n",
            "Author: Facebook AI Research\n",
            "Author-email: oncall+xformers@xmail.facebook.com\n",
            "License: \n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: numpy, torch\n",
            "Required-by: bytelatent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/blt\n",
        "!python download_blt_weights.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yh_jzXoZYFdp",
        "outputId": "2d72cc40-6673-4e44-bd52-ce6a7c2ec7d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/blt\n",
            "Fetching 8 files:   0% 0/8 [00:00<?, ?it/s]\n",
            "consolidated_with_rope.pth:   0% 0.00/199M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "params.json: 100% 7.94k/7.94k [00:00<00:00, 41.7MB/s]\n",
            "\n",
            "\n",
            "params.json: 100% 4.06k/4.06k [00:00<00:00, 34.6MB/s]\n",
            "\n",
            "\n",
            ".gitattributes: 100% 1.52k/1.52k [00:00<00:00, 11.5MB/s]\n",
            "Fetching 8 files:  12% 1/8 [00:00<00:01,  4.21it/s]\n",
            "\n",
            "consolidated.pth:   0% 0.00/9.07G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "consolidated.pth:   0% 0.00/199M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "LICENSE: 100% 11.5k/11.5k [00:00<00:00, 30.0MB/s]\n",
            "\n",
            "consolidated_with_rope.pth:   5% 10.5M/199M [00:00<00:03, 54.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "consolidated.pth:   5% 10.5M/199M [00:00<00:02, 71.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   0% 10.5M/9.07G [00:00<02:44, 55.1MB/s]\u001b[A\u001b[A\n",
            "consolidated_with_rope.pth:  16% 31.5M/199M [00:00<00:01, 99.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "consolidated.pth:  16% 31.5M/199M [00:00<00:01, 118MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "README.md: 100% 2.20k/2.20k [00:00<00:00, 12.9MB/s]\n",
            "Fetching 8 files:  38% 3/8 [00:00<00:00,  5.68it/s]\n",
            "\n",
            "consolidated.pth:   0% 31.5M/9.07G [00:00<01:37, 93.0MB/s]\u001b[A\u001b[A\n",
            "consolidated_with_rope.pth:  26% 52.4M/199M [00:00<00:01, 129MB/s] \u001b[A\n",
            "\n",
            "\n",
            "consolidated.pth:  26% 52.4M/199M [00:00<00:01, 124MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   1% 52.4M/9.07G [00:00<01:26, 104MB/s] \u001b[A\u001b[A\n",
            "consolidated_with_rope.pth:  37% 73.4M/199M [00:00<00:01, 120MB/s]\u001b[A\n",
            "\n",
            "\n",
            "consolidated.pth:  37% 73.4M/199M [00:00<00:01, 125MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   1% 73.4M/9.07G [00:00<01:16, 117MB/s]\u001b[A\u001b[A\n",
            "consolidated_with_rope.pth:  47% 94.4M/199M [00:00<00:00, 129MB/s]\u001b[A\n",
            "\n",
            "\n",
            "consolidated.pth:  47% 94.4M/199M [00:00<00:00, 138MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   1% 94.4M/9.07G [00:04<12:15, 12.2MB/s]\u001b[A\u001b[A\n",
            "consolidated_with_rope.pth:  58% 115M/199M [00:05<00:06, 13.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "consolidated.pth:  58% 115M/199M [00:05<00:06, 13.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   1% 115M/9.07G [00:05<08:20, 17.9MB/s] \u001b[A\u001b[A\n",
            "consolidated_with_rope.pth:  68% 136M/199M [00:05<00:03, 19.0MB/s]\u001b[A\n",
            "\n",
            "consolidated.pth:   1% 126M/9.07G [00:05<07:03, 21.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "consolidated.pth:  68% 136M/199M [00:05<00:03, 18.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "consolidated_with_rope.pth:  79% 157M/199M [00:05<00:01, 25.4MB/s]\u001b[A\n",
            "\n",
            "consolidated.pth:   2% 136M/9.07G [00:05<05:49, 25.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   2% 147M/9.07G [00:05<04:54, 30.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "consolidated.pth:  79% 157M/199M [00:05<00:01, 24.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   2% 157M/9.07G [00:05<04:07, 36.1MB/s]\u001b[A\u001b[A\n",
            "consolidated_with_rope.pth:  90% 178M/199M [00:05<00:00, 32.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "consolidated.pth:  84% 168M/199M [00:05<00:01, 28.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   2% 168M/9.07G [00:05<03:33, 41.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "consolidated.pth:  90% 178M/199M [00:05<00:00, 32.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "consolidated_with_rope.pth: 100% 199M/199M [00:05<00:00, 40.3MB/s]\u001b[A\n",
            "\n",
            "consolidated_with_rope.pth: 100% 199M/199M [00:06<00:00, 33.0MB/s]\n",
            "\n",
            "\n",
            "consolidated.pth:   2% 189M/9.07G [00:05<02:34, 57.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "consolidated.pth: 100% 199M/199M [00:06<00:00, 32.7MB/s]\n",
            "\n",
            "\n",
            "consolidated.pth:   2% 199M/9.07G [00:06<02:17, 64.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   2% 220M/9.07G [00:06<01:35, 92.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   3% 241M/9.07G [00:06<01:16, 116MB/s] \u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   3% 262M/9.07G [00:06<01:04, 136MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   3% 283M/9.07G [00:06<00:57, 153MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   3% 304M/9.07G [00:06<00:54, 161MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   4% 325M/9.07G [00:06<00:51, 171MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   4% 346M/9.07G [00:06<00:48, 181MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   4% 377M/9.07G [00:07<00:45, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   4% 398M/9.07G [00:07<00:44, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   5% 419M/9.07G [00:07<00:43, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   5% 451M/9.07G [00:07<00:42, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   5% 482M/9.07G [00:07<00:41, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   6% 514M/9.07G [00:07<00:41, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   6% 535M/9.07G [00:07<00:42, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   6% 556M/9.07G [00:07<00:42, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   6% 577M/9.07G [00:07<00:43, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   7% 598M/9.07G [00:08<00:44, 190MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   7% 619M/9.07G [00:08<00:45, 188MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   7% 640M/9.07G [00:08<00:45, 186MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   7% 661M/9.07G [00:08<00:45, 185MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   8% 682M/9.07G [00:08<00:44, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   8% 703M/9.07G [00:08<00:44, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   8% 724M/9.07G [00:08<00:44, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   8% 744M/9.07G [00:08<00:44, 185MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   8% 765M/9.07G [00:09<00:45, 184MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   9% 786M/9.07G [00:09<00:45, 180MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   9% 807M/9.07G [00:09<00:58, 140MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   9% 828M/9.07G [00:09<01:02, 132MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:   9% 849M/9.07G [00:09<00:57, 144MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  10% 870M/9.07G [00:09<00:54, 152MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  10% 891M/9.07G [00:09<00:51, 159MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  10% 912M/9.07G [00:10<00:49, 166MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  10% 933M/9.07G [00:10<00:47, 172MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  11% 954M/9.07G [00:10<00:53, 152MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  11% 975M/9.07G [00:10<01:01, 131MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  11% 996M/9.07G [00:10<00:54, 148MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  11% 1.02G/9.07G [00:10<01:10, 114MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  11% 1.04G/9.07G [00:11<01:23, 96.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  12% 1.06G/9.07G [00:11<01:21, 98.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  12% 1.08G/9.07G [00:11<01:09, 115MB/s] \u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  12% 1.10G/9.07G [00:11<01:23, 94.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  12% 1.12G/9.07G [00:12<01:22, 96.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  13% 1.15G/9.07G [00:12<01:04, 123MB/s] \u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  13% 1.17G/9.07G [00:12<00:57, 138MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  13% 1.20G/9.07G [00:12<00:52, 150MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  14% 1.23G/9.07G [00:12<00:46, 168MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  14% 1.25G/9.07G [00:12<00:44, 177MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  14% 1.27G/9.07G [00:12<00:42, 185MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  14% 1.29G/9.07G [00:12<00:40, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  14% 1.31G/9.07G [00:12<00:45, 169MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  15% 1.33G/9.07G [00:13<00:43, 178MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  15% 1.35G/9.07G [00:13<00:41, 186MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  15% 1.37G/9.07G [00:13<00:39, 192MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  15% 1.41G/9.07G [00:13<00:38, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  16% 1.43G/9.07G [00:13<00:40, 188MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  16% 1.45G/9.07G [00:13<00:40, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  16% 1.47G/9.07G [00:13<00:39, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  16% 1.49G/9.07G [00:13<00:40, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  17% 1.51G/9.07G [00:14<00:39, 190MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  17% 1.53G/9.07G [00:14<00:39, 192MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  17% 1.55G/9.07G [00:14<00:38, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  17% 1.57G/9.07G [00:14<00:39, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  18% 1.59G/9.07G [00:14<00:38, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  18% 1.61G/9.07G [00:14<00:37, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  18% 1.65G/9.07G [00:14<00:36, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  18% 1.67G/9.07G [00:14<00:37, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  19% 1.69G/9.07G [00:14<00:36, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  19% 1.71G/9.07G [00:15<00:36, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  19% 1.73G/9.07G [00:15<00:36, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  19% 1.75G/9.07G [00:15<00:37, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  20% 1.77G/9.07G [00:15<00:37, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  20% 1.79G/9.07G [00:15<00:37, 192MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  20% 1.81G/9.07G [00:15<00:37, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  20% 1.84G/9.07G [00:15<00:37, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  20% 1.86G/9.07G [00:15<00:36, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  21% 1.88G/9.07G [00:15<00:36, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  21% 1.91G/9.07G [00:16<00:35, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  21% 1.94G/9.07G [00:16<00:35, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  22% 1.97G/9.07G [00:16<00:34, 208MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  22% 2.00G/9.07G [00:16<00:33, 212MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  22% 2.03G/9.07G [00:16<00:33, 207MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  23% 2.06G/9.07G [00:16<00:34, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  23% 2.08G/9.07G [00:16<00:34, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  23% 2.10G/9.07G [00:16<00:34, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  23% 2.12G/9.07G [00:17<00:34, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  24% 2.14G/9.07G [00:17<00:34, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  24% 2.17G/9.07G [00:17<00:33, 207MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  24% 2.20G/9.07G [00:17<00:33, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  25% 2.23G/9.07G [00:17<00:32, 210MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  25% 2.26G/9.07G [00:17<00:31, 213MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  25% 2.30G/9.07G [00:17<00:31, 215MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  26% 2.33G/9.07G [00:18<00:31, 217MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  26% 2.36G/9.07G [00:18<00:32, 208MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  26% 2.38G/9.07G [00:18<00:32, 207MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  26% 2.40G/9.07G [00:18<00:32, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  27% 2.42G/9.07G [00:18<00:32, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  27% 2.44G/9.07G [00:18<00:32, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  27% 2.46G/9.07G [00:19<01:51, 59.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  27% 2.49G/9.07G [00:21<03:58, 27.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  28% 2.51G/9.07G [00:21<02:59, 36.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  28% 2.53G/9.07G [00:21<02:16, 47.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  28% 2.55G/9.07G [00:21<01:45, 61.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  28% 2.57G/9.07G [00:21<01:23, 77.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  29% 2.59G/9.07G [00:21<01:09, 93.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  29% 2.61G/9.07G [00:22<00:57, 112MB/s] \u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  29% 2.63G/9.07G [00:22<01:02, 104MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  29% 2.65G/9.07G [00:22<01:09, 92.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  29% 2.67G/9.07G [00:22<01:07, 94.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  30% 2.69G/9.07G [00:22<00:56, 113MB/s] \u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  30% 2.72G/9.07G [00:23<00:58, 109MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  30% 2.75G/9.07G [00:23<00:53, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  31% 2.77G/9.07G [00:23<01:02, 102MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  31% 2.79G/9.07G [00:23<01:08, 92.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  31% 2.82G/9.07G [00:24<00:54, 115MB/s] \u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  31% 2.84G/9.07G [00:24<00:49, 127MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  32% 2.86G/9.07G [00:24<00:47, 129MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  32% 2.88G/9.07G [00:24<01:05, 95.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  32% 2.90G/9.07G [00:24<01:06, 92.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  32% 2.93G/9.07G [00:25<00:56, 109MB/s] \u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  32% 2.95G/9.07G [00:26<02:58, 34.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  33% 2.96G/9.07G [00:27<03:50, 26.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  33% 2.99G/9.07G [00:27<02:24, 42.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  33% 3.02G/9.07G [00:27<01:39, 60.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  34% 3.05G/9.07G [00:27<01:14, 81.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  34% 3.07G/9.07G [00:28<01:04, 93.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  34% 3.09G/9.07G [00:28<00:55, 107MB/s] \u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  34% 3.11G/9.07G [00:28<00:48, 122MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  35% 3.14G/9.07G [00:28<00:43, 136MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  35% 3.16G/9.07G [00:28<00:40, 148MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  35% 3.18G/9.07G [00:28<00:36, 161MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  35% 3.20G/9.07G [00:28<00:34, 171MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  36% 3.22G/9.07G [00:28<00:32, 181MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  36% 3.25G/9.07G [00:28<00:30, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  36% 3.27G/9.07G [00:29<00:29, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  36% 3.30G/9.07G [00:29<00:28, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  37% 3.32G/9.07G [00:29<00:28, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  37% 3.34G/9.07G [00:29<00:28, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  37% 3.38G/9.07G [00:29<00:27, 208MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  38% 3.41G/9.07G [00:29<00:26, 210MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  38% 3.44G/9.07G [00:29<00:26, 210MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  38% 3.47G/9.07G [00:29<00:27, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  39% 3.49G/9.07G [00:30<00:27, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  39% 3.51G/9.07G [00:30<00:28, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  39% 3.53G/9.07G [00:30<00:28, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  39% 3.55G/9.07G [00:30<00:27, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  39% 3.58G/9.07G [00:30<00:27, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  40% 3.60G/9.07G [00:30<00:27, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  40% 3.63G/9.07G [00:30<00:26, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  40% 3.65G/9.07G [00:30<00:26, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  40% 3.67G/9.07G [00:30<00:26, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  41% 3.70G/9.07G [00:31<00:25, 210MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  41% 3.73G/9.07G [00:31<00:25, 212MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  42% 3.76G/9.07G [00:31<00:24, 215MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  42% 3.80G/9.07G [00:31<00:26, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  42% 3.82G/9.07G [00:31<00:26, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  42% 3.84G/9.07G [00:31<00:26, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  43% 3.86G/9.07G [00:32<00:41, 126MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  43% 3.88G/9.07G [00:32<00:38, 134MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  43% 3.90G/9.07G [00:32<00:36, 142MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  43% 3.92G/9.07G [00:32<00:33, 154MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  43% 3.94G/9.07G [00:32<00:31, 164MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  44% 3.96G/9.07G [00:32<00:29, 173MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  44% 3.98G/9.07G [00:32<00:30, 169MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  44% 4.01G/9.07G [00:32<00:30, 168MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  44% 4.03G/9.07G [00:33<00:30, 165MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  45% 4.05G/9.07G [00:33<00:29, 169MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  45% 4.07G/9.07G [00:33<00:28, 177MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  45% 4.09G/9.07G [00:36<03:41, 22.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  45% 4.11G/9.07G [00:37<04:26, 18.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  46% 4.14G/9.07G [00:37<02:50, 29.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  46% 4.17G/9.07G [00:38<01:56, 42.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  46% 4.20G/9.07G [00:38<01:24, 57.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  47% 4.23G/9.07G [00:38<01:09, 69.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  47% 4.25G/9.07G [00:38<00:57, 83.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  47% 4.27G/9.07G [00:38<00:49, 97.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  47% 4.29G/9.07G [00:38<00:41, 114MB/s] \u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  48% 4.31G/9.07G [00:38<00:36, 130MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  48% 4.33G/9.07G [00:38<00:32, 145MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  48% 4.35G/9.07G [00:38<00:30, 157MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  48% 4.37G/9.07G [00:39<00:27, 168MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  48% 4.39G/9.07G [00:39<00:26, 177MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  49% 4.41G/9.07G [00:39<00:25, 182MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  49% 4.44G/9.07G [00:39<00:24, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  49% 4.47G/9.07G [00:39<00:23, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  49% 4.49G/9.07G [00:39<00:23, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  50% 4.51G/9.07G [00:39<00:23, 192MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  50% 4.53G/9.07G [00:39<00:23, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  50% 4.56G/9.07G [00:39<00:22, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  51% 4.59G/9.07G [00:40<00:21, 207MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  51% 4.62G/9.07G [00:40<00:21, 207MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  51% 4.65G/9.07G [00:40<00:22, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  51% 4.67G/9.07G [00:40<00:22, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  52% 4.69G/9.07G [00:40<00:28, 152MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  52% 4.71G/9.07G [00:41<00:36, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  52% 4.73G/9.07G [00:41<00:32, 133MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  52% 4.75G/9.07G [00:41<00:29, 148MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  53% 4.77G/9.07G [00:41<00:27, 159MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  53% 4.79G/9.07G [00:41<00:26, 163MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  53% 4.81G/9.07G [00:41<00:26, 160MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  53% 4.83G/9.07G [00:41<00:25, 165MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  54% 4.85G/9.07G [00:41<00:25, 164MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  54% 4.88G/9.07G [00:41<00:24, 171MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  54% 4.91G/9.07G [00:42<00:22, 187MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  54% 4.94G/9.07G [00:42<00:20, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  55% 4.96G/9.07G [00:42<00:21, 190MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  55% 4.99G/9.07G [00:42<00:20, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  55% 5.01G/9.07G [00:42<00:20, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  56% 5.03G/9.07G [00:42<00:20, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  56% 5.05G/9.07G [00:42<00:20, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  56% 5.08G/9.07G [00:42<00:21, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  56% 5.10G/9.07G [00:43<00:20, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  56% 5.12G/9.07G [00:43<00:20, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  57% 5.14G/9.07G [00:43<00:20, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  57% 5.16G/9.07G [00:43<00:19, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  57% 5.18G/9.07G [00:43<00:19, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  57% 5.21G/9.07G [00:43<00:18, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  58% 5.24G/9.07G [00:43<00:18, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  58% 5.27G/9.07G [00:43<00:17, 212MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  59% 5.31G/9.07G [00:44<00:17, 213MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  59% 5.34G/9.07G [00:44<00:18, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  59% 5.36G/9.07G [00:44<00:19, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  59% 5.38G/9.07G [00:44<00:19, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  60% 5.40G/9.07G [00:44<00:23, 153MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  60% 5.42G/9.07G [00:44<00:23, 154MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  60% 5.44G/9.07G [00:44<00:23, 157MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  60% 5.46G/9.07G [00:47<02:46, 21.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  60% 5.48G/9.07G [00:48<02:15, 26.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  61% 5.49G/9.07G [00:48<02:05, 28.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  61% 5.52G/9.07G [00:48<01:30, 39.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  61% 5.54G/9.07G [00:48<01:06, 52.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  61% 5.57G/9.07G [00:48<00:46, 76.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  62% 5.59G/9.07G [00:49<00:38, 89.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  62% 5.61G/9.07G [00:49<00:32, 105MB/s] \u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  62% 5.63G/9.07G [00:49<00:32, 106MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  62% 5.65G/9.07G [00:49<00:29, 116MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  63% 5.68G/9.07G [00:49<00:25, 133MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  63% 5.70G/9.07G [00:50<00:30, 109MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  63% 5.73G/9.07G [00:50<00:34, 96.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  63% 5.75G/9.07G [00:50<00:37, 88.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  63% 5.76G/9.07G [00:50<00:36, 90.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  64% 5.78G/9.07G [00:50<00:29, 110MB/s] \u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  64% 5.80G/9.07G [00:50<00:25, 128MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  64% 5.82G/9.07G [00:51<00:23, 136MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  64% 5.84G/9.07G [00:51<00:21, 149MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  65% 5.86G/9.07G [00:51<00:19, 162MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  65% 5.88G/9.07G [00:51<00:18, 174MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  65% 5.90G/9.07G [00:51<00:26, 121MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  65% 5.93G/9.07G [00:51<00:21, 144MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  66% 5.97G/9.07G [00:51<00:18, 164MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  66% 6.00G/9.07G [00:52<00:17, 174MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  66% 6.02G/9.07G [00:52<00:17, 173MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  67% 6.04G/9.07G [00:52<00:16, 179MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  67% 6.06G/9.07G [00:52<00:16, 185MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  67% 6.08G/9.07G [00:52<00:15, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  67% 6.10G/9.07G [00:52<00:15, 192MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  68% 6.12G/9.07G [00:52<00:15, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  68% 6.14G/9.07G [00:52<00:14, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  68% 6.17G/9.07G [00:52<00:14, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  68% 6.20G/9.07G [00:53<00:14, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  69% 6.22G/9.07G [00:53<00:14, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  69% 6.25G/9.07G [00:53<00:13, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  69% 6.27G/9.07G [00:53<00:13, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  69% 6.29G/9.07G [00:53<00:13, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  70% 6.31G/9.07G [00:53<00:13, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  70% 6.33G/9.07G [00:53<00:13, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  70% 6.36G/9.07G [00:53<00:13, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  70% 6.39G/9.07G [00:56<01:27, 30.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  71% 6.41G/9.07G [00:56<01:07, 39.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  71% 6.43G/9.07G [00:56<00:52, 50.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  71% 6.45G/9.07G [00:56<00:41, 63.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  71% 6.47G/9.07G [00:56<00:32, 78.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  72% 6.49G/9.07G [00:56<00:27, 93.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  72% 6.51G/9.07G [00:56<00:23, 108MB/s] \u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  72% 6.53G/9.07G [00:57<00:20, 123MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  72% 6.55G/9.07G [00:57<00:18, 133MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  73% 6.57G/9.07G [00:57<00:17, 143MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  73% 6.60G/9.07G [00:57<00:16, 149MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  73% 6.62G/9.07G [00:57<00:15, 157MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  73% 6.64G/9.07G [00:57<00:14, 165MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  73% 6.66G/9.07G [00:57<00:14, 170MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  74% 6.68G/9.07G [00:57<00:14, 167MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  74% 6.70G/9.07G [00:58<00:14, 167MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  74% 6.72G/9.07G [00:58<00:14, 166MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  74% 6.74G/9.07G [00:58<00:13, 176MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  75% 6.76G/9.07G [00:58<00:12, 182MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  75% 6.79G/9.07G [00:58<00:11, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  75% 6.83G/9.07G [00:58<00:11, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  76% 6.85G/9.07G [00:58<00:11, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  76% 6.87G/9.07G [00:58<00:11, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  76% 6.89G/9.07G [00:59<00:11, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  76% 6.91G/9.07G [00:59<00:11, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  77% 6.94G/9.07G [00:59<00:10, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  77% 6.97G/9.07G [00:59<00:10, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  77% 6.99G/9.07G [00:59<00:10, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  77% 7.03G/9.07G [00:59<00:10, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  78% 7.05G/9.07G [00:59<00:10, 190MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  78% 7.07G/9.07G [00:59<00:10, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  78% 7.09G/9.07G [01:00<00:10, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  79% 7.12G/9.07G [01:00<00:09, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  79% 7.14G/9.07G [01:01<00:29, 64.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  79% 7.16G/9.07G [01:01<00:25, 73.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  79% 7.18G/9.07G [01:01<00:21, 88.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  79% 7.20G/9.07G [01:01<00:17, 104MB/s] \u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  80% 7.22G/9.07G [01:01<00:17, 106MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  80% 7.25G/9.07G [01:02<00:20, 90.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  80% 7.27G/9.07G [01:02<00:16, 107MB/s] \u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  80% 7.29G/9.07G [01:02<00:14, 122MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  81% 7.31G/9.07G [01:02<00:15, 116MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  81% 7.33G/9.07G [01:02<00:18, 94.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  81% 7.35G/9.07G [01:02<00:17, 97.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  81% 7.37G/9.07G [01:03<00:18, 91.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  82% 7.40G/9.07G [01:03<00:14, 115MB/s] \u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  82% 7.42G/9.07G [01:03<00:12, 130MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  82% 7.44G/9.07G [01:03<00:15, 104MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  82% 7.47G/9.07G [01:03<00:15, 105MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  83% 7.49G/9.07G [01:04<00:13, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  83% 7.52G/9.07G [01:04<00:12, 128MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  83% 7.54G/9.07G [01:04<00:11, 137MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  83% 7.57G/9.07G [01:04<00:09, 159MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  84% 7.59G/9.07G [01:04<00:08, 169MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  84% 7.62G/9.07G [01:04<00:07, 183MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  84% 7.64G/9.07G [01:04<00:07, 186MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  85% 7.67G/9.07G [01:05<00:07, 188MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  85% 7.70G/9.07G [01:05<00:06, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  85% 7.73G/9.07G [01:05<00:06, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  86% 7.76G/9.07G [01:05<00:06, 209MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  86% 7.79G/9.07G [01:05<00:06, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  86% 7.81G/9.07G [01:05<00:06, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  86% 7.83G/9.07G [01:05<00:06, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  87% 7.85G/9.07G [01:05<00:06, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  87% 7.87G/9.07G [01:06<00:05, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  87% 7.90G/9.07G [01:06<00:05, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  87% 7.92G/9.07G [01:06<00:05, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  88% 7.94G/9.07G [01:06<00:05, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  88% 7.96G/9.07G [01:06<00:05, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  88% 7.98G/9.07G [01:06<00:05, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  88% 8.00G/9.07G [01:06<00:05, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  88% 8.02G/9.07G [01:06<00:05, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  89% 8.05G/9.07G [01:07<00:05, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  89% 8.07G/9.07G [01:07<00:05, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  89% 8.11G/9.07G [01:07<00:04, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  90% 8.13G/9.07G [01:07<00:04, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  90% 8.15G/9.07G [01:07<00:04, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  90% 8.17G/9.07G [01:07<00:04, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  90% 8.19G/9.07G [01:07<00:04, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  91% 8.21G/9.07G [01:07<00:04, 192MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  91% 8.24G/9.07G [01:07<00:04, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  91% 8.27G/9.07G [01:08<00:03, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  92% 8.30G/9.07G [01:08<00:03, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  92% 8.33G/9.07G [01:08<00:03, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  92% 8.35G/9.07G [01:08<00:03, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  92% 8.37G/9.07G [01:08<00:03, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  93% 8.39G/9.07G [01:08<00:03, 182MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  93% 8.41G/9.07G [01:08<00:03, 180MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  93% 8.43G/9.07G [01:08<00:03, 183MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  93% 8.45G/9.07G [01:09<00:03, 184MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  93% 8.47G/9.07G [01:09<00:03, 184MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  94% 8.49G/9.07G [01:12<00:29, 19.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  94% 8.52G/9.07G [01:12<00:17, 30.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  94% 8.56G/9.07G [01:12<00:11, 43.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  95% 8.59G/9.07G [01:12<00:08, 59.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  95% 8.61G/9.07G [01:13<00:06, 72.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  95% 8.63G/9.07G [01:13<00:05, 85.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  95% 8.65G/9.07G [01:13<00:04, 101MB/s] \u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  96% 8.67G/9.07G [01:13<00:03, 117MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  96% 8.70G/9.07G [01:13<00:02, 139MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  96% 8.72G/9.07G [01:13<00:02, 151MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  97% 8.76G/9.07G [01:13<00:01, 169MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  97% 8.78G/9.07G [01:13<00:01, 171MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  97% 8.80G/9.07G [01:14<00:01, 174MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  97% 8.82G/9.07G [01:14<00:01, 182MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  97% 8.84G/9.07G [01:14<00:01, 188MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  98% 8.86G/9.07G [01:14<00:01, 125MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  98% 8.88G/9.07G [01:16<00:06, 30.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  98% 8.90G/9.07G [01:16<00:04, 38.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  99% 8.93G/9.07G [01:16<00:02, 56.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  99% 8.97G/9.07G [01:16<00:01, 76.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  99% 8.99G/9.07G [01:17<00:00, 91.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth:  99% 9.01G/9.07G [01:17<00:00, 105MB/s] \u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth: 100% 9.03G/9.07G [01:17<00:00, 109MB/s]\u001b[A\u001b[A\n",
            "\n",
            "consolidated.pth: 100% 9.07G/9.07G [01:17<00:00, 117MB/s]\n",
            "Fetching 8 files: 100% 8/8 [01:17<00:00,  9.73s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python demo.py \"A BLT has\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gX8ekKKYKdg",
        "outputId": "79caccae-ca7a-4e98-9fc2-2eb564bf30c8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BLT model: blt-1b\n",
            "📦 تحميل المعالج من: hf-weights/blt-1b\n",
            "[rank0]: \u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/content/blt/\u001b[0m\u001b[1;33mdemo.py\u001b[0m:\u001b[94m20\u001b[0m in \u001b[92mmain\u001b[0m                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   │   \u001b[0msetup_torch_distributed(distributed_args)                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m18 \u001b[0m\u001b[2m│   \u001b[0mcheckpoint_path = os.path.join(\u001b[33m\"\u001b[0m\u001b[33mhf-weights\u001b[0m\u001b[33m\"\u001b[0m, model_name)            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mLoading BLT model: \u001b[0m\u001b[33m{\u001b[0mmodel_name\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m20 \u001b[2m│   \u001b[0mmodel, tokenizer, train_cfg = \u001b[1;4mload_consolidated_model_and_tokenizer\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mcheckpoint_path,\u001b[0m                                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[1;2;4m│   \u001b[0m\u001b[1;4m)\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m23 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94massert\u001b[0m \u001b[96misinstance\u001b[0m(model, ByteLatentTransformer)                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m─────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m──────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  checkpoint_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m                              \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m distributed_args = \u001b[1;35mDistributedArgs\u001b[0m\u001b[1m(\u001b[0m                                 \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdp_shard\u001b[0m=\u001b[94m1\u001b[0m,                                  \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdp_replicate\u001b[0m=\u001b[94m1\u001b[0m,                              \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mtp_size\u001b[0m=\u001b[94m1\u001b[0m,                                   \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mselective_activation_checkpointing\u001b[0m=\u001b[94mFalse\u001b[0m,    \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mcompile\u001b[0m=\u001b[94mFalse\u001b[0m,                               \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfsdp_type\u001b[0m=\u001b[33m'no_shard'\u001b[0m,                        \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mmodel_dtype\u001b[0m=\u001b[33m'bf16'\u001b[0m,                          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfloat8_recipe\u001b[0m=\u001b[94mNone\u001b[0m,                          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfloat8_filter\u001b[0m=\u001b[33m'layers\\\\.\u001b[0m\u001b[1;33m[\u001b[0m\u001b[33m0-9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[33m+\\\\.'\u001b[0m,          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mmatmul_allow_tf32\u001b[0m=\u001b[94mFalse\u001b[0m,                     \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mallow_bf16_reduced_precision_reduction\u001b[0m=\u001b[94mTrue\u001b[0m, \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdetect_anomaly\u001b[0m=\u001b[94mFalse\u001b[0m,                        \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mcompile_cache_size_limit\u001b[0m=\u001b[94m8\u001b[0m,                  \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mspawn_method\u001b[0m=\u001b[33m'forkserver'\u001b[0m                    \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[1m)\u001b[0m                                                \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       model_name = \u001b[33m'blt-1b'\u001b[0m                                         \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           prompt = \u001b[33m'A BLT has'\u001b[0m                                      \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰─────────────────────────────────────────────────────────────────────╯\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/content/blt/bytelatent/\u001b[0m\u001b[1;33mgenerate.py\u001b[0m:\u001b[94m18\u001b[0m in                                    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[92mload_consolidated_model_and_tokenizer\u001b[0m                                        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33mتحميل النموذج والمعالج من Hugging Face بطريقة مرنة تدعم التوزيع وال\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m\\U0001F4E6\u001b[0m\u001b[33m تحميل المعالج من: \u001b[0m\u001b[33m{\u001b[0mcheckpoint_path\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m18 \u001b[2m│   \u001b[0mtokenizer = \u001b[1;4mAutoTokenizer.from_pretrained(\u001b[0m                          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mcheckpoint_path,\u001b[0m                                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mtrust_remote_code=trust_remote_code,\u001b[0m                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[1;2;4m│   \u001b[0m\u001b[1;4m)\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m───────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────\u001b[0m\u001b[33m─╮\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   checkpoint_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        device_map = \u001b[94mNone\u001b[0m                \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  init_distributed = \u001b[94mFalse\u001b[0m               \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                  \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m low_cpu_mem_usage = \u001b[94mFalse\u001b[0m               \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       torch_dtype = \u001b[94mNone\u001b[0m                \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m trust_remote_code = \u001b[94mTrue\u001b[0m                \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   use_safetensors = \u001b[94mTrue\u001b[0m                \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰─────────────────────────────────────────╯\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/\u001b[0m\u001b[1;33mtokenizatio\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[1;33mn_auto.py\u001b[0m:\u001b[94m966\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 963 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mconfig_dict = load_gguf_checkpoint(gguf_path, ret \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 964 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mconfig = AutoConfig.for_model(**config_dict)      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 965 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 966 \u001b[2m│   │   │   │   │   \u001b[0mconfig = \u001b[1;4mAutoConfig.from_pretrained(\u001b[0m              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 967 \u001b[0m\u001b[1;2;4m│   │   │   │   │   │   \u001b[0m\u001b[1;4mpretrained_model_name_or_path, trust_remote_c\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 968 \u001b[0m\u001b[1;2;4m│   │   │   │   │   \u001b[0m\u001b[1;4m)\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 969 \u001b[0m\u001b[2m│   │   │   \u001b[0mconfig_tokenizer_class = config.tokenizer_class           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m──────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m──────────────────────\u001b[0m\u001b[33m─╮\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        config = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        config_tokenizer_class = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     gguf_file = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        inputs = \u001b[1m(\u001b[0m\u001b[1m)\u001b[0m                   \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        kwargs = \u001b[1m{\u001b[0m\u001b[33m'_from_auto'\u001b[0m: \u001b[94mTrue\u001b[0m\u001b[1m}\u001b[0m \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m pretrained_model_name_or_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m  \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            tokenizer_auto_map = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              tokenizer_config = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                   \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                tokenizer_type = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             trust_remote_code = \u001b[94mTrue\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                use_auth_token = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      use_fast = \u001b[94mTrue\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────╯\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/\u001b[0m\u001b[1;33mconfigurati\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[1;33mon_auto.py\u001b[0m:\u001b[94m1151\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m                                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1148 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m pattern \u001b[95min\u001b[0m \u001b[96mstr\u001b[0m(pretrained_model_name_or_path):     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1149 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m CONFIG_MAPPING[pattern].from_dict(config_d \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1150 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1151 \u001b[2m│   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;96mValueError\u001b[0m\u001b[1;4m(\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1152 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;33mf\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mUnrecognized model in \u001b[0m\u001b[1;4;33m{\u001b[0m\u001b[1;4mpretrained_model_name_or_path\u001b[0m\u001b[1;4;33m}\u001b[0m\u001b[1;4;33m. \u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1153 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;33mf\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mShould have a `model_type` key in its \u001b[0m\u001b[1;4;33m{\u001b[0m\u001b[1;4mCONFIG_NAME\u001b[0m\u001b[1;4;33m}\u001b[0m\u001b[1;4;33m, or\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1154 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;33mf\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33min its name: \u001b[0m\u001b[1;4;33m{\u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4;33m, \u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4m.join(CONFIG_MAPPING.keys())\u001b[0m\u001b[1;4;33m}\u001b[0m\u001b[1;4;33m\"\u001b[0m        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 code_revision = \u001b[94mNone\u001b[0m                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   config_dict = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                has_local_code = \u001b[94mFalse\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               has_remote_code = \u001b[94mFalse\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        kwargs = \u001b[1m{\u001b[0m                                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                 \u001b[2m│   \u001b[0m\u001b[33m'_from_auto'\u001b[0m: \u001b[94mTrue\u001b[0m,                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                 \u001b[2m│   \u001b[0m\u001b[33m'name_or_path'\u001b[0m: \u001b[33m'hf-weights/blt-1b'\u001b[0m  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                 \u001b[1m}\u001b[0m                                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                       pattern = \u001b[33m't5'\u001b[0m                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m pretrained_model_name_or_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             trust_remote_code = \u001b[94mTrue\u001b[0m                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 unused_kwargs = \u001b[1m{\u001b[0m\u001b[33m'name_or_path'\u001b[0m: \u001b[33m'hf-weights/blt-1b'\u001b[0m\u001b[1m}\u001b[0m    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                use_auth_token = \u001b[94mNone\u001b[0m                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "[rank0]: \u001b[1;91mValueError: \u001b[0mUnrecognized model in hf-weights/blt-1b. Should have a `model_type` \n",
            "[rank0]: key in its config.json, or contain one of the following strings in its name: \n",
            "[rank0]: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, \n",
            "[rank0]: autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, \n",
            "[rank0]: big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, \n",
            "[rank0]: blip-\u001b[1;36m2\u001b[0m, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, \n",
            "[rank0]: chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, \n",
            "[rank0]: clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, \n",
            "[rank0]: convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, \n",
            "[rank0]: data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, \n",
            "[rank0]: decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, \n",
            "[rank0]: depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, \n",
            "[rank0]: distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, \n",
            "[rank0]: encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, \n",
            "[rank0]: fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, \n",
            "[rank0]: gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, \n",
            "[rank0]: gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, \n",
            "[rank0]: granite, granitemoe, granitemoeshared, granitevision, graphormer, \n",
            "[rank0]: grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, \n",
            "[rank0]: idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, \n",
            "[rank0]: instructblipvideo, jamba, jetmoe, jukebox, kosmos-\u001b[1;36m2\u001b[0m, layoutlm, layoutlmv2, \n",
            "[rank0]: layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, \n",
            "[rank0]: llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, \n",
            "[rank0]: mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, \n",
            "[rank0]: mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, \n",
            "[rank0]: mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, \n",
            "[rank0]: modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, \n",
            "[rank0]: mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, \n",
            "[rank0]: omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, \n",
            "[rank0]: patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, \n",
            "[rank0]: phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, \n",
            "[rank0]: prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, \n",
            "[rank0]: qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, \n",
            "[rank0]: realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, \n",
            "[rank0]: roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, \n",
            "[rank0]: rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, \n",
            "[rank0]: sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, \n",
            "[rank0]: smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, \n",
            "[rank0]: speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, \n",
            "[rank0]: swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, \n",
            "[rank0]: tapas, textnet, time_series_transformer, timesformer, timm_backbone, \n",
            "[rank0]: timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, \n",
            "[rank0]: unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, \n",
            "[rank0]: vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, \n",
            "[rank0]: vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits,\n",
            "[rank0]: vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm,\n",
            "[rank0]: xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, \n",
            "[rank0]: zamba, zamba2, zoedepth\n",
            "[rank0]:[W423 22:21:51.745079934 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python demo2.py \"A BLT has\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oe2QlfhXYk_a",
        "outputId": "703824bb-86cc-4562-e468-aa7e0e3ea3d4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. Using device: cuda\n",
            "Loading BLT model: blt-1b\n",
            "📦 تحميل المعالج من: hf-weights/blt-1b\n",
            "[rank0]: \u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/content/blt/\u001b[0m\u001b[1;33mdemo2.py\u001b[0m:\u001b[94m34\u001b[0m in \u001b[92mmain\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m31 \u001b[0m\u001b[2m│   \u001b[0m                                                                    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m32 \u001b[0m\u001b[2m│   \u001b[0mcheckpoint_path = os.path.join(\u001b[33m\"\u001b[0m\u001b[33mhf-weights\u001b[0m\u001b[33m\"\u001b[0m, model_name) \u001b[2m# تأكد أن \u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m33 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mLoading BLT model: \u001b[0m\u001b[33m{\u001b[0mmodel_name\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m34 \u001b[2m│   \u001b[0mmodel, tokenizer, train_cfg = \u001b[1;4mload_consolidated_model_and_tokenizer\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m35 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mcheckpoint_path,\u001b[0m                                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m36 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;2;4m# device=device # <<< تحقق مما إذا كانت هذه الدالة تقبل وسيط 'd\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m37 \u001b[0m\u001b[1;2;4m│   │   │   │   │      \u001b[0m\u001b[1;2;4m# إذا كانت تقبل، فهذا أفضل مكان لوضع النموذج على\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m─────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m──────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  checkpoint_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m                              \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           device = \u001b[1;35mdevice\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtype\u001b[0m=\u001b[33m'cuda'\u001b[0m\u001b[1m)\u001b[0m                              \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m distributed_args = \u001b[1;35mDistributedArgs\u001b[0m\u001b[1m(\u001b[0m                                 \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdp_shard\u001b[0m=\u001b[94m1\u001b[0m,                                  \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdp_replicate\u001b[0m=\u001b[94m1\u001b[0m,                              \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mtp_size\u001b[0m=\u001b[94m1\u001b[0m,                                   \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mselective_activation_checkpointing\u001b[0m=\u001b[94mFalse\u001b[0m,    \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mcompile\u001b[0m=\u001b[94mFalse\u001b[0m,                               \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfsdp_type\u001b[0m=\u001b[33m'no_shard'\u001b[0m,                        \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mmodel_dtype\u001b[0m=\u001b[33m'bf16'\u001b[0m,                          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfloat8_recipe\u001b[0m=\u001b[94mNone\u001b[0m,                          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfloat8_filter\u001b[0m=\u001b[33m'layers\\\\.\u001b[0m\u001b[1;33m[\u001b[0m\u001b[33m0-9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[33m+\\\\.'\u001b[0m,          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mmatmul_allow_tf32\u001b[0m=\u001b[94mFalse\u001b[0m,                     \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mallow_bf16_reduced_precision_reduction\u001b[0m=\u001b[94mTrue\u001b[0m, \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdetect_anomaly\u001b[0m=\u001b[94mFalse\u001b[0m,                        \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mcompile_cache_size_limit\u001b[0m=\u001b[94m8\u001b[0m,                  \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mspawn_method\u001b[0m=\u001b[33m'forkserver'\u001b[0m                    \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[1m)\u001b[0m                                                \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       model_name = \u001b[33m'blt-1b'\u001b[0m                                         \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           prompt = \u001b[33m'A BLT has'\u001b[0m                                      \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰─────────────────────────────────────────────────────────────────────╯\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/content/blt/bytelatent/\u001b[0m\u001b[1;33mgenerate.py\u001b[0m:\u001b[94m18\u001b[0m in                                    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[92mload_consolidated_model_and_tokenizer\u001b[0m                                        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33mتحميل النموذج والمعالج من Hugging Face بطريقة مرنة تدعم التوزيع وال\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m\\U0001F4E6\u001b[0m\u001b[33m تحميل المعالج من: \u001b[0m\u001b[33m{\u001b[0mcheckpoint_path\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m18 \u001b[2m│   \u001b[0mtokenizer = \u001b[1;4mAutoTokenizer.from_pretrained(\u001b[0m                          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mcheckpoint_path,\u001b[0m                                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mtrust_remote_code=trust_remote_code,\u001b[0m                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[1;2;4m│   \u001b[0m\u001b[1;4m)\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m───────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────\u001b[0m\u001b[33m─╮\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   checkpoint_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        device_map = \u001b[94mNone\u001b[0m                \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  init_distributed = \u001b[94mFalse\u001b[0m               \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                  \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m low_cpu_mem_usage = \u001b[94mFalse\u001b[0m               \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       torch_dtype = \u001b[94mNone\u001b[0m                \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m trust_remote_code = \u001b[94mTrue\u001b[0m                \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   use_safetensors = \u001b[94mTrue\u001b[0m                \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰─────────────────────────────────────────╯\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/\u001b[0m\u001b[1;33mtokenizatio\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[1;33mn_auto.py\u001b[0m:\u001b[94m966\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 963 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mconfig_dict = load_gguf_checkpoint(gguf_path, ret \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 964 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mconfig = AutoConfig.for_model(**config_dict)      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 965 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 966 \u001b[2m│   │   │   │   │   \u001b[0mconfig = \u001b[1;4mAutoConfig.from_pretrained(\u001b[0m              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 967 \u001b[0m\u001b[1;2;4m│   │   │   │   │   │   \u001b[0m\u001b[1;4mpretrained_model_name_or_path, trust_remote_c\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 968 \u001b[0m\u001b[1;2;4m│   │   │   │   │   \u001b[0m\u001b[1;4m)\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 969 \u001b[0m\u001b[2m│   │   │   \u001b[0mconfig_tokenizer_class = config.tokenizer_class           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m──────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m──────────────────────\u001b[0m\u001b[33m─╮\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        config = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        config_tokenizer_class = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     gguf_file = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        inputs = \u001b[1m(\u001b[0m\u001b[1m)\u001b[0m                   \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        kwargs = \u001b[1m{\u001b[0m\u001b[33m'_from_auto'\u001b[0m: \u001b[94mTrue\u001b[0m\u001b[1m}\u001b[0m \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m pretrained_model_name_or_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m  \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            tokenizer_auto_map = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              tokenizer_config = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                   \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                tokenizer_type = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             trust_remote_code = \u001b[94mTrue\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                use_auth_token = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      use_fast = \u001b[94mTrue\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────╯\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/\u001b[0m\u001b[1;33mconfigurati\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[1;33mon_auto.py\u001b[0m:\u001b[94m1151\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m                                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1148 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m pattern \u001b[95min\u001b[0m \u001b[96mstr\u001b[0m(pretrained_model_name_or_path):     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1149 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m CONFIG_MAPPING[pattern].from_dict(config_d \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1150 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1151 \u001b[2m│   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;96mValueError\u001b[0m\u001b[1;4m(\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1152 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;33mf\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mUnrecognized model in \u001b[0m\u001b[1;4;33m{\u001b[0m\u001b[1;4mpretrained_model_name_or_path\u001b[0m\u001b[1;4;33m}\u001b[0m\u001b[1;4;33m. \u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1153 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;33mf\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mShould have a `model_type` key in its \u001b[0m\u001b[1;4;33m{\u001b[0m\u001b[1;4mCONFIG_NAME\u001b[0m\u001b[1;4;33m}\u001b[0m\u001b[1;4;33m, or\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1154 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;33mf\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33min its name: \u001b[0m\u001b[1;4;33m{\u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4;33m, \u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4m.join(CONFIG_MAPPING.keys())\u001b[0m\u001b[1;4;33m}\u001b[0m\u001b[1;4;33m\"\u001b[0m        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 code_revision = \u001b[94mNone\u001b[0m                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   config_dict = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                has_local_code = \u001b[94mFalse\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               has_remote_code = \u001b[94mFalse\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        kwargs = \u001b[1m{\u001b[0m                                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                 \u001b[2m│   \u001b[0m\u001b[33m'_from_auto'\u001b[0m: \u001b[94mTrue\u001b[0m,                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                 \u001b[2m│   \u001b[0m\u001b[33m'name_or_path'\u001b[0m: \u001b[33m'hf-weights/blt-1b'\u001b[0m  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                 \u001b[1m}\u001b[0m                                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                       pattern = \u001b[33m't5'\u001b[0m                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m pretrained_model_name_or_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             trust_remote_code = \u001b[94mTrue\u001b[0m                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 unused_kwargs = \u001b[1m{\u001b[0m\u001b[33m'name_or_path'\u001b[0m: \u001b[33m'hf-weights/blt-1b'\u001b[0m\u001b[1m}\u001b[0m    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                use_auth_token = \u001b[94mNone\u001b[0m                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "[rank0]: \u001b[1;91mValueError: \u001b[0mUnrecognized model in hf-weights/blt-1b. Should have a `model_type` \n",
            "[rank0]: key in its config.json, or contain one of the following strings in its name: \n",
            "[rank0]: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, \n",
            "[rank0]: autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, \n",
            "[rank0]: big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, \n",
            "[rank0]: blip-\u001b[1;36m2\u001b[0m, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, \n",
            "[rank0]: chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, \n",
            "[rank0]: clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, \n",
            "[rank0]: convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, \n",
            "[rank0]: data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, \n",
            "[rank0]: decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, \n",
            "[rank0]: depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, \n",
            "[rank0]: distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, \n",
            "[rank0]: encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, \n",
            "[rank0]: fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, \n",
            "[rank0]: gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, \n",
            "[rank0]: gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, \n",
            "[rank0]: granite, granitemoe, granitemoeshared, granitevision, graphormer, \n",
            "[rank0]: grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, \n",
            "[rank0]: idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, \n",
            "[rank0]: instructblipvideo, jamba, jetmoe, jukebox, kosmos-\u001b[1;36m2\u001b[0m, layoutlm, layoutlmv2, \n",
            "[rank0]: layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, \n",
            "[rank0]: llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, \n",
            "[rank0]: mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, \n",
            "[rank0]: mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, \n",
            "[rank0]: mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, \n",
            "[rank0]: modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, \n",
            "[rank0]: mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, \n",
            "[rank0]: omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, \n",
            "[rank0]: patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, \n",
            "[rank0]: phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, \n",
            "[rank0]: prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, \n",
            "[rank0]: qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, \n",
            "[rank0]: realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, \n",
            "[rank0]: roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, \n",
            "[rank0]: rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, \n",
            "[rank0]: sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, \n",
            "[rank0]: smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, \n",
            "[rank0]: speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, \n",
            "[rank0]: swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, \n",
            "[rank0]: tapas, textnet, time_series_transformer, timesformer, timm_backbone, \n",
            "[rank0]: timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, \n",
            "[rank0]: unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, \n",
            "[rank0]: vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, \n",
            "[rank0]: vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits,\n",
            "[rank0]: vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm,\n",
            "[rank0]: xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, \n",
            "[rank0]: zamba, zamba2, zoedepth\n",
            "[rank0]:[W423 22:21:30.911742004 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!!python setup.py install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x00iu_S8aXlL",
        "outputId": "1a480d16-b08c-4284-f0c8-b8305d58a364"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['running install',\n",
              " '/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.',\n",
              " '!!',\n",
              " '',\n",
              " '        ********************************************************************************',\n",
              " '        Please avoid running ``setup.py`` directly.',\n",
              " '        Instead, use pypa/build, pypa/installer or other',\n",
              " '        standards-based tools.',\n",
              " '',\n",
              " '        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.',\n",
              " '        ********************************************************************************',\n",
              " '',\n",
              " '!!',\n",
              " '  self.initialize_options()',\n",
              " '/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.',\n",
              " '!!',\n",
              " '',\n",
              " '        ********************************************************************************',\n",
              " '        Please avoid running ``setup.py`` and ``easy_install``.',\n",
              " '        Instead, use pypa/build, pypa/installer or other',\n",
              " '        standards-based tools.',\n",
              " '',\n",
              " '        See https://github.com/pypa/setuptools/issues/917 for details.',\n",
              " '        ********************************************************************************',\n",
              " '',\n",
              " '!!',\n",
              " '  self.initialize_options()',\n",
              " 'running bdist_egg',\n",
              " 'running egg_info',\n",
              " 'writing bytelatent.egg-info/PKG-INFO',\n",
              " 'writing dependency_links to bytelatent.egg-info/dependency_links.txt',\n",
              " 'writing requirements to bytelatent.egg-info/requires.txt',\n",
              " 'writing top-level names to bytelatent.egg-info/top_level.txt',\n",
              " \"reading manifest file 'bytelatent.egg-info/SOURCES.txt'\",\n",
              " \"adding license file 'LICENSE'\",\n",
              " \"writing manifest file 'bytelatent.egg-info/SOURCES.txt'\",\n",
              " 'installing library code to build/bdist.linux-x86_64/egg',\n",
              " 'running install_lib',\n",
              " 'running build_py',\n",
              " 'copying bytelatent/generate.py -> build/lib/bytelatent',\n",
              " 'copying bytelatent/constants.py -> build/lib/bytelatent',\n",
              " 'copying bytelatent/entropy_model.py -> build/lib/bytelatent',\n",
              " 'copying bytelatent/base_transformer.py -> build/lib/bytelatent',\n",
              " 'copying bytelatent/logger.py -> build/lib/bytelatent',\n",
              " 'copying bytelatent/probe.py -> build/lib/bytelatent',\n",
              " 'copying bytelatent/test_entropy_model.py -> build/lib/bytelatent',\n",
              " 'copying bytelatent/transformer.py -> build/lib/bytelatent',\n",
              " 'copying bytelatent/profiling.py -> build/lib/bytelatent',\n",
              " 'copying bytelatent/norms.py -> build/lib/bytelatent',\n",
              " 'copying bytelatent/print_config.py -> build/lib/bytelatent',\n",
              " 'copying bytelatent/metrics.py -> build/lib/bytelatent',\n",
              " 'copying bytelatent/test_config_parser.py -> build/lib/bytelatent',\n",
              " 'copying bytelatent/eval.py -> build/lib/bytelatent',\n",
              " 'copying bytelatent/iterate_data.py -> build/lib/bytelatent',\n",
              " 'copying bytelatent/args.py -> build/lib/bytelatent',\n",
              " 'copying bytelatent/stool.py -> build/lib/bytelatent',\n",
              " 'copying bytelatent/config_parser.py -> build/lib/bytelatent',\n",
              " 'copying bytelatent/float8.py -> build/lib/bytelatent',\n",
              " 'copying bytelatent/train.py -> build/lib/bytelatent',\n",
              " 'copying bytelatent/checkpoint.py -> build/lib/bytelatent',\n",
              " 'copying bytelatent/__init__.py -> build/lib/bytelatent',\n",
              " 'copying bytelatent/generate_blt.py -> build/lib/bytelatent',\n",
              " 'copying bytelatent/distributed.py -> build/lib/bytelatent',\n",
              " 'copying bytelatent/test_blt.py -> build/lib/bytelatent',\n",
              " 'copying bytelatent/optim.py -> build/lib/bytelatent',\n",
              " 'copying apps/__init__.py -> build/lib/apps',\n",
              " 'copying bytelatent/preprocess/preprocess_entropies.py -> build/lib/bytelatent/preprocess',\n",
              " 'copying bytelatent/preprocess/fsspec_target.py -> build/lib/bytelatent/preprocess',\n",
              " 'copying bytelatent/preprocess/parallel_entropies.py -> build/lib/bytelatent/preprocess',\n",
              " 'copying bytelatent/preprocess/data_pipeline.py -> build/lib/bytelatent/preprocess',\n",
              " 'copying bytelatent/preprocess/__init__.py -> build/lib/bytelatent/preprocess',\n",
              " 'copying bytelatent/tokenizers/constants.py -> build/lib/bytelatent/tokenizers',\n",
              " 'copying bytelatent/tokenizers/blt_tokenizer.py -> build/lib/bytelatent/tokenizers',\n",
              " 'copying bytelatent/tokenizers/abstract_tokenizer.py -> build/lib/bytelatent/tokenizers',\n",
              " 'copying bytelatent/tokenizers/build_tokenizer.py -> build/lib/bytelatent/tokenizers',\n",
              " 'copying bytelatent/tokenizers/test_blt_tokenizer.py -> build/lib/bytelatent/tokenizers',\n",
              " 'copying bytelatent/tokenizers/sentence_piece_tokenizer.py -> build/lib/bytelatent/tokenizers',\n",
              " 'copying bytelatent/tokenizers/tiktoken_tokenizer.py -> build/lib/bytelatent/tokenizers',\n",
              " 'copying bytelatent/tokenizers/__init__.py -> build/lib/bytelatent/tokenizers',\n",
              " 'copying bytelatent/data/ngram_processor.py -> build/lib/bytelatent/data',\n",
              " 'copying bytelatent/data/test_data.py -> build/lib/bytelatent/data',\n",
              " 'copying bytelatent/data/patcher.py -> build/lib/bytelatent/data',\n",
              " 'copying bytelatent/data/data_types.py -> build/lib/bytelatent/data',\n",
              " 'copying bytelatent/data/file_util.py -> build/lib/bytelatent/data',\n",
              " 'copying bytelatent/data/__init__.py -> build/lib/bytelatent/data',\n",
              " 'copying bytelatent/plotting/scaling_figures.py -> build/lib/bytelatent/plotting',\n",
              " 'copying bytelatent/plotting/__init__.py -> build/lib/bytelatent/plotting',\n",
              " 'copying bytelatent/plotting/entropy_figure.py -> build/lib/bytelatent/plotting',\n",
              " 'copying bytelatent/model/utils.py -> build/lib/bytelatent/model',\n",
              " 'copying bytelatent/model/latent_transformer.py -> build/lib/bytelatent/model',\n",
              " 'copying bytelatent/model/blt.py -> build/lib/bytelatent/model',\n",
              " 'copying bytelatent/model/local_models.py -> build/lib/bytelatent/model',\n",
              " 'copying bytelatent/model/__init__.py -> build/lib/bytelatent/model',\n",
              " 'copying bytelatent/data/iterators/test_iters.py -> build/lib/bytelatent/data/iterators',\n",
              " 'copying bytelatent/data/iterators/sequence_iterator.py -> build/lib/bytelatent/data/iterators',\n",
              " 'copying bytelatent/data/iterators/looping_iterator.py -> build/lib/bytelatent/data/iterators',\n",
              " 'copying bytelatent/data/iterators/sampling_iterator.py -> build/lib/bytelatent/data/iterators',\n",
              " 'copying bytelatent/data/iterators/packing_iterator.py -> build/lib/bytelatent/data/iterators',\n",
              " 'copying bytelatent/data/iterators/test_packing_iterator.py -> build/lib/bytelatent/data/iterators',\n",
              " 'copying bytelatent/data/iterators/test_limit_iterator.py -> build/lib/bytelatent/data/iterators',\n",
              " 'copying bytelatent/data/iterators/limit_iterator.py -> build/lib/bytelatent/data/iterators',\n",
              " 'copying bytelatent/data/iterators/preprocess_iterator.py -> build/lib/bytelatent/data/iterators',\n",
              " 'copying bytelatent/data/iterators/arrow_iterator.py -> build/lib/bytelatent/data/iterators',\n",
              " 'copying bytelatent/data/iterators/test_arrow_iterator.py -> build/lib/bytelatent/data/iterators',\n",
              " 'copying bytelatent/data/iterators/abstract_iterator.py -> build/lib/bytelatent/data/iterators',\n",
              " 'copying bytelatent/data/iterators/dev_iterators.py -> build/lib/bytelatent/data/iterators',\n",
              " 'copying bytelatent/data/iterators/multiprocess_iterator.py -> build/lib/bytelatent/data/iterators',\n",
              " 'copying bytelatent/data/iterators/__init__.py -> build/lib/bytelatent/data/iterators',\n",
              " 'copying apps/main/lingua_train.py -> build/lib/apps/main',\n",
              " 'copying apps/main/__init__.py -> build/lib/apps/main',\n",
              " 'creating build/bdist.linux-x86_64/egg',\n",
              " 'creating build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'creating build/bdist.linux-x86_64/egg/bytelatent/preprocess',\n",
              " 'copying build/lib/bytelatent/preprocess/preprocess_entropies.py -> build/bdist.linux-x86_64/egg/bytelatent/preprocess',\n",
              " 'copying build/lib/bytelatent/preprocess/fsspec_target.py -> build/bdist.linux-x86_64/egg/bytelatent/preprocess',\n",
              " 'copying build/lib/bytelatent/preprocess/parallel_entropies.py -> build/bdist.linux-x86_64/egg/bytelatent/preprocess',\n",
              " 'copying build/lib/bytelatent/preprocess/data_pipeline.py -> build/bdist.linux-x86_64/egg/bytelatent/preprocess',\n",
              " 'copying build/lib/bytelatent/preprocess/__init__.py -> build/bdist.linux-x86_64/egg/bytelatent/preprocess',\n",
              " 'copying build/lib/bytelatent/generate.py -> build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'copying build/lib/bytelatent/constants.py -> build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'creating build/bdist.linux-x86_64/egg/bytelatent/tokenizers',\n",
              " 'copying build/lib/bytelatent/tokenizers/constants.py -> build/bdist.linux-x86_64/egg/bytelatent/tokenizers',\n",
              " 'copying build/lib/bytelatent/tokenizers/blt_tokenizer.py -> build/bdist.linux-x86_64/egg/bytelatent/tokenizers',\n",
              " 'copying build/lib/bytelatent/tokenizers/abstract_tokenizer.py -> build/bdist.linux-x86_64/egg/bytelatent/tokenizers',\n",
              " 'copying build/lib/bytelatent/tokenizers/build_tokenizer.py -> build/bdist.linux-x86_64/egg/bytelatent/tokenizers',\n",
              " 'copying build/lib/bytelatent/tokenizers/test_blt_tokenizer.py -> build/bdist.linux-x86_64/egg/bytelatent/tokenizers',\n",
              " 'copying build/lib/bytelatent/tokenizers/sentence_piece_tokenizer.py -> build/bdist.linux-x86_64/egg/bytelatent/tokenizers',\n",
              " 'copying build/lib/bytelatent/tokenizers/tiktoken_tokenizer.py -> build/bdist.linux-x86_64/egg/bytelatent/tokenizers',\n",
              " 'copying build/lib/bytelatent/tokenizers/__init__.py -> build/bdist.linux-x86_64/egg/bytelatent/tokenizers',\n",
              " 'creating build/bdist.linux-x86_64/egg/bytelatent/data',\n",
              " 'copying build/lib/bytelatent/data/ngram_processor.py -> build/bdist.linux-x86_64/egg/bytelatent/data',\n",
              " 'copying build/lib/bytelatent/data/test_data.py -> build/bdist.linux-x86_64/egg/bytelatent/data',\n",
              " 'copying build/lib/bytelatent/data/patcher.py -> build/bdist.linux-x86_64/egg/bytelatent/data',\n",
              " 'copying build/lib/bytelatent/data/data_types.py -> build/bdist.linux-x86_64/egg/bytelatent/data',\n",
              " 'creating build/bdist.linux-x86_64/egg/bytelatent/data/iterators',\n",
              " 'copying build/lib/bytelatent/data/iterators/test_iters.py -> build/bdist.linux-x86_64/egg/bytelatent/data/iterators',\n",
              " 'copying build/lib/bytelatent/data/iterators/sequence_iterator.py -> build/bdist.linux-x86_64/egg/bytelatent/data/iterators',\n",
              " 'copying build/lib/bytelatent/data/iterators/looping_iterator.py -> build/bdist.linux-x86_64/egg/bytelatent/data/iterators',\n",
              " 'copying build/lib/bytelatent/data/iterators/sampling_iterator.py -> build/bdist.linux-x86_64/egg/bytelatent/data/iterators',\n",
              " 'copying build/lib/bytelatent/data/iterators/packing_iterator.py -> build/bdist.linux-x86_64/egg/bytelatent/data/iterators',\n",
              " 'copying build/lib/bytelatent/data/iterators/test_packing_iterator.py -> build/bdist.linux-x86_64/egg/bytelatent/data/iterators',\n",
              " 'copying build/lib/bytelatent/data/iterators/test_limit_iterator.py -> build/bdist.linux-x86_64/egg/bytelatent/data/iterators',\n",
              " 'copying build/lib/bytelatent/data/iterators/limit_iterator.py -> build/bdist.linux-x86_64/egg/bytelatent/data/iterators',\n",
              " 'copying build/lib/bytelatent/data/iterators/preprocess_iterator.py -> build/bdist.linux-x86_64/egg/bytelatent/data/iterators',\n",
              " 'copying build/lib/bytelatent/data/iterators/arrow_iterator.py -> build/bdist.linux-x86_64/egg/bytelatent/data/iterators',\n",
              " 'copying build/lib/bytelatent/data/iterators/test_arrow_iterator.py -> build/bdist.linux-x86_64/egg/bytelatent/data/iterators',\n",
              " 'copying build/lib/bytelatent/data/iterators/abstract_iterator.py -> build/bdist.linux-x86_64/egg/bytelatent/data/iterators',\n",
              " 'copying build/lib/bytelatent/data/iterators/dev_iterators.py -> build/bdist.linux-x86_64/egg/bytelatent/data/iterators',\n",
              " 'copying build/lib/bytelatent/data/iterators/multiprocess_iterator.py -> build/bdist.linux-x86_64/egg/bytelatent/data/iterators',\n",
              " 'copying build/lib/bytelatent/data/iterators/__init__.py -> build/bdist.linux-x86_64/egg/bytelatent/data/iterators',\n",
              " 'copying build/lib/bytelatent/data/file_util.py -> build/bdist.linux-x86_64/egg/bytelatent/data',\n",
              " 'copying build/lib/bytelatent/data/__init__.py -> build/bdist.linux-x86_64/egg/bytelatent/data',\n",
              " 'copying build/lib/bytelatent/entropy_model.py -> build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'copying build/lib/bytelatent/base_transformer.py -> build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'copying build/lib/bytelatent/logger.py -> build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'copying build/lib/bytelatent/probe.py -> build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'copying build/lib/bytelatent/test_entropy_model.py -> build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'creating build/bdist.linux-x86_64/egg/bytelatent/plotting',\n",
              " 'copying build/lib/bytelatent/plotting/scaling_figures.py -> build/bdist.linux-x86_64/egg/bytelatent/plotting',\n",
              " 'copying build/lib/bytelatent/plotting/__init__.py -> build/bdist.linux-x86_64/egg/bytelatent/plotting',\n",
              " 'copying build/lib/bytelatent/plotting/entropy_figure.py -> build/bdist.linux-x86_64/egg/bytelatent/plotting',\n",
              " 'copying build/lib/bytelatent/transformer.py -> build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'copying build/lib/bytelatent/profiling.py -> build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'copying build/lib/bytelatent/norms.py -> build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'copying build/lib/bytelatent/print_config.py -> build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'copying build/lib/bytelatent/metrics.py -> build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'copying build/lib/bytelatent/test_config_parser.py -> build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'creating build/bdist.linux-x86_64/egg/bytelatent/model',\n",
              " 'copying build/lib/bytelatent/model/utils.py -> build/bdist.linux-x86_64/egg/bytelatent/model',\n",
              " 'copying build/lib/bytelatent/model/latent_transformer.py -> build/bdist.linux-x86_64/egg/bytelatent/model',\n",
              " 'copying build/lib/bytelatent/model/blt.py -> build/bdist.linux-x86_64/egg/bytelatent/model',\n",
              " 'copying build/lib/bytelatent/model/local_models.py -> build/bdist.linux-x86_64/egg/bytelatent/model',\n",
              " 'copying build/lib/bytelatent/model/__init__.py -> build/bdist.linux-x86_64/egg/bytelatent/model',\n",
              " 'copying build/lib/bytelatent/eval.py -> build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'copying build/lib/bytelatent/iterate_data.py -> build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'copying build/lib/bytelatent/args.py -> build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'copying build/lib/bytelatent/stool.py -> build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'copying build/lib/bytelatent/config_parser.py -> build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'copying build/lib/bytelatent/float8.py -> build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'copying build/lib/bytelatent/train.py -> build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'copying build/lib/bytelatent/checkpoint.py -> build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'copying build/lib/bytelatent/__init__.py -> build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'copying build/lib/bytelatent/generate_blt.py -> build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'copying build/lib/bytelatent/distributed.py -> build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'copying build/lib/bytelatent/test_blt.py -> build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'copying build/lib/bytelatent/optim.py -> build/bdist.linux-x86_64/egg/bytelatent',\n",
              " 'creating build/bdist.linux-x86_64/egg/apps',\n",
              " 'creating build/bdist.linux-x86_64/egg/apps/main',\n",
              " 'copying build/lib/apps/main/lingua_train.py -> build/bdist.linux-x86_64/egg/apps/main',\n",
              " 'copying build/lib/apps/main/__init__.py -> build/bdist.linux-x86_64/egg/apps/main',\n",
              " 'copying build/lib/apps/__init__.py -> build/bdist.linux-x86_64/egg/apps',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/preprocess/preprocess_entropies.py to preprocess_entropies.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/preprocess/fsspec_target.py to fsspec_target.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/preprocess/parallel_entropies.py to parallel_entropies.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/preprocess/data_pipeline.py to data_pipeline.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/preprocess/__init__.py to __init__.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/generate.py to generate.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/constants.py to constants.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/tokenizers/constants.py to constants.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/tokenizers/blt_tokenizer.py to blt_tokenizer.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/tokenizers/abstract_tokenizer.py to abstract_tokenizer.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/tokenizers/build_tokenizer.py to build_tokenizer.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/tokenizers/test_blt_tokenizer.py to test_blt_tokenizer.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/tokenizers/sentence_piece_tokenizer.py to sentence_piece_tokenizer.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/tokenizers/tiktoken_tokenizer.py to tiktoken_tokenizer.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/tokenizers/__init__.py to __init__.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/data/ngram_processor.py to ngram_processor.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/data/test_data.py to test_data.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/data/patcher.py to patcher.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/data/data_types.py to data_types.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/data/iterators/test_iters.py to test_iters.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/data/iterators/sequence_iterator.py to sequence_iterator.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/data/iterators/looping_iterator.py to looping_iterator.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/data/iterators/sampling_iterator.py to sampling_iterator.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/data/iterators/packing_iterator.py to packing_iterator.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/data/iterators/test_packing_iterator.py to test_packing_iterator.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/data/iterators/test_limit_iterator.py to test_limit_iterator.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/data/iterators/limit_iterator.py to limit_iterator.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/data/iterators/preprocess_iterator.py to preprocess_iterator.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/data/iterators/arrow_iterator.py to arrow_iterator.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/data/iterators/test_arrow_iterator.py to test_arrow_iterator.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/data/iterators/abstract_iterator.py to abstract_iterator.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/data/iterators/dev_iterators.py to dev_iterators.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/data/iterators/multiprocess_iterator.py to multiprocess_iterator.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/data/iterators/__init__.py to __init__.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/data/file_util.py to file_util.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/data/__init__.py to __init__.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/entropy_model.py to entropy_model.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/base_transformer.py to base_transformer.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/logger.py to logger.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/probe.py to probe.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/test_entropy_model.py to test_entropy_model.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/plotting/scaling_figures.py to scaling_figures.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/plotting/__init__.py to __init__.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/plotting/entropy_figure.py to entropy_figure.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/transformer.py to transformer.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/profiling.py to profiling.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/norms.py to norms.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/print_config.py to print_config.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/metrics.py to metrics.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/test_config_parser.py to test_config_parser.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/model/utils.py to utils.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/model/latent_transformer.py to latent_transformer.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/model/blt.py to blt.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/model/local_models.py to local_models.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/model/__init__.py to __init__.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/eval.py to eval.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/iterate_data.py to iterate_data.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/args.py to args.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/stool.py to stool.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/config_parser.py to config_parser.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/float8.py to float8.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/train.py to train.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/checkpoint.py to checkpoint.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/__init__.py to __init__.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/generate_blt.py to generate_blt.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/distributed.py to distributed.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/test_blt.py to test_blt.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/bytelatent/optim.py to optim.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/apps/main/lingua_train.py to lingua_train.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/apps/main/__init__.py to __init__.cpython-311.pyc',\n",
              " 'byte-compiling build/bdist.linux-x86_64/egg/apps/__init__.py to __init__.cpython-311.pyc',\n",
              " 'creating build/bdist.linux-x86_64/egg/EGG-INFO',\n",
              " 'copying bytelatent.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO',\n",
              " 'copying bytelatent.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO',\n",
              " 'copying bytelatent.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO',\n",
              " 'copying bytelatent.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO',\n",
              " 'copying bytelatent.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO',\n",
              " 'zip_safe flag not set; analyzing archive contents...',\n",
              " 'bytelatent.__pycache__.profiling.cpython-311: module references __file__',\n",
              " 'creating dist',\n",
              " \"creating 'dist/bytelatent-0.1.0-py3.11.egg' and adding 'build/bdist.linux-x86_64/egg' to it\",\n",
              " \"removing 'build/bdist.linux-x86_64/egg' (and everything under it)\",\n",
              " 'Processing bytelatent-0.1.0-py3.11.egg',\n",
              " 'creating /usr/local/lib/python3.11/dist-packages/bytelatent-0.1.0-py3.11.egg',\n",
              " 'Extracting bytelatent-0.1.0-py3.11.egg to /usr/local/lib/python3.11/dist-packages',\n",
              " 'Adding bytelatent 0.1.0 to easy-install.pth file',\n",
              " '',\n",
              " 'Installed /usr/local/lib/python3.11/dist-packages/bytelatent-0.1.0-py3.11.egg',\n",
              " 'Processing dependencies for bytelatent==0.1.0',\n",
              " 'Searching for xformers==0.0.28.post2',\n",
              " 'Best match: xformers 0.0.28.post2',\n",
              " 'Adding xformers 0.0.28.post2 to easy-install.pth file',\n",
              " \"detected new path './bytelatent-0.1.0-py3.11.egg'\",\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for tiktoken==0.9.0',\n",
              " 'Best match: tiktoken 0.9.0',\n",
              " 'Adding tiktoken 0.9.0 to easy-install.pth file',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for sentencepiece==0.2.0',\n",
              " 'Best match: sentencepiece 0.2.0',\n",
              " 'Adding sentencepiece 0.2.0 to easy-install.pth file',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for torch==2.5.0+cu121',\n",
              " 'Best match: torch 2.5.0+cu121',\n",
              " 'Adding torch 2.5.0+cu121 to easy-install.pth file',\n",
              " 'Installing convert-caffe2-to-onnx script to /usr/local/bin',\n",
              " 'Installing convert-onnx-to-caffe2 script to /usr/local/bin',\n",
              " 'Installing torchfrtrace script to /usr/local/bin',\n",
              " 'Installing torchrun script to /usr/local/bin',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for numpy==2.0.2',\n",
              " 'Best match: numpy 2.0.2',\n",
              " 'Adding numpy 2.0.2 to easy-install.pth file',\n",
              " 'Installing f2py script to /usr/local/bin',\n",
              " 'Installing numpy-config script to /usr/local/bin',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for requests==2.32.3',\n",
              " 'Best match: requests 2.32.3',\n",
              " 'Adding requests 2.32.3 to easy-install.pth file',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for regex==2024.11.6',\n",
              " 'Best match: regex 2024.11.6',\n",
              " 'Adding regex 2024.11.6 to easy-install.pth file',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for sympy==1.13.1',\n",
              " 'Best match: sympy 1.13.1',\n",
              " 'Adding sympy 1.13.1 to easy-install.pth file',\n",
              " 'Installing isympy script to /usr/local/bin',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for triton==3.1.0',\n",
              " 'Best match: triton 3.1.0',\n",
              " 'Adding triton 3.1.0 to easy-install.pth file',\n",
              " 'Installing proton script to /usr/local/bin',\n",
              " 'Installing proton-viewer script to /usr/local/bin',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for nvidia-nvtx-cu12==12.1.105',\n",
              " 'Best match: nvidia-nvtx-cu12 12.1.105',\n",
              " 'Adding nvidia-nvtx-cu12 12.1.105 to easy-install.pth file',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for nvidia-nccl-cu12==2.21.5',\n",
              " 'Best match: nvidia-nccl-cu12 2.21.5',\n",
              " 'Adding nvidia-nccl-cu12 2.21.5 to easy-install.pth file',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for nvidia-cusparse-cu12==12.1.0.106',\n",
              " 'Best match: nvidia-cusparse-cu12 12.1.0.106',\n",
              " 'Adding nvidia-cusparse-cu12 12.1.0.106 to easy-install.pth file',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for nvidia-cusolver-cu12==11.4.5.107',\n",
              " 'Best match: nvidia-cusolver-cu12 11.4.5.107',\n",
              " 'Adding nvidia-cusolver-cu12 11.4.5.107 to easy-install.pth file',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for nvidia-curand-cu12==10.3.2.106',\n",
              " 'Best match: nvidia-curand-cu12 10.3.2.106',\n",
              " 'Adding nvidia-curand-cu12 10.3.2.106 to easy-install.pth file',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for nvidia-cufft-cu12==11.0.2.54',\n",
              " 'Best match: nvidia-cufft-cu12 11.0.2.54',\n",
              " 'Adding nvidia-cufft-cu12 11.0.2.54 to easy-install.pth file',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for nvidia-cublas-cu12==12.1.3.1',\n",
              " 'Best match: nvidia-cublas-cu12 12.1.3.1',\n",
              " 'Adding nvidia-cublas-cu12 12.1.3.1 to easy-install.pth file',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for nvidia-cudnn-cu12==9.1.0.70',\n",
              " 'Best match: nvidia-cudnn-cu12 9.1.0.70',\n",
              " 'Adding nvidia-cudnn-cu12 9.1.0.70 to easy-install.pth file',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for nvidia-cuda-cupti-cu12==12.1.105',\n",
              " 'Best match: nvidia-cuda-cupti-cu12 12.1.105',\n",
              " 'Adding nvidia-cuda-cupti-cu12 12.1.105 to easy-install.pth file',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for nvidia-cuda-runtime-cu12==12.1.105',\n",
              " 'Best match: nvidia-cuda-runtime-cu12 12.1.105',\n",
              " 'Adding nvidia-cuda-runtime-cu12 12.1.105 to easy-install.pth file',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for nvidia-cuda-nvrtc-cu12==12.1.105',\n",
              " 'Best match: nvidia-cuda-nvrtc-cu12 12.1.105',\n",
              " 'Adding nvidia-cuda-nvrtc-cu12 12.1.105 to easy-install.pth file',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for fsspec==2024.12.0',\n",
              " 'Best match: fsspec 2024.12.0',\n",
              " 'Adding fsspec 2024.12.0 to easy-install.pth file',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for jinja2==3.1.6',\n",
              " 'Best match: jinja2 3.1.6',\n",
              " 'Adding jinja2 3.1.6 to easy-install.pth file',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for networkx==3.4.2',\n",
              " 'Best match: networkx 3.4.2',\n",
              " 'Adding networkx 3.4.2 to easy-install.pth file',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for typing-extensions==4.13.2',\n",
              " 'Best match: typing-extensions 4.13.2',\n",
              " 'Adding typing-extensions 4.13.2 to easy-install.pth file',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for filelock==3.18.0',\n",
              " 'Best match: filelock 3.18.0',\n",
              " 'Adding filelock 3.18.0 to easy-install.pth file',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for certifi==2025.1.31',\n",
              " 'Best match: certifi 2025.1.31',\n",
              " 'Adding certifi 2025.1.31 to easy-install.pth file',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for urllib3==2.3.0',\n",
              " 'Best match: urllib3 2.3.0',\n",
              " 'Adding urllib3 2.3.0 to easy-install.pth file',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for idna==3.10',\n",
              " 'Best match: idna 3.10',\n",
              " 'Adding idna 3.10 to easy-install.pth file',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for charset-normalizer==3.4.1',\n",
              " 'Best match: charset-normalizer 3.4.1',\n",
              " 'Adding charset-normalizer 3.4.1 to easy-install.pth file',\n",
              " 'Installing normalizer script to /usr/local/bin',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for mpmath==1.3.0',\n",
              " 'Best match: mpmath 1.3.0',\n",
              " 'Adding mpmath 1.3.0 to easy-install.pth file',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for nvidia-nvjitlink-cu12==12.5.82',\n",
              " 'Best match: nvidia-nvjitlink-cu12 12.5.82',\n",
              " 'Adding nvidia-nvjitlink-cu12 12.5.82 to easy-install.pth file',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Searching for MarkupSafe==3.0.2',\n",
              " 'Best match: MarkupSafe 3.0.2',\n",
              " 'Adding MarkupSafe 3.0.2 to easy-install.pth file',\n",
              " '',\n",
              " 'Using /usr/local/lib/python3.11/dist-packages',\n",
              " 'Finished processing dependencies for bytelatent==0.1.0']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "7wPFCOSyagQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python demo2.py \"A BLT has\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZuutjKKZv3j",
        "outputId": "4452d815-e86b-4cb1-c254-e40f4b1e3341"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. Using device: cuda\n",
            "Loading BLT model: blt-1b\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python demo3.py \"A BLT has\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPTmNVs7Z9Mi",
        "outputId": "0ff80b55-c95e-4650-d6ad-75e7165cfcc0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. Primary device requested: cuda\n",
            "Loading BLT model: blt-1b with device_map='auto'\n",
            "Failed to load model with device_map='auto'. Error: load_consolidated_model_and_tokenizer() got an unexpected keyword argument 'device_map'\n",
            "Attempting to load without device_map, but explicitly on GPU (might fail if too large).\n",
            "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/blt/\u001b[0m\u001b[1;33mdemo3.py\u001b[0m:\u001b[94m36\u001b[0m in \u001b[92mmain\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 33 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 34 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# --- تعديل التحميل ---\u001b[0m                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 35 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mtry\u001b[0m:                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 36 \u001b[2m│   │   \u001b[0mmodel, tokenizer, train_cfg = load_consolidated_model_and_toke \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 37 \u001b[0m\u001b[2m│   │   │   \u001b[0mcheckpoint_path,                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 38 \u001b[0m\u001b[2m│   │   │   \u001b[0mdevice_map=\u001b[33m\"\u001b[0m\u001b[33mauto\u001b[0m\u001b[33m\"\u001b[0m,  \u001b[2m# <<< إضافة device_map\u001b[0m                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 39 \u001b[0m\u001b[2m│   │   │   \u001b[0mtorch_dtype=torch.float16 \u001b[2m# <<< جرب تحميل النموذج بـ float\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m──────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m───────────────\u001b[0m\u001b[33m─╮\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m checkpoint_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          device = \u001b[1;35mdevice\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtype\u001b[0m=\u001b[33m'cuda'\u001b[0m\u001b[1m)\u001b[0m \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m      model_name = \u001b[33m'blt-1b'\u001b[0m            \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          prompt = \u001b[33m'A BLT has'\u001b[0m         \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰───────────────────────────────────────╯\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[1;91mTypeError: \u001b[0m\u001b[1;35mload_consolidated_model_and_tokenizer\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m got an unexpected keyword \n",
            "argument \u001b[32m'device_map'\u001b[0m\n",
            "\n",
            "\u001b[3mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
            "\n",
            "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/blt/\u001b[0m\u001b[1;33mdemo3.py\u001b[0m:\u001b[94m52\u001b[0m in \u001b[92mmain\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 49 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mFailed to load model with device_map=\u001b[0m\u001b[33m'\u001b[0m\u001b[33mauto\u001b[0m\u001b[33m'\u001b[0m\u001b[33m. Error: \u001b[0m\u001b[33m{\u001b[0me \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 50 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mAttempting to load without device_map, but explicitly o\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 51 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# العودة للخطة البديلة (إذا فشل device_map)\u001b[0m                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 52 \u001b[2m│   │   \u001b[0mmodel, tokenizer, train_cfg = \u001b[1;4mload_consolidated_model_and_toke\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 53 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4mcheckpoint_path,\u001b[0m                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 54 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4mtorch_dtype=torch.float16 \u001b[0m\u001b[1;2;4m# <<< حافظ على float16\u001b[0m           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 55 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4m)\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m──────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m───────────────\u001b[0m\u001b[33m─╮\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m checkpoint_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          device = \u001b[1;35mdevice\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtype\u001b[0m=\u001b[33m'cuda'\u001b[0m\u001b[1m)\u001b[0m \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m      model_name = \u001b[33m'blt-1b'\u001b[0m            \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          prompt = \u001b[33m'A BLT has'\u001b[0m         \u001b[33m│\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰───────────────────────────────────────╯\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[1;91mTypeError: \u001b[0m\u001b[1;35mload_consolidated_model_and_tokenizer\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m got an unexpected keyword \n",
            "argument \u001b[32m'torch_dtype'\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python demo4.py \"hi\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0VHDSSjbKt0",
        "outputId": "29f2b2dd-a067-49b7-8305-8791cb6a0227"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. Using device: cuda\n",
            "Loading BLT model: blt-1b (will load on CPU first)\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import typer\n",
        "\n",
        "# لا نحتاج accelerate إذا كنا لن نستخدم device_map بنجاح\n",
        "# from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
        "\n",
        "from bytelatent.distributed import DistributedArgs, setup_torch_distributed\n",
        "from bytelatent.generate import load_consolidated_model_and_tokenizer\n",
        "from bytelatent.generate_blt import generate_nocache\n",
        "from bytelatent.model.blt import ByteLatentTransformer\n",
        "from bytelatent.tokenizers.blt_tokenizer import BltTokenizer\n",
        "\n",
        "def main(prompt: str, model_name: str = \"blt-1b\"):\n",
        "    # --- تبسيط أو إزالة جزء Distributed لـ Colab GPU واحد ---\n",
        "    # distributed_args = DistributedArgs()\n",
        "    # distributed_args.configure_world()\n",
        "    # if not torch.distributed.is_initialized():\n",
        "    #     setup_torch_distributed(distributed_args)\n",
        "    # --- نهاية التبسيط المقترح ---\n",
        "\n",
        "    # --- تحديد الجهاز ---\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        print(f\"CUDA is available. Using device: {device}\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        print(\"CUDA not available. Using CPU.\")\n",
        "    # --- نهاية تحديد الجهاز ---\n",
        "\n",
        "    checkpoint_path = os.path.join(\"hf-weights\", model_name)\n",
        "    print(f\"Loading BLT model: {model_name} (will load on CPU first)\")\n",
        "\n",
        "    # --- تحميل النموذج بدون وسائط إضافية ---\n",
        "    # ستقوم هذه الدالة غالبًا بتحميل النموذج على CPU وبدقة float32\n",
        "    try:\n",
        "        model, tokenizer, train_cfg = load_consolidated_model_and_tokenizer(\n",
        "            checkpoint_path,\n",
        "        )\n",
        "        print(\"Model loaded successfully (likely on CPU, fp32).\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load model using load_consolidated_model_and_tokenizer. Error: {e}\")\n",
        "        return # لا يمكن المتابعة إذا فشل التحميل الأساسي\n",
        "\n",
        "    assert isinstance(model, ByteLatentTransformer)\n",
        "    assert isinstance(tokenizer, BltTokenizer)\n",
        "\n",
        "    # --- المعالجة اليدوية بعد التحميل ---\n",
        "    try:\n",
        "        print(\"Converting model to float16...\")\n",
        "        model = model.half() # <<< تحويل إلى نصف دقة\n",
        "        print(\"Moving model to GPU...\")\n",
        "        model.to(device)     # <<< نقل إلى GPU\n",
        "        model.eval()         # <<< وضع التقييم\n",
        "        # تحقق من الجهاز والدقة\n",
        "        print(f\"Model is now on device: {next(model.parameters()).device} with dtype: {next(model.parameters()).dtype}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed during model conversion/moving to GPU. Error: {e}\")\n",
        "        print(\"This might be due to insufficient GPU RAM even for float16.\")\n",
        "        return\n",
        "    # --- نهاية المعالجة اليدوية ---\n",
        "\n",
        "\n",
        "    patcher_args = train_cfg.data.patcher_args.model_copy(deep=True)\n",
        "    patcher_args.realtime_patching = True\n",
        "    print(\"Loading entropy model and patcher\")\n",
        "    patcher_args.entropy_model_checkpoint_dir = os.path.join(\n",
        "        checkpoint_path, \"entropy_model\"\n",
        "    )\n",
        "    patcher = patcher_args.build()\n",
        "\n",
        "    # --- محاولة نقل Patcher ---\n",
        "    try:\n",
        "        # لا يزال من الجيد محاولة نقل Patcher\n",
        "        patcher.to(device)\n",
        "        print(f\"Patcher moved to device: {device}\")\n",
        "         # يمكنك إضافة تحقق أدق إذا كان patcher يحتوي على parameters\n",
        "        # if hasattr(patcher, 'parameters') and list(patcher.parameters()):\n",
        "        #    print(f\"Patcher first param device: {next(patcher.parameters()).device}\")\n",
        "    except Exception as e: # استخدام Exception أعم للالتقاط\n",
        "        print(f\"Could not move patcher to device (may not be necessary). Error: {e}\")\n",
        "    # --- نهاية نقل Patcher ---\n",
        "\n",
        "    prompts = [prompt]\n",
        "    print(f\"Generating completion for prompt on device: {device}...\")\n",
        "\n",
        "    # --- استدعاء generate_nocache مع تمرير الجهاز ---\n",
        "    # لا يزال يعتمد على أن هذه الدالة ستستخدم الجهاز للمدخلات\n",
        "    try:\n",
        "        with torch.no_grad(): # مهم للاستدلال لتوفير الذاكرة وتسريع الحسابات\n",
        "            outputs = generate_nocache(\n",
        "                prompts, model=model, tokenizer=tokenizer, patcher=patcher, device=device # <<< مرر device\n",
        "            )\n",
        "    except RuntimeError as e:\n",
        "         print(f\"\\n!!! Runtime Error during generation: {e}\")\n",
        "         print(\"!!! This often means tensors are on different devices (CPU vs GPU).\")\n",
        "         print(\"!!! Check if generate_nocache correctly uses the 'device' argument for inputs.\")\n",
        "         return\n",
        "    except Exception as e:\n",
        "        print(f\"\\n!!! An unexpected error occurred during generation: {e}\")\n",
        "        return\n",
        "    # --- نهاية استدعاء generate_nocache ---\n",
        "\n",
        "    text_outputs = [tokenizer.decode(t) for t in outputs]\n",
        "    for p, t in zip(prompts, text_outputs):\n",
        "        print(f'Prompt: \"{p}\" Completion: \"{t}\"')\n",
        "        print()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    typer.run(main)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-6qlxLrIbpiE",
        "outputId": "9c12413a-efb3-4d4c-eb57-ded8cf1ec106"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[33mUsage: \u001b[0mcolab_kernel_launcher.py [OPTIONS] PROMPT\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Usage: </span>colab_kernel_launcher.py [OPTIONS] PROMPT\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2mTry \u001b[0m\u001b[2;34m'colab_kernel_launcher.py \u001b[0m\u001b[1;2;34m-\u001b[0m\u001b[1;2;34m-help\u001b[0m\u001b[2;34m'\u001b[0m\u001b[2m for help.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Try </span><span style=\"color: #7f7fbf; text-decoration-color: #7f7fbf\">'colab_kernel_launcher.py </span><span style=\"color: #7f7fbf; text-decoration-color: #7f7fbf; font-weight: bold\">--help</span><span style=\"color: #7f7fbf; text-decoration-color: #7f7fbf\">'</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> for help.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m Error \u001b[0m\u001b[31m────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m No such option: \u001b[1;32m-f\u001b[0m                                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─ Error ─────────────────────────────────────────────────────────────────────────────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> No such option: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">-f</span>                                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/parser.py\", line 516, in _process_opts\n",
            "    self._match_long_opt(norm_long_opt, explicit_value, state)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/parser.py\", line 400, in _match_long_opt\n",
            "    raise NoSuchOption(opt, possibilities=possibilities, ctx=self.ctx)\n",
            "click.exceptions.NoSuchOption: No such option: -f\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/typer/core.py\", line 194, in _main\n",
            "    with self.make_context(prog_name, args, **extra) as ctx:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 949, in make_context\n",
            "    self.parse_args(ctx, args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 1414, in parse_args\n",
            "    opts, args, param_order = parser.parse_args(args=args)\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/parser.py\", line 339, in parse_args\n",
            "    self._process_args_for_options(state)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/parser.py\", line 366, in _process_args_for_options\n",
            "    self._process_opts(arg, state)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/parser.py\", line 525, in _process_opts\n",
            "    self._match_short_opt(arg, state)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/parser.py\", line 438, in _match_short_opt\n",
            "    raise NoSuchOption(opt, ctx=self.ctx)\n",
            "click.exceptions.NoSuchOption: No such option: -f\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-13-1723cb561a9c>\", line 111, in <cell line: 0>\n",
            "    typer.run(main)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/typer/main.py\", line 1073, in run\n",
            "    app()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/typer/main.py\", line 322, in __call__\n",
            "    return get_command(self)(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 1161, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/typer/core.py\", line 677, in main\n",
            "    return _main(\n",
            "           ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/typer/core.py\", line 220, in _main\n",
            "    sys.exit(e.exit_code)\n",
            "SystemExit: 2\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1671, in getframeinfo\n",
            "    lineno = frame.f_lineno\n",
            "             ^^^^^^^^^^^^^^\n",
            "AttributeError: 'tuple' object has no attribute 'f_lineno'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNoSuchOption\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/click/parser.py\u001b[0m in \u001b[0;36m_process_opts\u001b[0;34m(self, arg, state)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_match_long_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_long_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mNoSuchOption\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/click/parser.py\u001b[0m in \u001b[0;36m_match_long_opt\u001b[0;34m(self, opt, explicit_value, state)\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0mpossibilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_close_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_long_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchOption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibilities\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpossibilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNoSuchOption\u001b[0m: No such option: -f",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNoSuchOption\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/typer/core.py\u001b[0m in \u001b[0;36m_main\u001b[0;34m(self, args, prog_name, complete_var, standalone_mode, windows_expand_args, rich_markup_mode, **extra)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m                 \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/click/core.py\u001b[0m in \u001b[0;36mmake_context\u001b[0;34m(self, info_name, args, parent, **extra)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/click/core.py\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m   1413\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m         \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_order\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/click/parser.py\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_args_for_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_args_for_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/click/parser.py\u001b[0m in \u001b[0;36m_process_args_for_options\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opt_prefixes\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marglen\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_interspersed_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/click/parser.py\u001b[0m in \u001b[0;36m_process_opts\u001b[0;34m(self, arg, state)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opt_prefixes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_match_short_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/click/parser.py\u001b[0m in \u001b[0;36m_match_short_opt\u001b[0;34m(self, arg, state)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchOption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtakes_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNoSuchOption\u001b[0m: No such option: -f",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-1723cb561a9c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mtyper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/typer/main.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(function)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m     \u001b[0mapp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/typer/main.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mget_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/click/core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0;34m\"\"\"Alias for :meth:`main`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/typer/core.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self, args, prog_name, complete_var, standalone_mode, windows_expand_args, **extra)\u001b[0m\n\u001b[1;32m    676\u001b[0m     ) -> Any:\n\u001b[0;32m--> 677\u001b[0;31m         return _main(\n\u001b[0m\u001b[1;32m    678\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/typer/core.py\u001b[0m in \u001b[0;36m_main\u001b[0;34m(self, args, prog_name, complete_var, standalone_mode, windows_expand_args, rich_markup_mode, **extra)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;31m# Typer override end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemExit\u001b[0m: 2",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     stb = ['An exception has occurred, use %tb to see '\n\u001b[1;32m   2091\u001b[0m                            'the full traceback.\\n']\n\u001b[0;32m-> 2092\u001b[0;31m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0m\u001b[1;32m   2093\u001b[0m                                                                      value))\n\u001b[1;32m   2094\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mchained_exceptions_tb_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             out_list = (\n\u001b[0;32m--> 629\u001b[0;31m                 self.structured_traceback(\n\u001b[0m\u001b[1;32m    630\u001b[0m                     \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0metb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchained_exc_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "source": [
        "import os\n",
        "import torch\n",
        "import typer\n",
        "\n",
        "# ... (rest of your imports and code) ...\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Handle potential extra arguments from Jupyter/IPython\n",
        "    import sys\n",
        "    try:\n",
        "        # Remove any arguments starting with '-' that Typer might not recognize\n",
        "        filtered_args = [arg for arg in sys.argv[1:] if not arg.startswith('-')]\n",
        "        typer.run(main, args=filtered_args)  # Pass filtered arguments to typer.run\n",
        "    except SystemExit as e:\n",
        "        # Handle SystemExit gracefully, avoiding potential TypeError\n",
        "        if e.code != 0:\n",
        "            print(f\"Typer exited with code: {e.code}. This might be due to unrecognized arguments.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "aypFfTqpcQ_9",
        "outputId": "d95f90ed-3331-42e1-df22-04d75a523e9c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "run() got an unexpected keyword argument 'args'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-1f584ec5380a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Remove any arguments starting with '-' that Typer might not recognize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mfiltered_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtyper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiltered_args\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Pass filtered arguments to typer.run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mSystemExit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Handle SystemExit gracefully, avoiding potential TypeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: run() got an unexpected keyword argument 'args'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import typer\n",
        "\n",
        "from bytelatent.distributed import DistributedArgs, setup_torch_distributed\n",
        "from bytelatent.generate import load_consolidated_model_and_tokenizer\n",
        "from bytelatent.generate_blt import generate_nocache\n",
        "from bytelatent.model.blt import ByteLatentTransformer\n",
        "from bytelatent.tokenizers.blt_tokenizer import BltTokenizer\n",
        "\n",
        "\n",
        "def main(prompt: str, model_name: str = \"blt-1b\"):\n",
        "    distributed_args = DistributedArgs()\n",
        "    distributed_args.configure_world()\n",
        "    if not torch.distributed.is_initialized():\n",
        "        setup_torch_distributed(distributed_args)\n",
        "    checkpoint_path = os.path.join(\"hf-weights\", model_name)\n",
        "    print(f\"Loading BLT model: {model_name}\")\n",
        "    model, tokenizer, train_cfg = load_consolidated_model_and_tokenizer(\n",
        "        checkpoint_path,\n",
        "    )\n",
        "    assert isinstance(model, ByteLatentTransformer)\n",
        "    assert isinstance(tokenizer, BltTokenizer)\n",
        "    patcher_args = train_cfg.data.patcher_args.model_copy(deep=True)\n",
        "    patcher_args.realtime_patching = True\n",
        "    print(\"Loading entropy model and patcher\")\n",
        "    patcher_args.entropy_model_checkpoint_dir = os.path.join(\n",
        "        checkpoint_path, \"entropy_model\"\n",
        "    )\n",
        "    patcher = patcher_args.build()\n",
        "    prompts = [prompt]\n",
        "    outputs = generate_nocache(\n",
        "        prompts, model=model, tokenizer=tokenizer, patcher=patcher\n",
        "    )\n",
        "    text_outputs = [tokenizer.decode(t) for t in outputs]\n",
        "    for p, t in zip(prompts, text_outputs):\n",
        "        print(f'Prompt: \"{p}\" Completion: \"{t}\"')\n",
        "        print()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    typer.run(main)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nlxe1onbcWer",
        "outputId": "4c62aa89-c6a6-4d53-dcdf-ebdcda3f6054"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[33mUsage: \u001b[0mcolab_kernel_launcher.py [OPTIONS] PROMPT\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Usage: </span>colab_kernel_launcher.py [OPTIONS] PROMPT\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2mTry \u001b[0m\u001b[2;34m'colab_kernel_launcher.py \u001b[0m\u001b[1;2;34m-\u001b[0m\u001b[1;2;34m-help\u001b[0m\u001b[2;34m'\u001b[0m\u001b[2m for help.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Try </span><span style=\"color: #7f7fbf; text-decoration-color: #7f7fbf\">'colab_kernel_launcher.py </span><span style=\"color: #7f7fbf; text-decoration-color: #7f7fbf; font-weight: bold\">--help</span><span style=\"color: #7f7fbf; text-decoration-color: #7f7fbf\">'</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> for help.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m Error \u001b[0m\u001b[31m────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m No such option: \u001b[1;32m-f\u001b[0m                                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─ Error ─────────────────────────────────────────────────────────────────────────────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> No such option: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">-f</span>                                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/parser.py\", line 516, in _process_opts\n",
            "    self._match_long_opt(norm_long_opt, explicit_value, state)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/parser.py\", line 400, in _match_long_opt\n",
            "    raise NoSuchOption(opt, possibilities=possibilities, ctx=self.ctx)\n",
            "click.exceptions.NoSuchOption: No such option: -f\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/typer/core.py\", line 194, in _main\n",
            "    with self.make_context(prog_name, args, **extra) as ctx:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 949, in make_context\n",
            "    self.parse_args(ctx, args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 1414, in parse_args\n",
            "    opts, args, param_order = parser.parse_args(args=args)\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/parser.py\", line 339, in parse_args\n",
            "    self._process_args_for_options(state)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/parser.py\", line 366, in _process_args_for_options\n",
            "    self._process_opts(arg, state)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/parser.py\", line 525, in _process_opts\n",
            "    self._match_short_opt(arg, state)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/parser.py\", line 438, in _match_short_opt\n",
            "    raise NoSuchOption(opt, ctx=self.ctx)\n",
            "click.exceptions.NoSuchOption: No such option: -f\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-15-2b9ad956fdf9>\", line 43, in <cell line: 0>\n",
            "    typer.run(main)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/typer/main.py\", line 1073, in run\n",
            "    app()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/typer/main.py\", line 322, in __call__\n",
            "    return get_command(self)(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 1161, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/typer/core.py\", line 677, in main\n",
            "    return _main(\n",
            "           ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/typer/core.py\", line 220, in _main\n",
            "    sys.exit(e.exit_code)\n",
            "SystemExit: 2\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1671, in getframeinfo\n",
            "    lineno = frame.f_lineno\n",
            "             ^^^^^^^^^^^^^^\n",
            "AttributeError: 'tuple' object has no attribute 'f_lineno'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNoSuchOption\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/click/parser.py\u001b[0m in \u001b[0;36m_process_opts\u001b[0;34m(self, arg, state)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_match_long_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_long_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mNoSuchOption\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/click/parser.py\u001b[0m in \u001b[0;36m_match_long_opt\u001b[0;34m(self, opt, explicit_value, state)\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0mpossibilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_close_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_long_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchOption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibilities\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpossibilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNoSuchOption\u001b[0m: No such option: -f",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNoSuchOption\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/typer/core.py\u001b[0m in \u001b[0;36m_main\u001b[0;34m(self, args, prog_name, complete_var, standalone_mode, windows_expand_args, rich_markup_mode, **extra)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m                 \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/click/core.py\u001b[0m in \u001b[0;36mmake_context\u001b[0;34m(self, info_name, args, parent, **extra)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/click/core.py\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m   1413\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m         \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_order\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/click/parser.py\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_args_for_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_args_for_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/click/parser.py\u001b[0m in \u001b[0;36m_process_args_for_options\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opt_prefixes\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marglen\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_interspersed_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/click/parser.py\u001b[0m in \u001b[0;36m_process_opts\u001b[0;34m(self, arg, state)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opt_prefixes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_match_short_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/click/parser.py\u001b[0m in \u001b[0;36m_match_short_opt\u001b[0;34m(self, arg, state)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchOption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtakes_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNoSuchOption\u001b[0m: No such option: -f",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-2b9ad956fdf9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mtyper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/typer/main.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(function)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m     \u001b[0mapp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/typer/main.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mget_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/click/core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0;34m\"\"\"Alias for :meth:`main`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/typer/core.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self, args, prog_name, complete_var, standalone_mode, windows_expand_args, **extra)\u001b[0m\n\u001b[1;32m    676\u001b[0m     ) -> Any:\n\u001b[0;32m--> 677\u001b[0;31m         return _main(\n\u001b[0m\u001b[1;32m    678\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/typer/core.py\u001b[0m in \u001b[0;36m_main\u001b[0;34m(self, args, prog_name, complete_var, standalone_mode, windows_expand_args, rich_markup_mode, **extra)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;31m# Typer override end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemExit\u001b[0m: 2",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     stb = ['An exception has occurred, use %tb to see '\n\u001b[1;32m   2091\u001b[0m                            'the full traceback.\\n']\n\u001b[0;32m-> 2092\u001b[0;31m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0m\u001b[1;32m   2093\u001b[0m                                                                      value))\n\u001b[1;32m   2094\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mchained_exceptions_tb_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             out_list = (\n\u001b[0;32m--> 629\u001b[0;31m                 self.structured_traceback(\n\u001b[0m\u001b[1;32m    630\u001b[0m                     \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0metb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchained_exc_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/blt\n",
        "\n",
        "# --- استيراد المكتبات اللازمة ---\n",
        "import os\n",
        "import torch\n",
        "# import typer # لا نحتاج typer في الخلية\n",
        "from bytelatent.distributed import DistributedArgs, setup_torch_distributed # قد لا نحتاجها\n",
        "from bytelatent.generate import load_consolidated_model_and_tokenizer\n",
        "from bytelatent.generate_blt import generate_nocache\n",
        "from bytelatent.model.blt import ByteLatentTransformer\n",
        "from bytelatent.tokenizers.blt_tokenizer import BltTokenizer\n",
        "import traceback # لطباعة الأخطاء بشكل أفضل\n",
        "\n",
        "# --- 2. تحديد المتغيرات ---\n",
        "prompt_text: str = \"A BLT has\"\n",
        "model_name: str = \"blt-1b\" # أو اسم النموذج الذي قمت بتحميل أوزانه\n",
        "checkpoint_path = os.path.join(\"hf-weights\", model_name)\n",
        "\n",
        "# --- 3. تحديد الجهاز ---\n",
        "model = None # نهيئ النموذج كـ None\n",
        "tokenizer = None\n",
        "train_cfg = None\n",
        "patcher = None\n",
        "device = None\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"CUDA is available. Attempting to use device: {device}\")\n",
        "    # نحاول تفريغ ذاكرة الـ GPU كإجراء احترازي\n",
        "    torch.cuda.empty_cache()\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CUDA not available. Using CPU.\")\n",
        "\n",
        "# --- 4. محاولة تحميل النموذج ومعالجته ---\n",
        "try:\n",
        "    print(f\"\\n--- Attempting to load model '{model_name}' from {checkpoint_path} ---\")\n",
        "    print(\"!!! WARNING: This step requires significant RAM and will likely fail on standard Colab (12GB RAM) !!!\")\n",
        "\n",
        "    # --- 4.1. التحميل الأولي (الأكثر خطورة لاستهلاك الـ RAM) ---\n",
        "    model, tokenizer, train_cfg = load_consolidated_model_and_tokenizer(\n",
        "        checkpoint_path, init_distributed=True\n",
        "    )\n",
        "    print(\"--- Model loaded successfully (initially on CPU/fp32) ---\")\n",
        "\n",
        "    # --- 4.2. المعالجة بعد التحميل (فقط إذا نجح التحميل) ---\n",
        "    print(\"--- Attempting to convert model to fp16 and move to GPU ---\")\n",
        "    model = model.half() # تحويل إلى float16\n",
        "    model.to(device)     # نقل إلى GPU\n",
        "    model.eval()         # وضع التقييم\n",
        "\n",
        "    # تحقق من الجهاز والدقة\n",
        "    print(f\"--- Model successfully moved to device: {next(model.parameters()).device} with dtype: {next(model.parameters()).dtype} ---\")\n",
        "\n",
        "    # --- 4.3. تحميل الـ Patcher ---\n",
        "    print(\"--- Loading entropy model and patcher ---\")\n",
        "    patcher_args = train_cfg.data.patcher_args.model_copy(deep=True)\n",
        "    patcher_args.realtime_patching = True\n",
        "    patcher_args.entropy_model_checkpoint_dir = os.path.join(\n",
        "        checkpoint_path, \"entropy_model\"\n",
        "    )\n",
        "    patcher = patcher_args.build()\n",
        "\n",
        "    # --- محاولة نقل Patcher (اختياري) ---\n",
        "    try:\n",
        "        patcher.to(device)\n",
        "        print(f\"--- Patcher moved to device: {device} ---\")\n",
        "    except Exception as e:\n",
        "        print(f\"--- Could not move patcher to device (may not be necessary or possible). Error: {e} ---\")\n",
        "\n",
        "except MemoryError:\n",
        "    print(\"\\n**************** MEMORY ERROR ****************\")\n",
        "    print(\"Failed to load the model due to insufficient system RAM (likely exceeded 12GB).\")\n",
        "    print(\"Cannot proceed with GPU processing or generation.\")\n",
        "    print(\"Consider using a Colab Pro environment with High RAM or modifying the library's loading function.\")\n",
        "    print(\"**********************************************\")\n",
        "    model = None # تأكد من أن النموذج لا يزال None\n",
        "except Exception as e:\n",
        "    print(f\"\\n**************** AN ERROR OCCURRED DURING LOADING/PROCESSING ****************\")\n",
        "    print(f\"Error type: {type(e).__name__}\")\n",
        "    print(f\"Error message: {e}\")\n",
        "    print(\"Traceback:\")\n",
        "    traceback.print_exc()\n",
        "    print(\"Cannot proceed with generation.\")\n",
        "    print(\"*****************************************************************************\")\n",
        "    model = None # تأكد من أن النموذج لا يزال None\n",
        "\n",
        "# --- 5. تشغيل التوليد (فقط إذا تم تحميل النموذج بنجاح) ---\n",
        "if model is not None and tokenizer is not None and patcher is not None and device is not None:\n",
        "    print(f\"\\n--- Generating completion for prompt on device: {device} ---\")\n",
        "    prompts_list = [prompt_text]\n",
        "    outputs = None\n",
        "\n",
        "    try:\n",
        "        with torch.no_grad(): # مهم للاستدلال\n",
        "            outputs = generate_nocache(\n",
        "                prompts_list,\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                patcher=patcher,\n",
        "                device=device # <<< تمرير الجهاز لدالة التوليد\n",
        "            )\n",
        "        print(\"--- Generation successful ---\")\n",
        "\n",
        "    except RuntimeError as e:\n",
        "         print(f\"\\n!!! Runtime Error during generation: {e}\")\n",
        "         if \"CUDA out of memory\" in str(e):\n",
        "             print(\"!!! This indicates the model (even in fp16) and activations require more GPU RAM than available on T4 (15GB).\")\n",
        "         elif \"Expected all tensors to be on the same device\" in str(e):\n",
        "             print(\"!!! This likely means the 'generate_nocache' function isn't correctly placing input tensors on the GPU.\")\n",
        "             print(\"!!! The 'generate_nocache' function in 'bytelatent/generate_blt.py' might need modification.\")\n",
        "         else:\n",
        "             print(\"!!! An unexpected runtime error occurred.\")\n",
        "         traceback.print_exc() # طباعة تتبع الخطأ الكامل\n",
        "         outputs = None # إعادة تعيين المخرجات\n",
        "    except Exception as e:\n",
        "        print(f\"\\n!!! An unexpected error occurred during generation: {e}\")\n",
        "        traceback.print_exc()\n",
        "        outputs = None # إعادة تعيين المخرجات\n",
        "\n",
        "    # --- 6. طباعة النتائج ---\n",
        "    if outputs is not None:\n",
        "        print(\"\\n--- Results ---\")\n",
        "        text_outputs = [tokenizer.decode(t) for t in outputs]\n",
        "        for p, t in zip(prompts_list, text_outputs):\n",
        "            print(f'Prompt: \"{p}\"')\n",
        "            print(f'Completion: \"{t}\"')\n",
        "            print(\"-\" * 20)\n",
        "else:\n",
        "    print(\"\\n--- Skipping generation because the model could not be loaded/processed successfully. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ca8A4YXucW3q",
        "outputId": "bdfdfecf-c785-4b2e-d9c5-f1ddd6c41ac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/blt\n",
            "CUDA is available. Attempting to use device: cuda\n",
            "\n",
            "--- Attempting to load model 'blt-1b' from hf-weights/blt-1b ---\n",
            "!!! WARNING: This step requires significant RAM and will likely fail on standard Colab (12GB RAM) !!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    model, tokenizer, train_cfg = load_consolidated_model_and_tokenizer(\n",
        "        checkpoint_path,\n",
        "    )\n",
        "    print(\"--- Model loaded successfully (initially on CPU/fp32) ---\")\n",
        "\n",
        "    # --- 4.2. المعالجة بعد التحميل (فقط إذا نجح التحميل) ---\n",
        "    print(\"--- Attempting to convert model to fp16 and move to GPU ---\")\n",
        "    model = model.half() # تحويل إلى float16\n",
        "    model.to(device)     # نقل إلى GPU\n",
        "    model.eval()         # وضع التقييم\n",
        "\n",
        "(consolidated_path: Any, init_distributed: bool = False) -> tuple[LMTransformer | ByteLatentTransformer, Tokenizer, TrainArgs]"
      ],
      "metadata": {
        "id": "5SjPX_9ngv9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "model, tokenizer, train_cfg = load_consolidated_model_and_tokenizer(\n",
        "    checkpoint_path, device_map=\"auto\" # Assuming the function accepts this argument\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "JS6qjYqqfGz8",
        "outputId": "5d197451-2670-407c-bb0a-2995a147de06"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "load_consolidated_model_and_tokenizer() got an unexpected keyword argument 'device_map'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3e19688b71c9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model, tokenizer, train_cfg = load_consolidated_model_and_tokenizer(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m \u001b[0;31m# Assuming the function accepts this argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n",
            "\u001b[0;31mTypeError\u001b[0m: load_consolidated_model_and_tokenizer() got an unexpected keyword argument 'device_map'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ax9cGKVhenTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BNEZGYmyiNY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uES--aT0iNb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# تعديل كود BLT من Facebook Research للعمل على RAM و GPU معًا\n",
        "%cd /content/blt\n",
        "\n",
        "# --- استيراد المكتبات اللازمة ---\n",
        "import os\n",
        "import torch\n",
        "import gc  # لجمع البيانات المهملة بشكل صريح\n",
        "from bytelatent.distributed import DistributedArgs, setup_torch_distributed\n",
        "from bytelatent.generate import load_consolidated_model_and_tokenizer\n",
        "from bytelatent.generate_blt import generate_nocache\n",
        "from bytelatent.model.blt import ByteLatentTransformer\n",
        "from bytelatent.tokenizers.blt_tokenizer import BltTokenizer\n",
        "import traceback\n",
        "\n",
        "# --- تهيئة المسار وأسماء النموذج ---\n",
        "prompt_text = \"A BLT has\"\n",
        "model_name = \"blt-1b\"  # أو اسم النموذج الذي تستخدمه\n",
        "checkpoint_path = os.path.join(\"hf-weights\", model_name)\n",
        "\n",
        "# --- تعريف الدالة المساعدة للتنظيف ---\n",
        "def clean_memory():\n",
        "    \"\"\"تنظيف الذاكرة بإزالة المتغيرات غير المستخدمة وإجبار جمع البيانات المهملة\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"تم تنظيف ذاكرة GPU\")\n",
        "    print(\"تم تنظيف ذاكرة النظام\")\n",
        "\n",
        "# --- تحديد الأجهزة المتاحة ---\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"CUDA متاح. استخدام الجهاز: {device}\")\n",
        "    print(f\"إجمالي ذاكرة GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    print(f\"ذاكرة GPU المستخدمة حاليًا: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "    print(f\"ذاكرة GPU المحجوزة حاليًا: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
        "    torch.cuda.empty_cache()\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CUDA غير متاح. استخدام وحدة المعالجة المركزية.\")\n",
        "\n",
        "# --- حاول تهيئة النموذج مع استراتيجيات توفير الذاكرة ---\n",
        "model = None\n",
        "tokenizer = None\n",
        "patcher = None\n",
        "train_cfg = None\n",
        "\n",
        "try:\n",
        "    print(f\"\\n--- محاولة تحميل النموذج '{model_name}' من {checkpoint_path} ---\")\n",
        "\n",
        "    # --- تعديل 1: استخدام low_cpu_mem_usage=True لتقليل استخدام ذاكرة CPU ---\n",
        "    # ملاحظة: قد تحتاج لتعديل دالة load_consolidated_model_and_tokenizer\n",
        "    # للدعم هذا الخيار إذا لم يكن مدعومًا بالفعل\n",
        "\n",
        "    # --- تعديل 2: تحميل النموذج مباشرة كـ half precision ---\n",
        "    # نقترح تعديل الدالة لتحميل النموذج مباشرة بدقة fp16\n",
        "    print(\"جاري تحميل النموذج والمعالج...\")\n",
        "\n",
        "    # --- طريقة بديلة للتحميل إذا كانت الطريقة الأصلية تستهلك الكثير من الذاكرة ---\n",
        "    # يمكنك تجربة مقاربة مختلفة بفصل عملية التحميل:\n",
        "\n",
        "    # الخطوة 1: تحميل المعالج فقط أولاً\n",
        "    try:\n",
        "        # افترض أن هناك دالة لتحميل المعالج فقط (قد تحتاج لتعديلها)\n",
        "        # tokenizer = BltTokenizer.from_pretrained(checkpoint_path)\n",
        "        # وإلا استخدم الطريقة الأصلية\n",
        "        _, tokenizer, _ = load_consolidated_model_and_tokenizer(\n",
        "            checkpoint_path,\n",
        "            init_distributed=True,\n",
        "            just_tokenizer=True  # افترض وجود هذا الخيار أو أضفه\n",
        "        )\n",
        "        print(\"تم تحميل المعالج (tokenizer) بنجاح\")\n",
        "    except Exception as e:\n",
        "        print(f\"خطأ في تحميل المعالج: {e}\")\n",
        "        # حاول بالطريقة العادية\n",
        "        model, tokenizer, train_cfg = load_consolidated_model_and_tokenizer(\n",
        "            checkpoint_path,\n",
        "            init_distributed=True,\n",
        "            just_tokenizer=True,\n",
        "            just_model=True,\n",
        "            just_train_cfg=True,\n",
        "            device_map=\"auto\",\n",
        "            dtype=torch.float32,\n",
        "            low_cpu_mem_usage=True,\n",
        "            #load_in_8bit=True,\n",
        "            #quantization_config=None,\n",
        "            use_safetensors=True,\n",
        "            trust_remote_code=True,\n",
        "            use_auth_token=None,\n",
        "            force_download=False,\n",
        "            resume_download=False,\n",
        "            local_files_only=False,\n",
        "\n",
        "        )\n",
        "        print(\"تم تحميل النموذج والمعالج بالطريقة الأصلية\")\n",
        "\n",
        "    clean_memory()  # تنظيف بعد تحميل المعالج\n",
        "\n",
        "    # الخطوة 2: تحميل النموذج بدقة fp16 مباشرة وعلى الجهاز الصحيح\n",
        "    if model is None:\n",
        "        print(\"محاولة تحميل النموذج بدقة fp16 مباشرة...\")\n",
        "        try:\n",
        "            # افترض وجود خيار dtype & device في دالة load_consolidated_model_and_tokenizer\n",
        "            # أو أضفها كتعديل\n",
        "            model_loading_device = \"auto\"  # ابدأ بتحميل على CPU\n",
        "            model, _, train_cfg = load_consolidated_model_and_tokenizer(\n",
        "                checkpoint_path,\n",
        "                init_distributed=True,\n",
        "                just_model=True,  # افترض وجود هذا الخيار أو أضفه\n",
        "                dtype=torch.float32,\n",
        "                device=model_loading_device\n",
        "            )\n",
        "            print(f\"تم تحميل النموذج الأولي على {model_loading_device}\")\n",
        "        except Exception as e:\n",
        "            print(f\"فشل في تحميل النموذج مباشرة بدقة fp16: {e}\")\n",
        "            # إذا فشلت المحاولة السابقة، جرب الطريقة التقليدية مع التعديلات\n",
        "            if model is None and train_cfg is None:\n",
        "                print(\"محاولة تحميل النموذج بالطريقة التقليدية...\")\n",
        "                model, _, train_cfg = load_consolidated_model_and_tokenizer(\n",
        "                    checkpoint_path,\n",
        "                    init_distributed=True,\n",
        "                    just_model=True\n",
        "                )\n",
        "                print(\"تم تحميل النموذج بالطريقة التقليدية بنجاح\")\n",
        "\n",
        "\n",
        "    # --- تعديل 3: تحميل النموذج بأقسام على GPU ---\n",
        "    print(\"إعداد النموذج للاستخدام...\")\n",
        "\n",
        "    # تأكد من تحويل النموذج إلى fp16\n",
        "    if model is not None:\n",
        "        model = model.half()\n",
        "        print(\"تم تحويل النموذج إلى half precision (fp16)\")\n",
        "\n",
        "        # تشغيل gradient checkpointing لتقليل استهلاك الذاكرة (إن كان مدعومًا)\n",
        "        try:\n",
        "            model.gradient_checkpointing_enable()\n",
        "            print(\"تم تفعيل gradient checkpointing\")\n",
        "        except:\n",
        "            print(\"gradient checkpointing غير مدعوم في هذا النموذج\")\n",
        "\n",
        "        # نقل النموذج إلى وضع التقييم\n",
        "        model.eval()\n",
        "        print(\"تم وضع النموذج في وضع التقييم\")\n",
        "\n",
        "        # نقل النموذج إلى GPU بشكل تدريجي (قد تحتاج لتعديل هذا حسب هيكل النموذج)\n",
        "        try:\n",
        "            # نقل النموذج كاملاً إذا كانت الذاكرة كافية\n",
        "            model.to(device)\n",
        "            print(f\"تم نقل النموذج بالكامل إلى {device}\")\n",
        "        except RuntimeError as e:\n",
        "            print(f\"فشل نقل النموذج بالكامل إلى GPU: {e}\")\n",
        "            print(\"جاري محاولة نقل أجزاء من النموذج...\")\n",
        "\n",
        "            # محاولة نقل الأجزاء بشكل منفصل\n",
        "            # هذا مثال قد يحتاج للتعديل حسب هيكلية النموذج\n",
        "            try:\n",
        "                if hasattr(model, \"encoder\"):\n",
        "                    model.encoder.to(device)\n",
        "                    print(\"تم نقل encoder إلى GPU\")\n",
        "                if hasattr(model, \"decoder\"):\n",
        "                    model.decoder.to(device)\n",
        "                    print(\"تم نقل decoder إلى GPU\")\n",
        "                if hasattr(model, \"embeddings\"):\n",
        "                    model.embeddings.to(\"cpu\")  # إبقاء embeddings على CPU لتوفير مساحة\n",
        "                    print(\"تم إبقاء embeddings على CPU\")\n",
        "                # قد تحتاج لتعديل هذا حسب هيكلية النموذج الخاص بك\n",
        "            except Exception as e:\n",
        "                print(f\"فشل نقل أجزاء من النموذج: {e}\")\n",
        "                print(\"سنستمر بالنموذج على CPU\")\n",
        "\n",
        "    clean_memory()  # تنظيف بعد إعداد النموذج\n",
        "\n",
        "    # --- تحميل الـ Patcher ---\n",
        "    if train_cfg is not None:\n",
        "        print(\"جاري تحميل entropy model و patcher...\")\n",
        "        patcher_args = train_cfg.data.patcher_args.model_copy(deep=True)\n",
        "        patcher_args.realtime_patching = True\n",
        "        patcher_args.entropy_model_checkpoint_dir = os.path.join(\n",
        "            checkpoint_path, \"entropy_model\"\n",
        "        )\n",
        "        patcher = patcher_args.build()\n",
        "\n",
        "        # محاولة نقل patcher إلى device المناسب\n",
        "        try:\n",
        "            patcher_device = \"cpu\"  # قد يكون من الأفضل إبقاء patcher على CPU\n",
        "            patcher.to(patcher_device)\n",
        "            print(f\"تم نقل patcher إلى {patcher_device}\")\n",
        "        except Exception as e:\n",
        "            print(f\"تعذر نقل patcher: {e}\")\n",
        "\n",
        "    print(\"تم الانتهاء من إعداد النموذج\")\n",
        "\n",
        "except MemoryError as e:\n",
        "    print(\"\\n**************** خطأ في الذاكرة ****************\")\n",
        "    print(f\"فشل تحميل النموذج بسبب عدم كفاية ذاكرة النظام: {e}\")\n",
        "    print(\"لا يمكن المتابعة بالمعالجة أو التوليد.\")\n",
        "    print(\"فكر في استخدام بيئة Colab Pro مع ذاكرة عالية.\")\n",
        "    clean_memory()\n",
        "    model = None\n",
        "except Exception as e:\n",
        "    print(f\"\\n**************** حدث خطأ أثناء التحميل/المعالجة ****************\")\n",
        "    print(f\"نوع الخطأ: {type(e).__name__}\")\n",
        "    print(f\"رسالة الخطأ: {e}\")\n",
        "    print(\"تتبع الخطأ:\")\n",
        "    traceback.print_exc()\n",
        "    print(\"لا يمكن المتابعة بالتوليد.\")\n",
        "    print(\"***************************************************\")\n",
        "    clean_memory()\n",
        "    model = None\n",
        "\n",
        "# --- تشغيل التوليد (فقط إذا تم تحميل النموذج بنجاح) ---\n",
        "if model is not None and tokenizer is not None and patcher is not None:\n",
        "    print(f\"\\n--- توليد النص للمدخل على الجهاز: {next(model.parameters()).device} ---\")\n",
        "    prompts_list = [prompt_text]\n",
        "    outputs = None\n",
        "\n",
        "    try:\n",
        "        # إضافة استراتيجية توفير الذاكرة أثناء التوليد\n",
        "        with torch.no_grad():\n",
        "            # تأكد من أن المدخلات ستكون على نفس الجهاز مع النموذج\n",
        "            generate_device = next(model.parameters()).device\n",
        "            print(f\"جهاز التوليد: {generate_device}\")\n",
        "\n",
        "            # هنا قمنا بتمرير جهاز التوليد المكتشف\n",
        "            outputs = generate_nocache(\n",
        "                prompts_list,\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                patcher=patcher,\n",
        "                device=generate_device  # استخدم الجهاز المكتشف من النموذج\n",
        "            )\n",
        "        print(\"--- تم التوليد بنجاح ---\")\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        print(f\"\\n!!! خطأ في وقت التشغيل أثناء التوليد: {e}\")\n",
        "        if \"CUDA out of memory\" in str(e):\n",
        "            print(\"!!! هذا يشير إلى أن النموذج (حتى بدقة fp16) والتنشيطات تتطلب ذاكرة GPU أكثر من المتاحة على T4 (15GB).\")\n",
        "            print(\"!!! جرب تقليل batch size أو max_length للتوليد\")\n",
        "        elif \"Expected all tensors to be on the same device\" in str(e):\n",
        "            print(\"!!! هذا يعني على الأرجح أن دالة 'generate_nocache' لا تضع المصفوفات المدخلة بشكل صحيح على GPU.\")\n",
        "            print(\"!!! قد تحتاج دالة 'generate_nocache' في 'bytelatent/generate_blt.py' إلى تعديل.\")\n",
        "\n",
        "            # محاولة تصحيح الإمكان - عرض إضافي قد لا يتم تنفيذه\n",
        "            print(\"\\n!!! محاولة اقتراح تصحيح:\")\n",
        "            print(\"قد تحتاج إلى تعديل دالة generate_nocache في bytelatent/generate_blt.py\")\n",
        "            print(\"لتضمين السطر التالي في بداية الدالة:\")\n",
        "            print(\"input_ids = input_ids.to(device)\")\n",
        "            print(\"attention_mask = attention_mask.to(device) # إذا كان موجودًا\")\n",
        "\n",
        "        else:\n",
        "            print(\"!!! حدث خطأ غير متوقع في وقت التشغيل.\")\n",
        "        traceback.print_exc()\n",
        "        outputs = None\n",
        "    except Exception as e:\n",
        "        print(f\"\\n!!! حدث خطأ غير متوقع أثناء التوليد: {e}\")\n",
        "        traceback.print_exc()\n",
        "        outputs = None\n",
        "\n",
        "    # --- طباعة النتائج ---\n",
        "    if outputs is not None:\n",
        "        print(\"\\n--- النتائج ---\")\n",
        "        text_outputs = [tokenizer.decode(t) for t in outputs]\n",
        "        for p, t in zip(prompts_list, text_outputs):\n",
        "            print(f'المدخل: \"{p}\"')\n",
        "            print(f'الناتج: \"{t}\"')\n",
        "            print(\"-\" * 20)\n",
        "\n",
        "    # تنظيف نهائي للذاكرة\n",
        "    clean_memory()\n",
        "else:\n",
        "    print(\"\\n--- تم تخطي التوليد لأن النموذج لم يتم تحميله/معالجته بنجاح. ---\")\n",
        "    print(\"راجع الأخطاء أعلاه لمزيد من المعلومات.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TM0P2idiNet",
        "outputId": "3c5f5b9f-e716-4a16-b781-145eb5ba39ea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/blt\n",
            "CUDA متاح. استخدام الجهاز: cuda\n",
            "إجمالي ذاكرة GPU: 15.83 GB\n",
            "ذاكرة GPU المستخدمة حاليًا: 0.00 GB\n",
            "ذاكرة GPU المحجوزة حاليًا: 0.00 GB\n",
            "\n",
            "--- محاولة تحميل النموذج 'blt-1b' من hf-weights/blt-1b ---\n",
            "جاري تحميل النموذج والمعالج...\n",
            "خطأ في تحميل المعالج: load_consolidated_model_and_tokenizer() got an unexpected keyword argument 'just_tokenizer'\n",
            "\n",
            "**************** حدث خطأ أثناء التحميل/المعالجة ****************\n",
            "نوع الخطأ: TypeError\n",
            "رسالة الخطأ: load_consolidated_model_and_tokenizer() got an unexpected keyword argument 'just_tokenizer'\n",
            "تتبع الخطأ:\n",
            "لا يمكن المتابعة بالتوليد.\n",
            "***************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-1-3be0827d1a9e>\", line 66, in <cell line: 0>\n",
            "    _, tokenizer, _ = load_consolidated_model_and_tokenizer(\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: load_consolidated_model_and_tokenizer() got an unexpected keyword argument 'just_tokenizer'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-1-3be0827d1a9e>\", line 75, in <cell line: 0>\n",
            "    model, tokenizer, train_cfg = load_consolidated_model_and_tokenizer(\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: load_consolidated_model_and_tokenizer() got an unexpected keyword argument 'just_tokenizer'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تم تنظيف ذاكرة GPU\n",
            "تم تنظيف ذاكرة النظام\n",
            "\n",
            "--- تم تخطي التوليد لأن النموذج لم يتم تحميله/معالجته بنجاح. ---\n",
            "راجع الأخطاء أعلاه لمزيد من المعلومات.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7-XwCkmpiQxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kkt3iRW5iQus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bytelatent/generate_blt.py\n",
        "\n",
        "# هذا تعديل مقترح لدالة generate_nocache في ملف bytelatent/generate_blt.py\n",
        "# يجب أن تبحث عن هذه الدالة في الملف المذكور وتقوم بتعديلها ليتم نقل المصفوفات إلى\n",
        "# نفس الجهاز الذي يوجد عليه النموذج\n",
        "\n",
        "def generate_nocache(\n",
        "    prompts,\n",
        "    model: ByteLatentTransformer,\n",
        "    tokenizer: BltTokenizer,\n",
        "    patcher,\n",
        "    max_new_tokens: int = 100,\n",
        "    top_k: int = 50,\n",
        "    top_p: float = 0.9,\n",
        "    temperature: float = 1.0,\n",
        "    do_sample: bool = True,\n",
        "    device=None  # أضفنا معامل الجهاز هنا\n",
        "):\n",
        "    \"\"\"\n",
        "    دالة معدلة للتوليد تضمن أن كل المصفوفات على نفس الجهاز\n",
        "    \"\"\"\n",
        "    # تحديد الجهاز من النموذج إذا لم يتم تحديده\n",
        "    if device is None:\n",
        "        device = next(model.parameters()).device\n",
        "        print(f\"تم اكتشاف جهاز النموذج تلقائيًا: {device}\")\n",
        "\n",
        "    print(f\"استخدام الجهاز للتوليد: {device}\")\n",
        "\n",
        "    # تحويل المدخلات النصية إلى رموز\n",
        "    prompts_tokens = [tokenizer.encode(p) for p in prompts]\n",
        "\n",
        "    # تعبئة المدخلات لتكون بنفس الطول\n",
        "    max_prompt_len = max(len(tokens) for tokens in prompts_tokens)\n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    for tokens in prompts_tokens:\n",
        "        padding_len = max_prompt_len - len(tokens)\n",
        "        input_ids.append(tokens + [tokenizer.pad_token_id] * padding_len)\n",
        "        attention_mask.append([1] * len(tokens) + [0] * padding_len)\n",
        "\n",
        "    # تحويل القوائم إلى مصفوفات PyTorch\n",
        "    input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
        "    attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n",
        "\n",
        "    # نقل المصفوفات إلى نفس جهاز النموذج - هذه هي الخطوة المهمة\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "\n",
        "    # التأكد من أن patcher على نفس الجهاز إذا كان مطلوبًا\n",
        "    if hasattr(patcher, 'device') and patcher.device != device:\n",
        "        try:\n",
        "            patcher.to(device)\n",
        "            print(f\"تم نقل patcher إلى {device}\")\n",
        "        except:\n",
        "            print(f\"تعذر نقل patcher إلى {device}، سنستمر على أي حال\")\n",
        "\n",
        "    # استدعاء دالة التوليد في النموذج\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        patcher=patcher,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        temperature=temperature,\n",
        "        do_sample=do_sample,\n",
        "    )\n",
        "\n",
        "    # استخراج الإخراج بدون المدخل\n",
        "    prompt_lens = [len(tokens) for tokens in prompts_tokens]\n",
        "    completions = []\n",
        "    for i, output in enumerate(outputs):\n",
        "        completion = output[prompt_lens[i]:].tolist()\n",
        "        completions.append(completion)\n",
        "\n",
        "    return completions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "JPakwthmiQrk",
        "outputId": "766b4438-157d-4790-abbc-7e8e178b9e02"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ByteLatentTransformer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-eb95d7b0236d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m def generate_nocache(\n\u001b[1;32m      8\u001b[0m     \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mByteLatentTransformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBltTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpatcher\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ByteLatentTransformer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# استراتيجية لتحميل نموذج BLT مع CPU-GPU Offloading\n",
        "# هذا الكود يمكن استخدامه للتعامل مع المشكلات الخاصة بالذاكرة\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "class ModelOffloader:\n",
        "    \"\"\"\n",
        "    فئة لإدارة نقل أجزاء من النموذج بين CPU و GPU\n",
        "    لتحسين استخدام الذاكرة المتاحة\n",
        "    \"\"\"\n",
        "    def __init__(self, model, gpu_threshold_mb=1000):\n",
        "        self.model = model\n",
        "        self.gpu_threshold_mb = gpu_threshold_mb\n",
        "        self.modules_map = {}\n",
        "        self._map_modules()\n",
        "\n",
        "    def _map_modules(self):\n",
        "        \"\"\"تخزين إشارات لكل وحدات النموذج للتحكم بها لاحقًا\"\"\"\n",
        "        # تحديد أهم وحدات النموذج\n",
        "        for name, module in self.model.named_children():\n",
        "            self.modules_map[name] = module\n",
        "            # يمكن إضافة المزيد من التفاصيل هنا حسب هيكلية نموذج BLT\n",
        "\n",
        "    def print_model_structure(self):\n",
        "        \"\"\"عرض هيكلية النموذج لفهم أفضل\"\"\"\n",
        "        print(\"=== هيكلية النموذج ===\")\n",
        "        for name, module in self.model.named_children():\n",
        "            param_size = sum(p.nelement() * p.element_size() for p in module.parameters() if p.requires_grad) / 1024 / 1024\n",
        "            print(f\"- {name}: {param_size:.2f} MB\")\n",
        "\n",
        "            # عرض الوحدات الفرعية للوحدات الكبيرة\n",
        "            if param_size > 100:  # عرض تفاصيل للوحدات الأكبر من 100 ميجابايت\n",
        "                for subname, _ in module.named_children():\n",
        "                    print(f\"  - {name}.{subname}\")\n",
        "\n",
        "    def check_gpu_memory(self):\n",
        "        \"\"\"التحقق من ذاكرة GPU المتاحة\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()\n",
        "            free_memory_mb = free_memory / 1024 / 1024\n",
        "            print(f\"ذاكرة GPU المتاحة: {free_memory_mb:.2f} MB\")\n",
        "            return free_memory_mb\n",
        "        return 0\n",
        "\n",
        "    def offload_module_to_cpu(self, module_name):\n",
        "        \"\"\"نقل وحدة معينة إلى CPU\"\"\"\n",
        "        if module_name in self.modules_map:\n",
        "            module = self.modules_map[module_name]\n",
        "            module.to(\"cpu\")\n",
        "            print(f\"تم نقل {module_name} إلى CPU\")\n",
        "            torch.cuda.empty_cache()\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def load_module_to_gpu(self, module_name):\n",
        "        \"\"\"نقل وحدة معينة إلى GPU\"\"\"\n",
        "        if self.check_gpu_memory() < self.gpu_threshold_mb:\n",
        "            print(f\"ذاكرة GPU غير كافية (أقل من {self.gpu_threshold_mb} MB)\")\n",
        "            return False\n",
        "\n",
        "        if module_name in self.modules_map:\n",
        "            module = self.modules_map[module_name]\n",
        "            module.to(\"cuda\")\n",
        "            print(f\"تم نقل {module_name} إلى GPU\")\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def smart_module_placement(self):\n",
        "        \"\"\"استراتيجية ذكية لوضع الوحدات على CPU/GPU\"\"\"\n",
        "        print(\"تطبيق استراتيجية النقل الذكي للوحدات...\")\n",
        "\n",
        "        # ترتيب افتراضي للأولوية - يجب تعديله بناءً على هيكلية BLT\n",
        "        priority_modules = []\n",
        "\n",
        "        # حدد الوحدات ذات الأولوية حسب هيكلية نموذج BLT\n",
        "        # على سبيل المثال: محولات الانتباه، وحدات التوليد، إلخ.\n",
        "        for name in self.modules_map:\n",
        "            if \"decoder\" in name.lower():\n",
        "                priority_modules.insert(0, name)  # وحدات فك التشفير لها أولوية عالية\n",
        "            elif \"encoder\" in name.lower():\n",
        "                priority_modules.append(name)     # ثم وحدات التشفير\n",
        "            else:\n",
        "                priority_modules.append(name)     # باقي الوحدات\n",
        "\n",
        "        # محاولة نقل الوحدات ذات الأولوية العالية إلى GPU\n",
        "        gpu_modules = []\n",
        "        cpu_modules = []\n",
        "\n",
        "        for module_name in priority_modules:\n",
        "            if self.load_module_to_gpu(module_name):\n",
        "                gpu_modules.append(module_name)\n",
        "            else:\n",
        "                self.offload_module_to_cpu(module_name)\n",
        "                cpu_modules.append(module_name)\n",
        "\n",
        "        print(f\"الوحدات على GPU: {', '.join(gpu_modules)}\")\n",
        "        print(f\"الوحدات على CPU: {', '.join(cpu_modules)}\")\n",
        "\n",
        "        return len(gpu_modules) > 0\n",
        "\n",
        "    def optimize_for_inference(self):\n",
        "        \"\"\"تحسين النموذج للاستدلال مع استراتيجية offloading\"\"\"\n",
        "        # تنظيف الذاكرة\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # طباعة هيكلية النموذج للمساعدة في الفهم\n",
        "        self.print_model_structure()\n",
        "\n",
        "        # تحويل النموذج لوضع الاستدلال\n",
        "        self.model.eval()\n",
        "\n",
        "        # تطبيق half precision\n",
        "        self.model.half()\n",
        "\n",
        "        # محاولة تفعيل gradient checkpointing إن أمكن\n",
        "        try:\n",
        "            self.model.gradient_checkpointing_enable()\n",
        "            print(\"تم تفعيل gradient checkpointing\")\n",
        "        except:\n",
        "            print(\"gradient checkpointing غير مدعوم\")\n",
        "\n",
        "        # تطبيق استراتيجية النقل الذكي\n",
        "        result = self.smart_module_placement()\n",
        "\n",
        "        if result:\n",
        "            print(\"تم تحسين النموذج للاستدلال بنجاح\")\n",
        "        else:\n",
        "            print(\"واجهنا صعوبات في نقل أي من وحدات النموذج إلى GPU\")\n",
        "            print(\"سيتم استخدام CPU للمعالجة\")\n",
        "\n",
        "        return result\n",
        "\n",
        "# مثال على الاستخدام\n",
        "\"\"\"\n",
        "# كيفية استخدام الفئة الموضحة أعلاه:\n",
        "\n",
        "# بعد تحميل النموذج\n",
        "offloader = ModelOffloader(model, gpu_threshold_mb=500)\n",
        "\n",
        "# تحسين النموذج للاستدلال\n",
        "offloader.optimize_for_inference()\n",
        "\n",
        "# الآن يمكنك استخدام النموذج للتوليد\n",
        "# النموذج سيكون موزعًا بين CPU و GPU بطريقة ذكية\n",
        "\"\"\"\n",
        "\n",
        "# ملاحظة: قد تحتاج لتعديل الاستراتيجية حسب هيكلية محددة لنموذج BLT\n",
        "# هذا الكود قابل للتعديل ويعتمد على فهم هيكلية النموذج"
      ],
      "metadata": {
        "id": "imWGq0t0ilit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AmEEbUhvlIrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jW2sUVuflIou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "elrMag2BlIly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cwsP-4_IlIim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/blt  # فقط إذا كنت تعمل على Colab وتحتاج لتغيير المسار\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import gc\n",
        "from bytelatent.distributed import DistributedArgs, setup_torch_distributed\n",
        "from bytelatent.generate import load_consolidated_model_and_tokenizer\n",
        "import traceback\n",
        "\n",
        "# --- الإعدادات ---\n",
        "prompt_text = \"A BLT has\"\n",
        "model_name = \"blt-1b\"\n",
        "checkpoint_path = os.path.join(\"hf-weights\", model_name)\n",
        "\n",
        "# --- تنظيف الذاكرة ---\n",
        "def clean_memory():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"✅ تم تنظيف ذاكرة GPU\")\n",
        "    print(\"✅ تم تنظيف ذاكرة النظام\")\n",
        "\n",
        "# --- إعداد الجهاز ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"🔧 استخدام الجهاز: {device}\")\n",
        "if device.type == \"cuda\":\n",
        "    print(f\"GPU Total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "# --- تهيئة التوزيع (اختياري) ---\n",
        "try:\n",
        "    dist_args = DistributedArgs(local_rank=0)\n",
        "    setup_torch_distributed(dist_args)\n",
        "except Exception as e:\n",
        "    print(\"⚠️ لم يتم إعداد التوزيع: \", e)\n",
        "\n",
        "# --- تحميل tokenizer فقط ---\n",
        "print(\"\\n🚀 تحميل المعالج (tokenizer)...\")\n",
        "try:\n",
        "    _, tokenizer, _ = load_consolidated_model_and_tokenizer(\n",
        "        checkpoint_path,\n",
        "        init_distributed=True,\n",
        "        just_tokenizer=True\n",
        "    )\n",
        "    print(\"✅ تم تحميل tokenizer\")\n",
        "except Exception as e:\n",
        "    print(\"❌ خطأ في تحميل tokenizer:\", e)\n",
        "    traceback.print_exc()\n",
        "\n",
        "clean_memory()\n",
        "\n",
        "# --- تحميل النموذج وتوزيعه بين GPU و CPU ---\n",
        "print(\"\\n🚀 تحميل النموذج بشكل اقتصادي...\")\n",
        "try:\n",
        "    model, _, train_cfg = load_consolidated_model_and_tokenizer(\n",
        "        checkpoint_path,\n",
        "        init_distributed=True,\n",
        "        just_model=True,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",  # توزيع تلقائي حسب الذاكرة\n",
        "        low_cpu_mem_usage=True,\n",
        "    )\n",
        "    print(\"✅ تم تحميل النموذج وتوزيعه تلقائيًا بين CPU و GPU\")\n",
        "except Exception as e:\n",
        "    print(\"❌ خطأ في تحميل النموذج:\", e)\n",
        "    traceback.print_exc()\n",
        "\n",
        "clean_memory()\n",
        "\n",
        "# --- نموذج جاهز للاستخدام الآن ---\n",
        "print(\"\\n✨ النموذج جاهز. يمكنك الآن توليد النصوص عبر generate_nocache مثلاً أو حسب احتياجك.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acoa6Er9lIfX",
        "outputId": "df9a1753-0d22-461a-acdf-788a258be8f4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 استخدام الجهاز: cuda\n",
            "GPU Total: 15.83 GB\n",
            "⚠️ لم يتم إعداد التوزيع:  1 validation error for DistributedArgs\n",
            "local_rank\n",
            "  Extra inputs are not permitted [type=extra_forbidden, input_value=0, input_type=int]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\n",
            "\n",
            "🚀 تحميل المعالج (tokenizer)...\n",
            "❌ خطأ في تحميل tokenizer: load_consolidated_model_and_tokenizer() got an unexpected keyword argument 'just_tokenizer'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-1-f4d2c36ffea4>\", line 39, in <cell line: 0>\n",
            "    _, tokenizer, _ = load_consolidated_model_and_tokenizer(\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: load_consolidated_model_and_tokenizer() got an unexpected keyword argument 'just_tokenizer'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ تم تنظيف ذاكرة GPU\n",
            "✅ تم تنظيف ذاكرة النظام\n",
            "\n",
            "🚀 تحميل النموذج بشكل اقتصادي...\n",
            "❌ خطأ في تحميل النموذج: load_consolidated_model_and_tokenizer() got an unexpected keyword argument 'just_model'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-1-f4d2c36ffea4>\", line 54, in <cell line: 0>\n",
            "    model, _, train_cfg = load_consolidated_model_and_tokenizer(\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: load_consolidated_model_and_tokenizer() got an unexpected keyword argument 'just_model'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ تم تنظيف ذاكرة GPU\n",
            "✅ تم تنظيف ذاكرة النظام\n",
            "\n",
            "✨ النموذج جاهز. يمكنك الآن توليد النصوص عبر generate_nocache مثلاً أو حسب احتياجك.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/blt/bytelatent"
      ],
      "metadata": {
        "id": "fRFZ4-xOm7kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/blt\n",
        "import os\n",
        "import torch\n",
        "import gc\n",
        "import traceback\n",
        "from bytelatent.generate import load_consolidated_model_and_tokenizer\n",
        "from bytelatent.generate import load_consolidated_model_and_tokenizer\n",
        "\n",
        "checkpoint_path = os.path.join(\"hf-weights\", \"blt-1b\")\n",
        "\n",
        "def clean_memory():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"✅ تم تنظيف ذاكرة GPU\")\n",
        "    print(\"✅ تم تنظيف ذاكرة النظام\")\n",
        "\n",
        "# تحميل النموذج والمعالج باستخدام device_map\n",
        "print(\"\\n🚀 تحميل النموذج والمعالج...\")\n",
        "try:\n",
        "    model, tokenizer, _ = load_consolidated_model_and_tokenizer(\n",
        "    checkpoint_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "    print(\"✅ تم تحميل النموذج وتوزيعه بين GPU و RAM\")\n",
        "except Exception as e:\n",
        "    print(\"❌ فشل تحميل النموذج:\", e)\n",
        "    traceback.print_exc()\n",
        "\n",
        "clean_memory()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4_TdtC4l60n",
        "outputId": "46d4c3e4-742c-4065-dd11-5a9d18f597ba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/blt\n",
            "\n",
            "🚀 تحميل النموذج والمعالج...\n",
            "📦 تحميل المعالج من: hf-weights/blt-1b\n",
            "❌ فشل تحميل النموذج: Unrecognized model in hf-weights/blt-1b. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-4-ddf9a9987647>\", line 21, in <cell line: 0>\n",
            "    model, tokenizer, _ = load_consolidated_model_and_tokenizer(\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/blt/bytelatent/generate.py\", line 18, in load_consolidated_model_and_tokenizer\n",
            "    tokenizer = AutoTokenizer.from_pretrained(\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\", line 966, in from_pretrained\n",
            "    config = AutoConfig.from_pretrained(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\", line 1151, in from_pretrained\n",
            "    raise ValueError(\n",
            "ValueError: Unrecognized model in hf-weights/blt-1b. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ تم تنظيف ذاكرة GPU\n",
            "✅ تم تنظيف ذاكرة النظام\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/blt/demo.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbdAKfgumrbW",
        "outputId": "a82a11c9-77d0-4f5d-f8f7-ff8b51302744"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mUsage: \u001b[0mdemo.py [OPTIONS] PROMPT\n",
            "\u001b[2mTry \u001b[0m\u001b[2;34m'demo.py \u001b[0m\u001b[1;2;34m-\u001b[0m\u001b[1;2;34m-help\u001b[0m\u001b[2;34m'\u001b[0m\u001b[2m for help.\u001b[0m\n",
            "\u001b[31m╭─\u001b[0m\u001b[31m Error \u001b[0m\u001b[31m─────────────────────────────────────────────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m Missing argument 'PROMPT'.                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/blt/bytelatent/generate.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob8uRfO2oS2f",
        "outputId": "94de2000-be88-4fd2-b6fa-e8fe7c3d28bc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1967, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/integrations/ggml.py\", line 24, in <module>\n",
            "    from tokenizers import Tokenizer, decoders, normalizers, pre_tokenizers, processors\n",
            "ImportError: cannot import name 'Tokenizer' from 'tokenizers' (/content/blt/bytelatent/tokenizers/__init__.py)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1967, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\", line 24, in <module>\n",
            "    from ...configuration_utils import PretrainedConfig\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\", line 27, in <module>\n",
            "    from .modeling_gguf_pytorch_utils import load_gguf_checkpoint\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_gguf_pytorch_utils.py\", line 22, in <module>\n",
            "    from .integrations import (\n",
            "  File \"<frozen importlib._bootstrap>\", line 1229, in _handle_fromlist\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1955, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1969, in _get_module\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Failed to import transformers.integrations.ggml because of the following error (look up to see its traceback):\n",
            "cannot import name 'Tokenizer' from 'tokenizers' (/content/blt/bytelatent/tokenizers/__init__.py)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/blt/bytelatent/generate.py\", line 1, in <module>\n",
            "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
            "  File \"<frozen importlib._bootstrap>\", line 1229, in _handle_fromlist\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1956, in __getattr__\n",
            "    value = getattr(module, name)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1955, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1969, in _get_module\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Failed to import transformers.models.auto.tokenization_auto because of the following error (look up to see its traceback):\n",
            "Failed to import transformers.integrations.ggml because of the following error (look up to see its traceback):\n",
            "cannot import name 'Tokenizer' from 'tokenizers' (/content/blt/bytelatent/tokenizers/__init__.py)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/blt/demo.py --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVZYICTzoZL_",
        "outputId": "0f250a8c-02b0-4396-f50f-a9564b79ee25"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m                                                                                \u001b[0m\n",
            "\u001b[1m \u001b[0m\u001b[1;33mUsage: \u001b[0m\u001b[1mdemo.py [OPTIONS] PROMPT\u001b[0m\u001b[1m                                               \u001b[0m\u001b[1m \u001b[0m\n",
            "\u001b[1m                                                                                \u001b[0m\n",
            "\u001b[2m╭─\u001b[0m\u001b[2m Arguments \u001b[0m\u001b[2m─────────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[31m*\u001b[0m    prompt      \u001b[1;33mTEXT\u001b[0m  \u001b[2m[default: None]\u001b[0m \u001b[2;31m[required]\u001b[0m                            \u001b[2m│\u001b[0m\n",
            "\u001b[2m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[2m╭─\u001b[0m\u001b[2m Options \u001b[0m\u001b[2m───────────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-model\u001b[0m\u001b[1;36m-name\u001b[0m        \u001b[1;33mTEXT\u001b[0m  \u001b[2m[default: blt-1b]\u001b[0m                                  \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-help\u001b[0m              \u001b[1;33m    \u001b[0m  Show this message and exit.                        \u001b[2m│\u001b[0m\n",
            "\u001b[2m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/blt/demo.py --model-name blt-1b \"hi\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZOTAB3soihQ",
        "outputId": "11c366d1-f600-4dd0-e459-be2f0d2c8c0c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BLT model: blt-1b\n",
            "📦 تحميل المعالج من: hf-weights/blt-1b\n",
            "[rank0]: \u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/content/blt/\u001b[0m\u001b[1;33mdemo.py\u001b[0m:\u001b[94m20\u001b[0m in \u001b[92mmain\u001b[0m                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   │   \u001b[0msetup_torch_distributed(distributed_args)                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m18 \u001b[0m\u001b[2m│   \u001b[0mcheckpoint_path = os.path.join(\u001b[33m\"\u001b[0m\u001b[33mhf-weights\u001b[0m\u001b[33m\"\u001b[0m, model_name)            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mLoading BLT model: \u001b[0m\u001b[33m{\u001b[0mmodel_name\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m20 \u001b[2m│   \u001b[0mmodel, tokenizer, train_cfg = \u001b[1;4mload_consolidated_model_and_tokenizer\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mcheckpoint_path,\u001b[0m                                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[1;2;4m│   \u001b[0m\u001b[1;4m)\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m23 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94massert\u001b[0m \u001b[96misinstance\u001b[0m(model, ByteLatentTransformer)                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m─────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m──────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  checkpoint_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m                              \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m distributed_args = \u001b[1;35mDistributedArgs\u001b[0m\u001b[1m(\u001b[0m                                 \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdp_shard\u001b[0m=\u001b[94m1\u001b[0m,                                  \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdp_replicate\u001b[0m=\u001b[94m1\u001b[0m,                              \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mtp_size\u001b[0m=\u001b[94m1\u001b[0m,                                   \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mselective_activation_checkpointing\u001b[0m=\u001b[94mFalse\u001b[0m,    \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mcompile\u001b[0m=\u001b[94mFalse\u001b[0m,                               \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfsdp_type\u001b[0m=\u001b[33m'no_shard'\u001b[0m,                        \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mmodel_dtype\u001b[0m=\u001b[33m'bf16'\u001b[0m,                          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfloat8_recipe\u001b[0m=\u001b[94mNone\u001b[0m,                          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfloat8_filter\u001b[0m=\u001b[33m'layers\\\\.\u001b[0m\u001b[1;33m[\u001b[0m\u001b[33m0-9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[33m+\\\\.'\u001b[0m,          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mmatmul_allow_tf32\u001b[0m=\u001b[94mFalse\u001b[0m,                     \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mallow_bf16_reduced_precision_reduction\u001b[0m=\u001b[94mTrue\u001b[0m, \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdetect_anomaly\u001b[0m=\u001b[94mFalse\u001b[0m,                        \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mcompile_cache_size_limit\u001b[0m=\u001b[94m8\u001b[0m,                  \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mspawn_method\u001b[0m=\u001b[33m'forkserver'\u001b[0m                    \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[1m)\u001b[0m                                                \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       model_name = \u001b[33m'blt-1b'\u001b[0m                                         \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           prompt = \u001b[33m'hi'\u001b[0m                                             \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰─────────────────────────────────────────────────────────────────────╯\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/content/blt/bytelatent/\u001b[0m\u001b[1;33mgenerate.py\u001b[0m:\u001b[94m18\u001b[0m in                                    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[92mload_consolidated_model_and_tokenizer\u001b[0m                                        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33mتحميل النموذج والمعالج من Hugging Face بطريقة مرنة تدعم التوزيع وال\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m\\U0001F4E6\u001b[0m\u001b[33m تحميل المعالج من: \u001b[0m\u001b[33m{\u001b[0mcheckpoint_path\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m18 \u001b[2m│   \u001b[0mtokenizer = \u001b[1;4mAutoTokenizer.from_pretrained(\u001b[0m                          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mcheckpoint_path,\u001b[0m                                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mtrust_remote_code=trust_remote_code,\u001b[0m                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[1;2;4m│   \u001b[0m\u001b[1;4m)\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m───────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────\u001b[0m\u001b[33m─╮\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   checkpoint_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        device_map = \u001b[94mNone\u001b[0m                \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  init_distributed = \u001b[94mFalse\u001b[0m               \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                  \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m low_cpu_mem_usage = \u001b[94mFalse\u001b[0m               \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       torch_dtype = \u001b[94mNone\u001b[0m                \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m trust_remote_code = \u001b[94mTrue\u001b[0m                \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   use_safetensors = \u001b[94mTrue\u001b[0m                \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰─────────────────────────────────────────╯\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/\u001b[0m\u001b[1;33mtokenizatio\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[1;33mn_auto.py\u001b[0m:\u001b[94m966\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 963 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mconfig_dict = load_gguf_checkpoint(gguf_path, ret \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 964 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mconfig = AutoConfig.for_model(**config_dict)      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 965 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 966 \u001b[2m│   │   │   │   │   \u001b[0mconfig = \u001b[1;4mAutoConfig.from_pretrained(\u001b[0m              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 967 \u001b[0m\u001b[1;2;4m│   │   │   │   │   │   \u001b[0m\u001b[1;4mpretrained_model_name_or_path, trust_remote_c\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 968 \u001b[0m\u001b[1;2;4m│   │   │   │   │   \u001b[0m\u001b[1;4m)\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 969 \u001b[0m\u001b[2m│   │   │   \u001b[0mconfig_tokenizer_class = config.tokenizer_class           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m──────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m──────────────────────\u001b[0m\u001b[33m─╮\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        config = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        config_tokenizer_class = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     gguf_file = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        inputs = \u001b[1m(\u001b[0m\u001b[1m)\u001b[0m                   \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        kwargs = \u001b[1m{\u001b[0m\u001b[33m'_from_auto'\u001b[0m: \u001b[94mTrue\u001b[0m\u001b[1m}\u001b[0m \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m pretrained_model_name_or_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m  \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            tokenizer_auto_map = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              tokenizer_config = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                   \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                tokenizer_type = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             trust_remote_code = \u001b[94mTrue\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                use_auth_token = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      use_fast = \u001b[94mTrue\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────╯\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/\u001b[0m\u001b[1;33mconfigurati\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[1;33mon_auto.py\u001b[0m:\u001b[94m1151\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m                                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1148 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m pattern \u001b[95min\u001b[0m \u001b[96mstr\u001b[0m(pretrained_model_name_or_path):     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1149 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m CONFIG_MAPPING[pattern].from_dict(config_d \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1150 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1151 \u001b[2m│   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;96mValueError\u001b[0m\u001b[1;4m(\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1152 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;33mf\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mUnrecognized model in \u001b[0m\u001b[1;4;33m{\u001b[0m\u001b[1;4mpretrained_model_name_or_path\u001b[0m\u001b[1;4;33m}\u001b[0m\u001b[1;4;33m. \u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1153 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;33mf\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mShould have a `model_type` key in its \u001b[0m\u001b[1;4;33m{\u001b[0m\u001b[1;4mCONFIG_NAME\u001b[0m\u001b[1;4;33m}\u001b[0m\u001b[1;4;33m, or\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1154 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;33mf\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33min its name: \u001b[0m\u001b[1;4;33m{\u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4;33m, \u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4m.join(CONFIG_MAPPING.keys())\u001b[0m\u001b[1;4;33m}\u001b[0m\u001b[1;4;33m\"\u001b[0m        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 code_revision = \u001b[94mNone\u001b[0m                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   config_dict = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                has_local_code = \u001b[94mFalse\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               has_remote_code = \u001b[94mFalse\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        kwargs = \u001b[1m{\u001b[0m                                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                 \u001b[2m│   \u001b[0m\u001b[33m'_from_auto'\u001b[0m: \u001b[94mTrue\u001b[0m,                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                 \u001b[2m│   \u001b[0m\u001b[33m'name_or_path'\u001b[0m: \u001b[33m'hf-weights/blt-1b'\u001b[0m  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                 \u001b[1m}\u001b[0m                                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                       pattern = \u001b[33m't5'\u001b[0m                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m pretrained_model_name_or_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             trust_remote_code = \u001b[94mTrue\u001b[0m                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 unused_kwargs = \u001b[1m{\u001b[0m\u001b[33m'name_or_path'\u001b[0m: \u001b[33m'hf-weights/blt-1b'\u001b[0m\u001b[1m}\u001b[0m    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                use_auth_token = \u001b[94mNone\u001b[0m                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "[rank0]: \u001b[1;91mValueError: \u001b[0mUnrecognized model in hf-weights/blt-1b. Should have a `model_type` \n",
            "[rank0]: key in its config.json, or contain one of the following strings in its name: \n",
            "[rank0]: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, \n",
            "[rank0]: autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, \n",
            "[rank0]: big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, \n",
            "[rank0]: blip-\u001b[1;36m2\u001b[0m, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, \n",
            "[rank0]: chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, \n",
            "[rank0]: clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, \n",
            "[rank0]: convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, \n",
            "[rank0]: data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, \n",
            "[rank0]: decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, \n",
            "[rank0]: depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, \n",
            "[rank0]: distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, \n",
            "[rank0]: encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, \n",
            "[rank0]: fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, \n",
            "[rank0]: gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, \n",
            "[rank0]: gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, \n",
            "[rank0]: granite, granitemoe, granitemoeshared, granitevision, graphormer, \n",
            "[rank0]: grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, \n",
            "[rank0]: idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, \n",
            "[rank0]: instructblipvideo, jamba, jetmoe, jukebox, kosmos-\u001b[1;36m2\u001b[0m, layoutlm, layoutlmv2, \n",
            "[rank0]: layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, \n",
            "[rank0]: llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, \n",
            "[rank0]: mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, \n",
            "[rank0]: mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, \n",
            "[rank0]: mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, \n",
            "[rank0]: modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, \n",
            "[rank0]: mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, \n",
            "[rank0]: omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, \n",
            "[rank0]: patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, \n",
            "[rank0]: phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, \n",
            "[rank0]: prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, \n",
            "[rank0]: qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, \n",
            "[rank0]: realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, \n",
            "[rank0]: roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, \n",
            "[rank0]: rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, \n",
            "[rank0]: sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, \n",
            "[rank0]: smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, \n",
            "[rank0]: speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, \n",
            "[rank0]: swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, \n",
            "[rank0]: tapas, textnet, time_series_transformer, timesformer, timm_backbone, \n",
            "[rank0]: timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, \n",
            "[rank0]: unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, \n",
            "[rank0]: vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, \n",
            "[rank0]: vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits,\n",
            "[rank0]: vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm,\n",
            "[rank0]: xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, \n",
            "[rank0]: zamba, zamba2, zoedepth\n",
            "[rank0]:[W423 21:52:21.678463566 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/blt/demo.py --model-name /content/blt/hf-weights/blt-1b \"hi\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H82hJe3MovP_",
        "outputId": "7c1ffa33-12be-4ece-d12a-b4da9fa63f3f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_worker/__main__.py\", line 7, in <module>\n",
            "    from torch._inductor.async_compile import pre_fork_setup\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/async_compile.py\", line 16, in <module>\n",
            "    from torch._dynamo.device_interface import get_registered_device_interfaces\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/__init__.py\", line 3, in <module>\n",
            "    from . import convert_frame, eval_frame, resume_execution\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\", line 53, in <module>\n",
            "    from . import config, exc, trace_rules\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/trace_rules.py\", line 46, in <module>\n",
            "    from .variables import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/__init__.py\", line 65, in <module>\n",
            "    from .misc import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/misc.py\", line 1339, in <module>\n",
            "    class DeletedVariable(VariableTracker):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/base.py\", line 113, in __init__\n",
            "    def __init__(cls, name, bases, attrs) -> None:\n",
            "\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fMBtCIAoqHx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKOumxFupEBX",
        "outputId": "715f941d-d744-45ba-f6e7-3906d8a3f6a4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/blt\n",
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `read` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `read`\n",
            "Loading BLT model: hf-weights/blt-1b\n",
            "📦 تحميل المعالج من: hf-weights/hf-weights/blt-1b\n",
            "[rank0]: \u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/utils/\u001b[0m\u001b[1;33mhub.py\u001b[0m:\u001b[94m424\u001b[0m in     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[92mcached_files\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 421 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mtry\u001b[0m:                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 422 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mlen\u001b[0m(full_filenames) == \u001b[94m1\u001b[0m:                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 423 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# This is slightly better for only 1 file\u001b[0m                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 424 \u001b[2m│   │   │   \u001b[0mhf_hub_download(                                          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 425 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mpath_or_repo_id,                                      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 426 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mfilenames[\u001b[94m0\u001b[0m],                                         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 427 \u001b[0m\u001b[2m│   │   │   │   \u001b[0msubfolder=\u001b[94mNone\u001b[0m \u001b[94mif\u001b[0m \u001b[96mlen\u001b[0m(subfolder) == \u001b[94m0\u001b[0m \u001b[94melse\u001b[0m subfolder, \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      _commit_hash = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m _raise_exceptions_for_connection_e… \u001b[94mFalse\u001b[0m                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                   =                                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  _raise_exceptions_for_gated_repo = \u001b[94mFalse\u001b[0m                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m _raise_exceptions_for_missing_entr… \u001b[94mFalse\u001b[0m                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                   =                                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         cache_dir = \u001b[33m'/root/.cache/huggingface/hub'\u001b[0m       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 deprecated_kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    existing_files = \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      file_counter = \u001b[94m0\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                          filename = \u001b[33m'tokenizer_config.json'\u001b[0m              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         filenames = \u001b[1m[\u001b[0m\u001b[33m'tokenizer_config.json'\u001b[0m\u001b[1m]\u001b[0m            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    force_download = \u001b[94mFalse\u001b[0m                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    full_filenames = \u001b[1m[\u001b[0m\u001b[33m'tokenizer_config.json'\u001b[0m\u001b[1m]\u001b[0m            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  local_files_only = \u001b[94mFalse\u001b[0m                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   path_or_repo_id = \u001b[33m'hf-weights/hf-weights/blt-1b'\u001b[0m       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           proxies = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         repo_type = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   resume_download = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                          revision = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         subfolder = \u001b[33m''\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             token = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    use_auth_token = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        user_agent = \u001b[33m'transformers/4.51.3; \u001b[0m               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                     \u001b[33mpython/3.11.12; \u001b[0m                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                     \u001b[33msession_id/8ef7d037498b49bca28342d6…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/\u001b[0m\u001b[1;33m_validators.py\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m :\u001b[94m106\u001b[0m in \u001b[92m_inner_fn\u001b[0m                                                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m103 \u001b[0m\u001b[2m│   │   │   \u001b[0mkwargs.items(),  \u001b[2m# Kwargs values\u001b[0m                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m104 \u001b[0m\u001b[2m│   │   \u001b[0m):                                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m105 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m arg_name \u001b[95min\u001b[0m [\u001b[33m\"\u001b[0m\u001b[33mrepo_id\u001b[0m\u001b[33m\"\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33mfrom_id\u001b[0m\u001b[33m\"\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33mto_id\u001b[0m\u001b[33m\"\u001b[0m]:            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m106 \u001b[2m│   │   │   │   \u001b[0m\u001b[1;4mvalidate_repo_id(arg_value)\u001b[0m                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m107 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m108 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melif\u001b[0m arg_name == \u001b[33m\"\u001b[0m\u001b[33mtoken\u001b[0m\u001b[33m\"\u001b[0m \u001b[95mand\u001b[0m arg_value \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m109 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mhas_token = \u001b[94mTrue\u001b[0m                                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             arg_name = \u001b[33m'repo_id'\u001b[0m                                         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            arg_value = \u001b[33m'hf-weights/hf-weights/blt-1b'\u001b[0m                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 args = \u001b[1m(\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'hf-weights/hf-weights/blt-1b'\u001b[0m,               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'tokenizer_config.json'\u001b[0m                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[1m)\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m check_use_auth_token = \u001b[94mTrue\u001b[0m                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            has_token = \u001b[94mFalse\u001b[0m                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               kwargs = \u001b[1m{\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'subfolder'\u001b[0m: \u001b[94mNone\u001b[0m,                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'repo_type'\u001b[0m: \u001b[94mNone\u001b[0m,                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'revision'\u001b[0m: \u001b[94mNone\u001b[0m,                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'cache_dir'\u001b[0m: \u001b[33m'/root/.cache/huggingface/hub'\u001b[0m,  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'user_agent'\u001b[0m: \u001b[33m'transformers/4.51.3; \u001b[0m          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[33mpython/3.11.12; \u001b[0m                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[33msession_id/8ef7d037498b49bca28342d640b9cf14'\u001b[0m+\u001b[94m38\u001b[0m,  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'force_download'\u001b[0m: \u001b[94mFalse\u001b[0m,                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'proxies'\u001b[0m: \u001b[94mNone\u001b[0m,                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'resume_download'\u001b[0m: \u001b[94mNone\u001b[0m,                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'token'\u001b[0m: \u001b[94mNone\u001b[0m,                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'local_files_only'\u001b[0m: \u001b[94mFalse\u001b[0m                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[1m}\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            signature = \u001b[1m<\u001b[0m\u001b[1;95mSignature\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mrepo_id: str, filename: str, *, \u001b[0m      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39msubfolder: Optional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, repo_type: \u001b[0m      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mOptional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, revision: Optional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[94mNone\u001b[0m\u001b[39m, library_name: Optional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, \u001b[0m        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mlibrary_version: Optional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, cache_dir:\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mUnion\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr, pathlib.Path, NoneType\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, \u001b[0m       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mlocal_dir: Union\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr, pathlib.Path, NoneType\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[94mNone\u001b[0m\u001b[39m, user_agent: Union\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mDict, str, NoneType\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[94mNone\u001b[0m\u001b[39m, force_download: bool = \u001b[0m\u001b[94mFalse\u001b[0m\u001b[39m, proxies: \u001b[0m     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mOptional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mDict\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, etag_timeout: float = \u001b[0m\u001b[94m10\u001b[0m\u001b[39m, \u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mtoken: Union\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mbool, str, NoneType\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, \u001b[0m        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mlocal_files_only: bool = \u001b[0m\u001b[94mFalse\u001b[0m\u001b[39m, headers: \u001b[0m         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mOptional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mDict\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr, str\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, endpoint: \u001b[0m       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mOptional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, resume_download: \u001b[0m           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mOptional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mbool\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, force_filename: \u001b[0m           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mOptional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, local_dir_use_symlinks: \u001b[0m    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mUnion\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mbool, Literal\u001b[0m\u001b[1;39m[\u001b[0m\u001b[33m'auto'\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[33m'auto'\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m -> str\u001b[0m\u001b[1m>\u001b[0m    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/\u001b[0m\u001b[1;33m_validators.py\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m :\u001b[94m154\u001b[0m in \u001b[92mvalidate_repo_id\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m151 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m HFValidationError(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mRepo id must be a string, not \u001b[0m\u001b[33m{\u001b[0m\u001b[96mtype\u001b[0m( \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m152 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m153 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m repo_id.count(\u001b[33m\"\u001b[0m\u001b[33m/\u001b[0m\u001b[33m\"\u001b[0m) > \u001b[94m1\u001b[0m:                                         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m154 \u001b[2m│   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m HFValidationError(\u001b[0m                                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m155 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mRepo id must be in the form \u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4;33mrepo_name\u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4;33m or \u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4;33mnamespace/rep\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m156 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;33mf\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33m \u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4;33m{\u001b[0m\u001b[1;4mrepo_id\u001b[0m\u001b[1;4;33m}\u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4;33m. Use `repo_type` argument if needed.\u001b[0m\u001b[1;4;33m\"\u001b[0m       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m157 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4m)\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────\u001b[0m\u001b[33m─╮\u001b[0m                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m repo_id = \u001b[33m'hf-weights/hf-weights/blt-1b'\u001b[0m \u001b[33m│\u001b[0m                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────╯\u001b[0m                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "[rank0]: \u001b[1;91mHFValidationError: \u001b[0mRepo id must be in the form \u001b[32m'repo_name'\u001b[0m or \n",
            "[rank0]: \u001b[32m'namespace/repo_name'\u001b[0m: \u001b[32m'hf-weights/hf-weights/blt-1b'\u001b[0m. Use `repo_type` argument \n",
            "[rank0]: if needed.\n",
            "\n",
            "[rank0]: \u001b[3mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
            "\n",
            "[rank0]: \u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/content/blt/\u001b[0m\u001b[1;33mdemo.py\u001b[0m:\u001b[94m20\u001b[0m in \u001b[92mmain\u001b[0m                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   │   \u001b[0msetup_torch_distributed(distributed_args)                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m18 \u001b[0m\u001b[2m│   \u001b[0mcheckpoint_path = os.path.join(\u001b[33m\"\u001b[0m\u001b[33mhf-weights\u001b[0m\u001b[33m\"\u001b[0m, model_name)            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mLoading BLT model: \u001b[0m\u001b[33m{\u001b[0mmodel_name\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m20 \u001b[2m│   \u001b[0mmodel, tokenizer, train_cfg = \u001b[1;4mload_consolidated_model_and_tokenizer\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mcheckpoint_path,\u001b[0m                                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[1;2;4m│   \u001b[0m\u001b[1;4m)\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m23 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94massert\u001b[0m \u001b[96misinstance\u001b[0m(model, ByteLatentTransformer)                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m─────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m──────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  checkpoint_path = \u001b[33m'hf-weights/hf-weights/blt-1b'\u001b[0m                   \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m distributed_args = \u001b[1;35mDistributedArgs\u001b[0m\u001b[1m(\u001b[0m                                 \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdp_shard\u001b[0m=\u001b[94m1\u001b[0m,                                  \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdp_replicate\u001b[0m=\u001b[94m1\u001b[0m,                              \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mtp_size\u001b[0m=\u001b[94m1\u001b[0m,                                   \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mselective_activation_checkpointing\u001b[0m=\u001b[94mFalse\u001b[0m,    \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mcompile\u001b[0m=\u001b[94mFalse\u001b[0m,                               \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfsdp_type\u001b[0m=\u001b[33m'no_shard'\u001b[0m,                        \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mmodel_dtype\u001b[0m=\u001b[33m'bf16'\u001b[0m,                          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfloat8_recipe\u001b[0m=\u001b[94mNone\u001b[0m,                          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfloat8_filter\u001b[0m=\u001b[33m'layers\\\\.\u001b[0m\u001b[1;33m[\u001b[0m\u001b[33m0-9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[33m+\\\\.'\u001b[0m,          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mmatmul_allow_tf32\u001b[0m=\u001b[94mFalse\u001b[0m,                     \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mallow_bf16_reduced_precision_reduction\u001b[0m=\u001b[94mTrue\u001b[0m, \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdetect_anomaly\u001b[0m=\u001b[94mFalse\u001b[0m,                        \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mcompile_cache_size_limit\u001b[0m=\u001b[94m8\u001b[0m,                  \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mspawn_method\u001b[0m=\u001b[33m'forkserver'\u001b[0m                    \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[1m)\u001b[0m                                                \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       model_name = \u001b[33m'hf-weights/blt-1b'\u001b[0m                              \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           prompt = \u001b[33m'hi'\u001b[0m                                             \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰─────────────────────────────────────────────────────────────────────╯\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/content/blt/bytelatent/\u001b[0m\u001b[1;33mgenerate.py\u001b[0m:\u001b[94m18\u001b[0m in                                    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[92mload_consolidated_model_and_tokenizer\u001b[0m                                        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33mتحميل النموذج والمعالج من Hugging Face بطريقة مرنة تدعم التوزيع وال\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m\\U0001F4E6\u001b[0m\u001b[33m تحميل المعالج من: \u001b[0m\u001b[33m{\u001b[0mcheckpoint_path\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m18 \u001b[2m│   \u001b[0mtokenizer = \u001b[1;4mAutoTokenizer.from_pretrained(\u001b[0m                          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mcheckpoint_path,\u001b[0m                                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mtrust_remote_code=trust_remote_code,\u001b[0m                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[1;2;4m│   \u001b[0m\u001b[1;4m)\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m─────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m─────────────────────\u001b[0m\u001b[33m─╮\u001b[0m                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   checkpoint_path = \u001b[33m'hf-weights/hf-weights/blt-1b'\u001b[0m \u001b[33m│\u001b[0m                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        device_map = \u001b[94mNone\u001b[0m                           \u001b[33m│\u001b[0m                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  init_distributed = \u001b[94mFalse\u001b[0m                          \u001b[33m│\u001b[0m                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                             \u001b[33m│\u001b[0m                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m low_cpu_mem_usage = \u001b[94mFalse\u001b[0m                          \u001b[33m│\u001b[0m                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       torch_dtype = \u001b[94mNone\u001b[0m                           \u001b[33m│\u001b[0m                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m trust_remote_code = \u001b[94mTrue\u001b[0m                           \u001b[33m│\u001b[0m                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   use_safetensors = \u001b[94mTrue\u001b[0m                           \u001b[33m│\u001b[0m                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰────────────────────────────────────────────────────╯\u001b[0m                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/\u001b[0m\u001b[1;33mtokenizatio\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[1;33mn_auto.py\u001b[0m:\u001b[94m946\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 943 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m tokenizer_class.from_pretrained(pretrained_model_n \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 944 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 945 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Next, let's try to use the tokenizer_config file to get the\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 946 \u001b[2m│   │   \u001b[0mtokenizer_config = \u001b[1;4mget_tokenizer_config(pretrained_model_name\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 947 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[33m\"\u001b[0m\u001b[33m_commit_hash\u001b[0m\u001b[33m\"\u001b[0m \u001b[95min\u001b[0m tokenizer_config:                        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 948 \u001b[0m\u001b[2m│   │   │   \u001b[0mkwargs[\u001b[33m\"\u001b[0m\u001b[33m_commit_hash\u001b[0m\u001b[33m\"\u001b[0m] = tokenizer_config[\u001b[33m\"\u001b[0m\u001b[33m_commit_hash\u001b[0m\u001b[33m\"\u001b[0m] \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 949 \u001b[0m\u001b[2m│   │   \u001b[0mconfig_tokenizer_class = tokenizer_config.get(\u001b[33m\"\u001b[0m\u001b[33mtokenizer_clas\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m───────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m───────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        config = \u001b[94mNone\u001b[0m                           \u001b[33m│\u001b[0m           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     gguf_file = \u001b[94mNone\u001b[0m                           \u001b[33m│\u001b[0m           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        inputs = \u001b[1m(\u001b[0m\u001b[1m)\u001b[0m                             \u001b[33m│\u001b[0m           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        kwargs = \u001b[1m{\u001b[0m\u001b[33m'_from_auto'\u001b[0m: \u001b[94mTrue\u001b[0m\u001b[1m}\u001b[0m           \u001b[33m│\u001b[0m           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m pretrained_model_name_or_path = \u001b[33m'hf-weights/hf-weights/blt-1b'\u001b[0m \u001b[33m│\u001b[0m           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                tokenizer_type = \u001b[94mNone\u001b[0m                           \u001b[33m│\u001b[0m           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             trust_remote_code = \u001b[94mTrue\u001b[0m                           \u001b[33m│\u001b[0m           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                use_auth_token = \u001b[94mNone\u001b[0m                           \u001b[33m│\u001b[0m           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      use_fast = \u001b[94mTrue\u001b[0m                           \u001b[33m│\u001b[0m           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰────────────────────────────────────────────────────────────────╯\u001b[0m           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/\u001b[0m\u001b[1;33mtokenizatio\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[1;33mn_auto.py\u001b[0m:\u001b[94m778\u001b[0m in \u001b[92mget_tokenizer_config\u001b[0m                                        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 775 \u001b[0m\u001b[2m│   │   \u001b[0mtoken = use_auth_token                                        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 776 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 777 \u001b[0m\u001b[2m│   \u001b[0mcommit_hash = kwargs.get(\u001b[33m\"\u001b[0m\u001b[33m_commit_hash\u001b[0m\u001b[33m\"\u001b[0m, \u001b[94mNone\u001b[0m)                    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 778 \u001b[2m│   \u001b[0mresolved_config_file = cached_file(                               \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 779 \u001b[0m\u001b[2m│   │   \u001b[0mpretrained_model_name_or_path,                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 780 \u001b[0m\u001b[2m│   │   \u001b[0mTOKENIZER_CONFIG_FILE,                                        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 781 \u001b[0m\u001b[2m│   │   \u001b[0mcache_dir=cache_dir,                                          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m───────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m───────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     cache_dir = \u001b[94mNone\u001b[0m                           \u001b[33m│\u001b[0m           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   commit_hash = \u001b[94mNone\u001b[0m                           \u001b[33m│\u001b[0m           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                force_download = \u001b[94mFalse\u001b[0m                          \u001b[33m│\u001b[0m           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        kwargs = \u001b[1m{\u001b[0m\u001b[33m'_from_auto'\u001b[0m: \u001b[94mTrue\u001b[0m\u001b[1m}\u001b[0m           \u001b[33m│\u001b[0m           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              local_files_only = \u001b[94mFalse\u001b[0m                          \u001b[33m│\u001b[0m           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m pretrained_model_name_or_path = \u001b[33m'hf-weights/hf-weights/blt-1b'\u001b[0m \u001b[33m│\u001b[0m           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                       proxies = \u001b[94mNone\u001b[0m                           \u001b[33m│\u001b[0m           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               resume_download = \u001b[94mNone\u001b[0m                           \u001b[33m│\u001b[0m           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      revision = \u001b[94mNone\u001b[0m                           \u001b[33m│\u001b[0m           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     subfolder = \u001b[33m''\u001b[0m                             \u001b[33m│\u001b[0m           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         token = \u001b[94mNone\u001b[0m                           \u001b[33m│\u001b[0m           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                use_auth_token = \u001b[94mNone\u001b[0m                           \u001b[33m│\u001b[0m           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰────────────────────────────────────────────────────────────────╯\u001b[0m           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/utils/\u001b[0m\u001b[1;33mhub.py\u001b[0m:\u001b[94m266\u001b[0m in     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[92mcached_file\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 263 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33mmodel_weights_file = cached_file(\"google-bert/bert-base-uncased\",\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 264 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m```\u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 265 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 266 \u001b[2m│   \u001b[0mfile = \u001b[1;4mcached_files(path_or_repo_id=path_or_repo_id, filenames=[f\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 267 \u001b[0m\u001b[2m│   \u001b[0mfile = file[\u001b[94m0\u001b[0m] \u001b[94mif\u001b[0m file \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m file                      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 268 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m file                                                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 269 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m──────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m───────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        filename = \u001b[33m'tokenizer_config.json'\u001b[0m                             \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          kwargs = \u001b[1m{\u001b[0m                                                   \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m'cache_dir'\u001b[0m: \u001b[94mNone\u001b[0m,                              \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m'force_download'\u001b[0m: \u001b[94mFalse\u001b[0m,                        \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m'resume_download'\u001b[0m: \u001b[94mNone\u001b[0m,                        \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m'proxies'\u001b[0m: \u001b[94mNone\u001b[0m,                                \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m'token'\u001b[0m: \u001b[94mNone\u001b[0m,                                  \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m'revision'\u001b[0m: \u001b[94mNone\u001b[0m,                               \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m'local_files_only'\u001b[0m: \u001b[94mFalse\u001b[0m,                      \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m'subfolder'\u001b[0m: \u001b[33m''\u001b[0m,                                \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m'_raise_exceptions_for_gated_repo'\u001b[0m: \u001b[94mFalse\u001b[0m,      \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m'_raise_exceptions_for_missing_entries'\u001b[0m: \u001b[94mFalse\u001b[0m, \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m...\u001b[0m +\u001b[94m2\u001b[0m                                          \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[1m}\u001b[0m                                                   \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m path_or_repo_id = \u001b[33m'hf-weights/hf-weights/blt-1b'\u001b[0m                      \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰───────────────────────────────────────────────────────────────────────╯\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/utils/\u001b[0m\u001b[1;33mhub.py\u001b[0m:\u001b[94m470\u001b[0m in     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[92mcached_files\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 467 \u001b[0m\u001b[2m│   │   │   \u001b[0m) \u001b[94mfrom\u001b[0m \u001b[4;96me\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 468 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 469 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Now we try to recover if we can find all files correctly in\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 470 \u001b[2m│   │   \u001b[0mresolved_files = \u001b[1;4m[\u001b[0m                                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 471 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4m_get_cache_file_to_return(path_or_repo_id, filename, cach\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 472 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4m]\u001b[0m                                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 473 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mall\u001b[0m(file \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94mfor\u001b[0m file \u001b[95min\u001b[0m resolved_files):          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      _commit_hash = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m _raise_exceptions_for_connection_e… \u001b[94mFalse\u001b[0m                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                   =                                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  _raise_exceptions_for_gated_repo = \u001b[94mFalse\u001b[0m                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m _raise_exceptions_for_missing_entr… \u001b[94mFalse\u001b[0m                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                   =                                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         cache_dir = \u001b[33m'/root/.cache/huggingface/hub'\u001b[0m       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 deprecated_kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    existing_files = \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      file_counter = \u001b[94m0\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                          filename = \u001b[33m'tokenizer_config.json'\u001b[0m              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         filenames = \u001b[1m[\u001b[0m\u001b[33m'tokenizer_config.json'\u001b[0m\u001b[1m]\u001b[0m            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    force_download = \u001b[94mFalse\u001b[0m                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    full_filenames = \u001b[1m[\u001b[0m\u001b[33m'tokenizer_config.json'\u001b[0m\u001b[1m]\u001b[0m            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  local_files_only = \u001b[94mFalse\u001b[0m                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   path_or_repo_id = \u001b[33m'hf-weights/hf-weights/blt-1b'\u001b[0m       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           proxies = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         repo_type = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   resume_download = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                          revision = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         subfolder = \u001b[33m''\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             token = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    use_auth_token = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        user_agent = \u001b[33m'transformers/4.51.3; \u001b[0m               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                     \u001b[33mpython/3.11.12; \u001b[0m                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                     \u001b[33msession_id/8ef7d037498b49bca28342d6…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/utils/\u001b[0m\u001b[1;33mhub.py\u001b[0m:\u001b[94m471\u001b[0m in     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[92m<listcomp>\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 468 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 469 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Now we try to recover if we can find all files correctly in\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 470 \u001b[0m\u001b[2m│   │   \u001b[0mresolved_files = [                                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 471 \u001b[2m│   │   │   \u001b[0m\u001b[1;4m_get_cache_file_to_return(path_or_repo_id, filename, cach\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 472 \u001b[0m\u001b[2m│   │   \u001b[0m]                                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 473 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mall\u001b[0m(file \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94mfor\u001b[0m file \u001b[95min\u001b[0m resolved_files):          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 474 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m resolved_files                                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m─────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m─────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m               \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              .0 = \u001b[1m<\u001b[0m\u001b[1;95mlist_iterator\u001b[0m\u001b[39m object at \u001b[0m\u001b[94m0x1297926e6a70\u001b[0m\u001b[1m>\u001b[0m \u001b[33m│\u001b[0m               \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       cache_dir = \u001b[33m'/root/.cache/huggingface/hub'\u001b[0m           \u001b[33m│\u001b[0m               \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        filename = \u001b[33m'tokenizer_config.json'\u001b[0m                  \u001b[33m│\u001b[0m               \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m path_or_repo_id = \u001b[33m'hf-weights/hf-weights/blt-1b'\u001b[0m           \u001b[33m│\u001b[0m               \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        revision = \u001b[94mNone\u001b[0m                                     \u001b[33m│\u001b[0m               \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰────────────────────────────────────────────────────────────╯\u001b[0m               \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/utils/\u001b[0m\u001b[1;33mhub.py\u001b[0m:\u001b[94m134\u001b[0m in     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[92m_get_cache_file_to_return\u001b[0m                                                    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 131 \u001b[0m\u001b[2m│   \u001b[0mpath_or_repo_id: \u001b[96mstr\u001b[0m, full_filename: \u001b[96mstr\u001b[0m, cache_dir: Union[\u001b[96mstr\u001b[0m, P \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 132 \u001b[0m):                                                                    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 133 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# We try to see if we have a cached version (not up to date):\u001b[0m     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 134 \u001b[2m│   \u001b[0mresolved_file = \u001b[1;4mtry_to_load_from_cache(path_or_repo_id, full_file\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 135 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m resolved_file \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mand\u001b[0m resolved_file != _CACHED_NO_EXIS \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 136 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m resolved_file                                          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 137 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[94mNone\u001b[0m                                                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────\u001b[0m\u001b[33m─╮\u001b[0m                         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       cache_dir = \u001b[33m'/root/.cache/huggingface/hub'\u001b[0m \u001b[33m│\u001b[0m                         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   full_filename = \u001b[33m'tokenizer_config.json'\u001b[0m        \u001b[33m│\u001b[0m                         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m path_or_repo_id = \u001b[33m'hf-weights/hf-weights/blt-1b'\u001b[0m \u001b[33m│\u001b[0m                         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        revision = \u001b[94mNone\u001b[0m                           \u001b[33m│\u001b[0m                         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────╯\u001b[0m                         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/\u001b[0m\u001b[1;33m_validators.py\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m :\u001b[94m106\u001b[0m in \u001b[92m_inner_fn\u001b[0m                                                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m103 \u001b[0m\u001b[2m│   │   │   \u001b[0mkwargs.items(),  \u001b[2m# Kwargs values\u001b[0m                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m104 \u001b[0m\u001b[2m│   │   \u001b[0m):                                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m105 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m arg_name \u001b[95min\u001b[0m [\u001b[33m\"\u001b[0m\u001b[33mrepo_id\u001b[0m\u001b[33m\"\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33mfrom_id\u001b[0m\u001b[33m\"\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33mto_id\u001b[0m\u001b[33m\"\u001b[0m]:            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m106 \u001b[2m│   │   │   │   \u001b[0m\u001b[1;4mvalidate_repo_id(arg_value)\u001b[0m                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m107 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m108 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melif\u001b[0m arg_name == \u001b[33m\"\u001b[0m\u001b[33mtoken\u001b[0m\u001b[33m\"\u001b[0m \u001b[95mand\u001b[0m arg_value \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m109 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mhas_token = \u001b[94mTrue\u001b[0m                                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             arg_name = \u001b[33m'repo_id'\u001b[0m                                         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            arg_value = \u001b[33m'hf-weights/hf-weights/blt-1b'\u001b[0m                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 args = \u001b[1m(\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'hf-weights/hf-weights/blt-1b'\u001b[0m,               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'tokenizer_config.json'\u001b[0m                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[1m)\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m check_use_auth_token = \u001b[94mFalse\u001b[0m                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            has_token = \u001b[94mFalse\u001b[0m                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               kwargs = \u001b[1m{\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'cache_dir'\u001b[0m: \u001b[33m'/root/.cache/huggingface/hub'\u001b[0m,  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'revision'\u001b[0m: \u001b[94mNone\u001b[0m                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[1m}\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            signature = \u001b[1m<\u001b[0m\u001b[1;95mSignature\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mrepo_id: str, filename: str, \u001b[0m         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mcache_dir: Union\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr, pathlib.Path, NoneType\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[94mNone\u001b[0m\u001b[39m, revision: Optional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, repo_type: \u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mOptional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m -> Union\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr, Any, \u001b[0m         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mNoneType\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1m>\u001b[0m                                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/\u001b[0m\u001b[1;33m_validators.py\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m :\u001b[94m154\u001b[0m in \u001b[92mvalidate_repo_id\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m151 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m HFValidationError(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mRepo id must be a string, not \u001b[0m\u001b[33m{\u001b[0m\u001b[96mtype\u001b[0m( \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m152 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m153 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m repo_id.count(\u001b[33m\"\u001b[0m\u001b[33m/\u001b[0m\u001b[33m\"\u001b[0m) > \u001b[94m1\u001b[0m:                                         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m154 \u001b[2m│   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m HFValidationError(\u001b[0m                                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m155 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mRepo id must be in the form \u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4;33mrepo_name\u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4;33m or \u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4;33mnamespace/rep\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m156 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;33mf\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33m \u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4;33m{\u001b[0m\u001b[1;4mrepo_id\u001b[0m\u001b[1;4;33m}\u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4;33m. Use `repo_type` argument if needed.\u001b[0m\u001b[1;4;33m\"\u001b[0m       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m157 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4m)\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────\u001b[0m\u001b[33m─╮\u001b[0m                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m repo_id = \u001b[33m'hf-weights/hf-weights/blt-1b'\u001b[0m \u001b[33m│\u001b[0m                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────╯\u001b[0m                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "[rank0]: \u001b[1;91mHFValidationError: \u001b[0mRepo id must be in the form \u001b[32m'repo_name'\u001b[0m or \n",
            "[rank0]: \u001b[32m'namespace/repo_name'\u001b[0m: \u001b[32m'hf-weights/hf-weights/blt-1b'\u001b[0m. Use `repo_type` argument \n",
            "[rank0]: if needed.\n",
            "[rank0]:[W423 22:07:49.930862158 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModel.from_pretrained(\"username/my_model\", repo_type=\"model\")\n"
      ],
      "metadata": {
        "id": "94LSDtfzpR83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0T5fD7OereqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/blt/demo.py --model-name /content/blt/hf-weights/blt-1b \"hi\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czGGHWdwqL0J",
        "outputId": "99765a82-3ed9-46ef-bc4c-79311270d932"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BLT model: /content/blt/hf-weights/blt-1b\n",
            "📦 تحميل المعالج من: /content/blt/hf-weights/blt-1b\n",
            "[rank0]: \u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/content/blt/\u001b[0m\u001b[1;33mdemo.py\u001b[0m:\u001b[94m20\u001b[0m in \u001b[92mmain\u001b[0m                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   │   \u001b[0msetup_torch_distributed(distributed_args)                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m18 \u001b[0m\u001b[2m│   \u001b[0mcheckpoint_path = os.path.join(\u001b[33m\"\u001b[0m\u001b[33mhf-weights\u001b[0m\u001b[33m\"\u001b[0m, model_name)            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mLoading BLT model: \u001b[0m\u001b[33m{\u001b[0mmodel_name\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m20 \u001b[2m│   \u001b[0mmodel, tokenizer, train_cfg = \u001b[1;4mload_consolidated_model_and_tokenizer\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mcheckpoint_path,\u001b[0m                                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[1;2;4m│   \u001b[0m\u001b[1;4m)\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m23 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94massert\u001b[0m \u001b[96misinstance\u001b[0m(model, ByteLatentTransformer)                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m─────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m──────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  checkpoint_path = \u001b[33m'/content/blt/hf-weights/blt-1b'\u001b[0m                 \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m distributed_args = \u001b[1;35mDistributedArgs\u001b[0m\u001b[1m(\u001b[0m                                 \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdp_shard\u001b[0m=\u001b[94m1\u001b[0m,                                  \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdp_replicate\u001b[0m=\u001b[94m1\u001b[0m,                              \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mtp_size\u001b[0m=\u001b[94m1\u001b[0m,                                   \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mselective_activation_checkpointing\u001b[0m=\u001b[94mFalse\u001b[0m,    \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mcompile\u001b[0m=\u001b[94mFalse\u001b[0m,                               \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfsdp_type\u001b[0m=\u001b[33m'no_shard'\u001b[0m,                        \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mmodel_dtype\u001b[0m=\u001b[33m'bf16'\u001b[0m,                          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfloat8_recipe\u001b[0m=\u001b[94mNone\u001b[0m,                          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfloat8_filter\u001b[0m=\u001b[33m'layers\\\\.\u001b[0m\u001b[1;33m[\u001b[0m\u001b[33m0-9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[33m+\\\\.'\u001b[0m,          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mmatmul_allow_tf32\u001b[0m=\u001b[94mFalse\u001b[0m,                     \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mallow_bf16_reduced_precision_reduction\u001b[0m=\u001b[94mTrue\u001b[0m, \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdetect_anomaly\u001b[0m=\u001b[94mFalse\u001b[0m,                        \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mcompile_cache_size_limit\u001b[0m=\u001b[94m8\u001b[0m,                  \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mspawn_method\u001b[0m=\u001b[33m'forkserver'\u001b[0m                    \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[1m)\u001b[0m                                                \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       model_name = \u001b[33m'/content/blt/hf-weights/blt-1b'\u001b[0m                 \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           prompt = \u001b[33m'hi'\u001b[0m                                             \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰─────────────────────────────────────────────────────────────────────╯\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/content/blt/bytelatent/\u001b[0m\u001b[1;33mgenerate.py\u001b[0m:\u001b[94m18\u001b[0m in                                    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[92mload_consolidated_model_and_tokenizer\u001b[0m                                        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33mتحميل النموذج والمعالج من Hugging Face بطريقة مرنة تدعم التوزيع وال\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m\\U0001F4E6\u001b[0m\u001b[33m تحميل المعالج من: \u001b[0m\u001b[33m{\u001b[0mcheckpoint_path\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m18 \u001b[2m│   \u001b[0mtokenizer = \u001b[1;4mAutoTokenizer.from_pretrained(\u001b[0m                          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mcheckpoint_path,\u001b[0m                                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mtrust_remote_code=trust_remote_code,\u001b[0m                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[1;2;4m│   \u001b[0m\u001b[1;4m)\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m──────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m──────────────────────\u001b[0m\u001b[33m─╮\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   checkpoint_path = \u001b[33m'/content/blt/hf-weights/blt-1b'\u001b[0m \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        device_map = \u001b[94mNone\u001b[0m                             \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  init_distributed = \u001b[94mFalse\u001b[0m                            \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                               \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m low_cpu_mem_usage = \u001b[94mFalse\u001b[0m                            \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       torch_dtype = \u001b[94mNone\u001b[0m                             \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m trust_remote_code = \u001b[94mTrue\u001b[0m                             \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   use_safetensors = \u001b[94mTrue\u001b[0m                             \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────╯\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/\u001b[0m\u001b[1;33mtokenizatio\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[1;33mn_auto.py\u001b[0m:\u001b[94m966\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 963 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mconfig_dict = load_gguf_checkpoint(gguf_path, ret \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 964 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mconfig = AutoConfig.for_model(**config_dict)      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 965 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 966 \u001b[2m│   │   │   │   │   \u001b[0mconfig = \u001b[1;4mAutoConfig.from_pretrained(\u001b[0m              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 967 \u001b[0m\u001b[1;2;4m│   │   │   │   │   │   \u001b[0m\u001b[1;4mpretrained_model_name_or_path, trust_remote_c\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 968 \u001b[0m\u001b[1;2;4m│   │   │   │   │   \u001b[0m\u001b[1;4m)\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 969 \u001b[0m\u001b[2m│   │   │   \u001b[0mconfig_tokenizer_class = config.tokenizer_class           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        config = \u001b[94mNone\u001b[0m                             \u001b[33m│\u001b[0m         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        config_tokenizer_class = \u001b[94mNone\u001b[0m                             \u001b[33m│\u001b[0m         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     gguf_file = \u001b[94mNone\u001b[0m                             \u001b[33m│\u001b[0m         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        inputs = \u001b[1m(\u001b[0m\u001b[1m)\u001b[0m                               \u001b[33m│\u001b[0m         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        kwargs = \u001b[1m{\u001b[0m\u001b[33m'_from_auto'\u001b[0m: \u001b[94mTrue\u001b[0m\u001b[1m}\u001b[0m             \u001b[33m│\u001b[0m         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m pretrained_model_name_or_path = \u001b[33m'/content/blt/hf-weights/blt-1b'\u001b[0m \u001b[33m│\u001b[0m         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            tokenizer_auto_map = \u001b[94mNone\u001b[0m                             \u001b[33m│\u001b[0m         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              tokenizer_config = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                               \u001b[33m│\u001b[0m         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                tokenizer_type = \u001b[94mNone\u001b[0m                             \u001b[33m│\u001b[0m         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             trust_remote_code = \u001b[94mTrue\u001b[0m                             \u001b[33m│\u001b[0m         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                use_auth_token = \u001b[94mNone\u001b[0m                             \u001b[33m│\u001b[0m         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      use_fast = \u001b[94mTrue\u001b[0m                             \u001b[33m│\u001b[0m         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────╯\u001b[0m         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/\u001b[0m\u001b[1;33mconfigurati\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[1;33mon_auto.py\u001b[0m:\u001b[94m1151\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m                                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1148 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m pattern \u001b[95min\u001b[0m \u001b[96mstr\u001b[0m(pretrained_model_name_or_path):     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1149 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m CONFIG_MAPPING[pattern].from_dict(config_d \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1150 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1151 \u001b[2m│   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;96mValueError\u001b[0m\u001b[1;4m(\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1152 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;33mf\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mUnrecognized model in \u001b[0m\u001b[1;4;33m{\u001b[0m\u001b[1;4mpretrained_model_name_or_path\u001b[0m\u001b[1;4;33m}\u001b[0m\u001b[1;4;33m. \u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1153 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;33mf\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mShould have a `model_type` key in its \u001b[0m\u001b[1;4;33m{\u001b[0m\u001b[1;4mCONFIG_NAME\u001b[0m\u001b[1;4;33m}\u001b[0m\u001b[1;4;33m, or\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1154 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;33mf\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33min its name: \u001b[0m\u001b[1;4;33m{\u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4;33m, \u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4m.join(CONFIG_MAPPING.keys())\u001b[0m\u001b[1;4;33m}\u001b[0m\u001b[1;4;33m\"\u001b[0m        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 code_revision = \u001b[94mNone\u001b[0m                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   config_dict = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                has_local_code = \u001b[94mFalse\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               has_remote_code = \u001b[94mFalse\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        kwargs = \u001b[1m{\u001b[0m                                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                 \u001b[2m│   \u001b[0m\u001b[33m'_from_auto'\u001b[0m: \u001b[94mTrue\u001b[0m,                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                 \u001b[2m│   \u001b[0m\u001b[33m'name_or_path'\u001b[0m:                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                 \u001b[33m'/content/blt/hf-weights/blt-1b'\u001b[0m         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                 \u001b[1m}\u001b[0m                                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                       pattern = \u001b[33m't5'\u001b[0m                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m pretrained_model_name_or_path = \u001b[33m'/content/blt/hf-weights/blt-1b'\u001b[0m         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             trust_remote_code = \u001b[94mTrue\u001b[0m                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 unused_kwargs = \u001b[1m{\u001b[0m                                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                 \u001b[2m│   \u001b[0m\u001b[33m'name_or_path'\u001b[0m:                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                 \u001b[33m'/content/blt/hf-weights/blt-1b'\u001b[0m         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                 \u001b[1m}\u001b[0m                                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                use_auth_token = \u001b[94mNone\u001b[0m                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "[rank0]: \u001b[1;91mValueError: \u001b[0mUnrecognized model in \u001b[35m/content/blt/hf-weights/\u001b[0m\u001b[95mblt-1b.\u001b[0m Should have a \n",
            "[rank0]: `model_type` key in its config.json, or contain one of the following strings in \n",
            "[rank0]: its name: albert, align, altclip, aria, aria_text, \n",
            "[rank0]: audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, \n",
            "[rank0]: bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, \n",
            "[rank0]: blenderbot-small, blip, blip-\u001b[1;36m2\u001b[0m, bloom, bridgetower, bros, camembert, canine, \n",
            "[rank0]: chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model,\n",
            "[rank0]: clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali,\n",
            "[rank0]: conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, \n",
            "[rank0]: dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, \n",
            "[rank0]: decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, \n",
            "[rank0]: depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, \n",
            "[rank0]: distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, \n",
            "[rank0]: encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, \n",
            "[rank0]: fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, \n",
            "[rank0]: gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, \n",
            "[rank0]: gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, \n",
            "[rank0]: granite, granitemoe, granitemoeshared, granitevision, graphormer, \n",
            "[rank0]: grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, \n",
            "[rank0]: idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, \n",
            "[rank0]: instructblipvideo, jamba, jetmoe, jukebox, kosmos-\u001b[1;36m2\u001b[0m, layoutlm, layoutlmv2, \n",
            "[rank0]: layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, \n",
            "[rank0]: llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, \n",
            "[rank0]: mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, \n",
            "[rank0]: mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, \n",
            "[rank0]: mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, \n",
            "[rank0]: modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, \n",
            "[rank0]: mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, \n",
            "[rank0]: omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, \n",
            "[rank0]: patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, \n",
            "[rank0]: phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, \n",
            "[rank0]: prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, \n",
            "[rank0]: qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, \n",
            "[rank0]: realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, \n",
            "[rank0]: roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, \n",
            "[rank0]: rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, \n",
            "[rank0]: sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, \n",
            "[rank0]: smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, \n",
            "[rank0]: speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, \n",
            "[rank0]: swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, \n",
            "[rank0]: tapas, textnet, time_series_transformer, timesformer, timm_backbone, \n",
            "[rank0]: timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, \n",
            "[rank0]: unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, \n",
            "[rank0]: vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, \n",
            "[rank0]: vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits,\n",
            "[rank0]: vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm,\n",
            "[rank0]: xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, \n",
            "[rank0]: zamba, zamba2, zoedepth\n",
            "[rank0]:[W423 22:01:28.424746495 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/blt/demo.py --model-name blt-1b \"hi\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPow9PVrqWzK",
        "outputId": "ef35b1ad-3ea6-486a-b627-a5b5a9142d82"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BLT model: blt-1b\n",
            "📦 تحميل المعالج من: hf-weights/blt-1b\n",
            "[rank0]: \u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/\u001b[0m\u001b[1;33m_http.py\u001b[0m:\u001b[94m409\u001b[0m   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m in \u001b[92mhf_raise_for_status\u001b[0m                                                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m406 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m</Tip>\u001b[0m                                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m407 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m408 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mtry\u001b[0m:                                                               \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m409 \u001b[2m│   │   \u001b[0mresponse.raise_for_status()                                    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m410 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mexcept\u001b[0m HTTPError \u001b[94mas\u001b[0m e:                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m411 \u001b[0m\u001b[2m│   │   \u001b[0merror_code = response.headers.get(\u001b[33m\"\u001b[0m\u001b[33mX-Error-Code\u001b[0m\u001b[33m\"\u001b[0m)              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m412 \u001b[0m\u001b[2m│   │   \u001b[0merror_message = response.headers.get(\u001b[33m\"\u001b[0m\u001b[33mX-Error-Message\u001b[0m\u001b[33m\"\u001b[0m)        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m endpoint_name = \u001b[94mNone\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m    error_code = \u001b[33m'RepoNotFound'\u001b[0m                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m error_message = \u001b[33m'Repository not found'\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       message = \u001b[33m'404 Client Error.\\n\\nRepository Not Found for url: \u001b[0m     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[33mhttps://huggingface.co/hf-weigh'\u001b[0m+\u001b[94m282\u001b[0m                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m      response = \u001b[1m<\u001b[0m\u001b[1;95mResponse\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m[\u001b[0m\u001b[94m404\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1m>\u001b[0m                                         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/requests/\u001b[0m\u001b[1;33mmodels.py\u001b[0m:\u001b[94m1024\u001b[0m in           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[92mraise_for_status\u001b[0m                                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1021 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1022 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1023 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m http_error_msg:                                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1024 \u001b[2m│   │   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m HTTPError(http_error_msg, response=\u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m)\u001b[0m            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1025 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1026 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mclose\u001b[0m(\u001b[96mself\u001b[0m):                                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1027 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33m\"\"\"Releases the connection back to the pool. Once this method\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m http_error_msg = \u001b[33m'404 Client Error: Not Found for url: \u001b[0m                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  \u001b[33mhttps://huggingface.co/hf-weights/blt-1b/re'\u001b[0m+\u001b[94m32\u001b[0m         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m         reason = \u001b[33m'Not Found'\u001b[0m                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           self = \u001b[1m<\u001b[0m\u001b[1;95mResponse\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m[\u001b[0m\u001b[94m404\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1m>\u001b[0m                                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "[rank0]: \u001b[1;91mHTTPError: \u001b[0m\u001b[1;36m404\u001b[0m Client Error: Not Found for url: \n",
            "[rank0]: \u001b[4;94mhttps://huggingface.co/hf-weights/blt-1b/resolve/main/tokenizer_config.json\u001b[0m\n",
            "\n",
            "[rank0]: \u001b[3mThe above exception was the direct cause of the following exception:\u001b[0m\n",
            "\n",
            "[rank0]: \u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/utils/\u001b[0m\u001b[1;33mhub.py\u001b[0m:\u001b[94m424\u001b[0m in     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[92mcached_files\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 421 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mtry\u001b[0m:                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 422 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mlen\u001b[0m(full_filenames) == \u001b[94m1\u001b[0m:                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 423 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# This is slightly better for only 1 file\u001b[0m                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 424 \u001b[2m│   │   │   \u001b[0mhf_hub_download(                                          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 425 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mpath_or_repo_id,                                      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 426 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mfilenames[\u001b[94m0\u001b[0m],                                         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 427 \u001b[0m\u001b[2m│   │   │   │   \u001b[0msubfolder=\u001b[94mNone\u001b[0m \u001b[94mif\u001b[0m \u001b[96mlen\u001b[0m(subfolder) == \u001b[94m0\u001b[0m \u001b[94melse\u001b[0m subfolder, \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      _commit_hash = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m _raise_exceptions_for_connection_e… \u001b[94mFalse\u001b[0m                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                   =                                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  _raise_exceptions_for_gated_repo = \u001b[94mFalse\u001b[0m                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m _raise_exceptions_for_missing_entr… \u001b[94mFalse\u001b[0m                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                   =                                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         cache_dir = \u001b[33m'/root/.cache/huggingface/hub'\u001b[0m       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 deprecated_kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    existing_files = \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      file_counter = \u001b[94m0\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                          filename = \u001b[33m'tokenizer_config.json'\u001b[0m              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         filenames = \u001b[1m[\u001b[0m\u001b[33m'tokenizer_config.json'\u001b[0m\u001b[1m]\u001b[0m            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    force_download = \u001b[94mFalse\u001b[0m                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    full_filenames = \u001b[1m[\u001b[0m\u001b[33m'tokenizer_config.json'\u001b[0m\u001b[1m]\u001b[0m            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  local_files_only = \u001b[94mFalse\u001b[0m                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   path_or_repo_id = \u001b[33m'hf-weights/blt-1b'\u001b[0m                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           proxies = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         repo_type = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   resume_download = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                          revision = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         subfolder = \u001b[33m''\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             token = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    use_auth_token = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        user_agent = \u001b[33m'transformers/4.51.3; \u001b[0m               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                     \u001b[33mpython/3.11.12; \u001b[0m                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                     \u001b[33msession_id/b7e9f295b70349878560c80c…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/\u001b[0m\u001b[1;33m_validators.py\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m :\u001b[94m114\u001b[0m in \u001b[92m_inner_fn\u001b[0m                                                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m check_use_auth_token:                                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   │   │   \u001b[0mkwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[91m__na\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m114 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4mfn(*args, **kwargs)\u001b[0m                                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m115 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m _inner_fn  \u001b[2m# type: ignore\u001b[0m                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             arg_name = \u001b[33m'local_files_only'\u001b[0m                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            arg_value = \u001b[94mFalse\u001b[0m                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 args = \u001b[1m(\u001b[0m\u001b[33m'hf-weights/blt-1b'\u001b[0m, \u001b[33m'tokenizer_config.json'\u001b[0m\u001b[1m)\u001b[0m    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m check_use_auth_token = \u001b[94mTrue\u001b[0m                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            has_token = \u001b[94mFalse\u001b[0m                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               kwargs = \u001b[1m{\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'subfolder'\u001b[0m: \u001b[94mNone\u001b[0m,                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'repo_type'\u001b[0m: \u001b[94mNone\u001b[0m,                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'revision'\u001b[0m: \u001b[94mNone\u001b[0m,                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'cache_dir'\u001b[0m: \u001b[33m'/root/.cache/huggingface/hub'\u001b[0m,  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'user_agent'\u001b[0m: \u001b[33m'transformers/4.51.3; \u001b[0m          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[33mpython/3.11.12; \u001b[0m                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[33msession_id/b7e9f295b70349878560c80c12303ba1'\u001b[0m+\u001b[94m38\u001b[0m,  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'force_download'\u001b[0m: \u001b[94mFalse\u001b[0m,                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'proxies'\u001b[0m: \u001b[94mNone\u001b[0m,                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'resume_download'\u001b[0m: \u001b[94mNone\u001b[0m,                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'token'\u001b[0m: \u001b[94mNone\u001b[0m,                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'local_files_only'\u001b[0m: \u001b[94mFalse\u001b[0m                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[1m}\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            signature = \u001b[1m<\u001b[0m\u001b[1;95mSignature\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mrepo_id: str, filename: str, *, \u001b[0m      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39msubfolder: Optional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, repo_type: \u001b[0m      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mOptional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, revision: Optional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[94mNone\u001b[0m\u001b[39m, library_name: Optional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, \u001b[0m        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mlibrary_version: Optional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, cache_dir:\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mUnion\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr, pathlib.Path, NoneType\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, \u001b[0m       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mlocal_dir: Union\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr, pathlib.Path, NoneType\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[94mNone\u001b[0m\u001b[39m, user_agent: Union\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mDict, str, NoneType\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[94mNone\u001b[0m\u001b[39m, force_download: bool = \u001b[0m\u001b[94mFalse\u001b[0m\u001b[39m, proxies: \u001b[0m     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mOptional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mDict\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, etag_timeout: float = \u001b[0m\u001b[94m10\u001b[0m\u001b[39m, \u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mtoken: Union\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mbool, str, NoneType\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, \u001b[0m        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mlocal_files_only: bool = \u001b[0m\u001b[94mFalse\u001b[0m\u001b[39m, headers: \u001b[0m         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mOptional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mDict\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr, str\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, endpoint: \u001b[0m       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mOptional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, resume_download: \u001b[0m           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mOptional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mbool\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, force_filename: \u001b[0m           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mOptional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, local_dir_use_symlinks: \u001b[0m    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mUnion\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mbool, Literal\u001b[0m\u001b[1;39m[\u001b[0m\u001b[33m'auto'\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[33m'auto'\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m -> str\u001b[0m\u001b[1m>\u001b[0m    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/\u001b[0m\u001b[1;33mfile_download.py\u001b[0m:\u001b[94m961\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m in \u001b[92mhf_hub_download\u001b[0m                                                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 958 \u001b[0m\u001b[2m│   │   │   \u001b[0mlocal_files_only=local_files_only,                        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 959 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 960 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melse\u001b[0m:                                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 961 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m _hf_hub_download_to_cache_dir(                         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 962 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Destination\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 963 \u001b[0m\u001b[2m│   │   │   \u001b[0mcache_dir=cache_dir,                                      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 964 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# File info\u001b[0m                                               \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              cache_dir = \u001b[33m'/root/.cache/huggingface/hub'\u001b[0m                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               endpoint = \u001b[94mNone\u001b[0m                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           etag_timeout = \u001b[94m10\u001b[0m                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               filename = \u001b[33m'tokenizer_config.json'\u001b[0m                         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m         force_download = \u001b[94mFalse\u001b[0m                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m         force_filename = \u001b[94mNone\u001b[0m                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                headers = \u001b[94mNone\u001b[0m                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             hf_headers = \u001b[1m{\u001b[0m                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                          \u001b[2m│   \u001b[0m\u001b[33m'user-agent'\u001b[0m: \u001b[33m'unknown/None; hf_hub/0.30.2;\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                          \u001b[33mpython/3.11.12; torch/2.5.0+cu121; \u001b[0m             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                          \u001b[33mtensorflow/2.18.'\u001b[0m+\u001b[94m99\u001b[0m,                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                          \u001b[2m│   \u001b[0m\u001b[33m'authorization'\u001b[0m: \u001b[33m'Bearer \u001b[0m                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                          \u001b[33mhf_UaOxqAkeBUGymOIFYsDAuwvhjZtLznQhTz'\u001b[0m          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                          \u001b[1m}\u001b[0m                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           library_name = \u001b[94mNone\u001b[0m                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        library_version = \u001b[94mNone\u001b[0m                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              local_dir = \u001b[94mNone\u001b[0m                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m local_dir_use_symlinks = \u001b[33m'auto'\u001b[0m                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       local_files_only = \u001b[94mFalse\u001b[0m                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                proxies = \u001b[94mNone\u001b[0m                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                repo_id = \u001b[33m'hf-weights/blt-1b'\u001b[0m                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              repo_type = \u001b[33m'model'\u001b[0m                                         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        resume_download = \u001b[94mNone\u001b[0m                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               revision = \u001b[33m'main'\u001b[0m                                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              subfolder = \u001b[94mNone\u001b[0m                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  token = \u001b[94mNone\u001b[0m                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             user_agent = \u001b[33m'transformers/4.51.3; python/3.11.12; \u001b[0m          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                          \u001b[33msession_id/b7e9f295b70349878560c80c12303ba1'\u001b[0m+\u001b[94m38\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/\u001b[0m\u001b[1;33mfile_download.py\u001b[0m:\u001b[94m106\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[94m8\u001b[0m in \u001b[92m_hf_hub_download_to_cache_dir\u001b[0m                                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1065 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m pointer_path                               \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1066 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1067 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Otherwise, raise appropriate error\u001b[0m                          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1068 \u001b[2m│   │   \u001b[0m\u001b[1;4m_raise_on_head_call_error(head_call_error, force_download, lo\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1069 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1070 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# From now on, etag, commit_hash, url and size are not None.\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1071 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94massert\u001b[0m etag \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33metag must have been retrieved from serv\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m         cache_dir = \u001b[33m'/root/.cache/huggingface/hub'\u001b[0m                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       commit_hash = \u001b[94mNone\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          endpoint = \u001b[94mNone\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              etag = \u001b[94mNone\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m      etag_timeout = \u001b[94m10\u001b[0m                                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     expected_size = \u001b[94mNone\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          filename = \u001b[33m'tokenizer_config.json'\u001b[0m                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m    force_download = \u001b[94mFalse\u001b[0m                                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   head_call_error = \u001b[1;35mRepositoryNotFoundError\u001b[0m\u001b[1m(\u001b[0m\u001b[33m'404 Client Error. \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mRequest \u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[33mID: \u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[33mRoot\u001b[0m\u001b[33m=\u001b[0m\u001b[33m1\u001b[0m\u001b[33m-6809646b-0d865f943c96cf56196aff1f;7d30d2a6-3…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[33mNot Found for url: \u001b[0m                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[33mhttps://huggingface.co/hf-weights/blt-1b/resolve/ma…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[33mmake sure you specified the correct `repo_id` and \u001b[0m   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[33m`repo_type`.\\nIf you are trying to access a private \u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[33mor gated repo, make sure you are authenticated. For \u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[33mmore details, see \u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[33mhttps://huggingface.co/docs/huggingface_hub/authent…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           headers = \u001b[1m{\u001b[0m                                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[2m│   \u001b[0m\u001b[33m'user-agent'\u001b[0m: \u001b[33m'unknown/None; hf_hub/0.30.2; \u001b[0m     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[33mpython/3.11.12; torch/2.5.0+cu121; \u001b[0m                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[33mtensorflow/2.18.'\u001b[0m+\u001b[94m99\u001b[0m,                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[2m│   \u001b[0m\u001b[33m'authorization'\u001b[0m: \u001b[33m'Bearer \u001b[0m                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[33mhf_UaOxqAkeBUGymOIFYsDAuwvhjZtLznQhTz'\u001b[0m               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[1m}\u001b[0m                                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  local_files_only = \u001b[94mFalse\u001b[0m                                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m         locks_dir = \u001b[33m'/root/.cache/huggingface/hub/.locks'\u001b[0m                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           proxies = \u001b[94mNone\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          ref_path = \u001b[33m'/root/.cache/huggingface/hub/models--hf-weights--b…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m relative_filename = \u001b[33m'tokenizer_config.json'\u001b[0m                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           repo_id = \u001b[33m'hf-weights/blt-1b'\u001b[0m                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m         repo_type = \u001b[33m'model'\u001b[0m                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          revision = \u001b[33m'main'\u001b[0m                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m    storage_folder = \u001b[33m'/root/.cache/huggingface/hub/models--hf-weights--b…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             token = \u001b[94mNone\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   url_to_download = \u001b[33m'https://huggingface.co/hf-weights/blt-1b/resolve/m…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     xet_file_data = \u001b[94mNone\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/\u001b[0m\u001b[1;33mfile_download.py\u001b[0m:\u001b[94m159\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[94m6\u001b[0m in \u001b[92m_raise_on_head_call_error\u001b[0m                                               \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1593 \u001b[0m\u001b[2m│   \u001b[0m):                                                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1594 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Repo not found or gated => let's raise the actual error\u001b[0m     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1595 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Unauthorized => likely a token issue => let's raise the act\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1596 \u001b[2m│   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m head_call_error\u001b[0m                                         \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1597 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melse\u001b[0m:                                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1598 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Otherwise: most likely a connection issue or Hub downtime =\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1599 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m LocalEntryNotFoundError(                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   force_download = \u001b[94mFalse\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  head_call_error = \u001b[1;35mRepositoryNotFoundError\u001b[0m\u001b[1m(\u001b[0m\u001b[33m'404 Client Error. \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mRequest \u001b[0m  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[33mID: \u001b[0m                                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[33mRoot\u001b[0m\u001b[33m=\u001b[0m\u001b[33m1\u001b[0m\u001b[33m-6809646b-0d865f943c96cf56196aff1f;7d30d2a6-3a…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[33mNot Found for url: \u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[33mhttps://huggingface.co/hf-weights/blt-1b/resolve/mai…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[33mmake sure you specified the correct `repo_id` and \u001b[0m    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[33m`repo_type`.\\nIf you are trying to access a private \u001b[0m  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[33mor gated repo, make sure you are authenticated. For \u001b[0m  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[33mmore details, see \u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[33mhttps://huggingface.co/docs/huggingface_hub/authenti…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m local_files_only = \u001b[94mFalse\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/\u001b[0m\u001b[1;33mfile_download.py\u001b[0m:\u001b[94m148\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[94m4\u001b[0m in \u001b[92m_get_metadata_or_catch_error\u001b[0m                                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1481 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m local_files_only:                                          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1482 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1483 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1484 \u001b[2m│   │   │   │   \u001b[0mmetadata = get_hf_file_metadata(                      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1485 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0murl=url, proxies=proxies, timeout=etag_timeout, h \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1486 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1487 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mexcept\u001b[0m EntryNotFoundError \u001b[94mas\u001b[0m http_error:                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       commit_hash = \u001b[94mNone\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          endpoint = \u001b[94mNone\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              etag = \u001b[94mNone\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m      etag_timeout = \u001b[94m10\u001b[0m                                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     expected_size = \u001b[94mNone\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          filename = \u001b[33m'tokenizer_config.json'\u001b[0m                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   head_error_call = \u001b[1;35mRepositoryNotFoundError\u001b[0m\u001b[1m(\u001b[0m\u001b[33m'404 Client Error. \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mRequest \u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[33mID: \u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[33mRoot\u001b[0m\u001b[33m=\u001b[0m\u001b[33m1\u001b[0m\u001b[33m-6809646b-0d865f943c96cf56196aff1f;7d30d2a6-3…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[33mNot Found for url: \u001b[0m                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[33mhttps://huggingface.co/hf-weights/blt-1b/resolve/ma…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[33mmake sure you specified the correct `repo_id` and \u001b[0m   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[33m`repo_type`.\\nIf you are trying to access a private \u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[33mor gated repo, make sure you are authenticated. For \u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[33mmore details, see \u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[33mhttps://huggingface.co/docs/huggingface_hub/authent…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           headers = \u001b[1m{\u001b[0m                                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[2m│   \u001b[0m\u001b[33m'user-agent'\u001b[0m: \u001b[33m'unknown/None; hf_hub/0.30.2; \u001b[0m     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[33mpython/3.11.12; torch/2.5.0+cu121; \u001b[0m                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[33mtensorflow/2.18.'\u001b[0m+\u001b[94m99\u001b[0m,                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[2m│   \u001b[0m\u001b[33m'authorization'\u001b[0m: \u001b[33m'Bearer \u001b[0m                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[33mhf_UaOxqAkeBUGymOIFYsDAuwvhjZtLznQhTz'\u001b[0m               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     \u001b[1m}\u001b[0m                                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  local_files_only = \u001b[94mFalse\u001b[0m                                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           proxies = \u001b[94mNone\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m relative_filename = \u001b[33m'tokenizer_config.json'\u001b[0m                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           repo_id = \u001b[33m'hf-weights/blt-1b'\u001b[0m                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m         repo_type = \u001b[33m'model'\u001b[0m                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          revision = \u001b[33m'main'\u001b[0m                                               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m    storage_folder = \u001b[33m'/root/.cache/huggingface/hub/models--hf-weights--b…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             token = \u001b[94mNone\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               url = \u001b[33m'https://huggingface.co/hf-weights/blt-1b/resolve/m…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   url_to_download = \u001b[33m'https://huggingface.co/hf-weights/blt-1b/resolve/m…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m     xet_file_data = \u001b[94mNone\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/\u001b[0m\u001b[1;33m_validators.py\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m :\u001b[94m114\u001b[0m in \u001b[92m_inner_fn\u001b[0m                                                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m check_use_auth_token:                                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   │   │   \u001b[0mkwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[91m__na\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m114 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4mfn(*args, **kwargs)\u001b[0m                                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m115 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m _inner_fn  \u001b[2m# type: ignore\u001b[0m                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             arg_name = \u001b[33m'token'\u001b[0m                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            arg_value = \u001b[94mNone\u001b[0m                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 args = \u001b[1m(\u001b[0m\u001b[1m)\u001b[0m                                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m check_use_auth_token = \u001b[94mTrue\u001b[0m                                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            has_token = \u001b[94mFalse\u001b[0m                                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               kwargs = \u001b[1m{\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'url'\u001b[0m:                                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[33m'https://huggingface.co/hf-weights/blt-1b/resolv…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'proxies'\u001b[0m: \u001b[94mNone\u001b[0m,                              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'timeout'\u001b[0m: \u001b[94m10\u001b[0m,                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'headers'\u001b[0m: \u001b[1m{\u001b[0m                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   │   \u001b[0m\u001b[33m'user-agent'\u001b[0m: \u001b[33m'unknown/None; \u001b[0m             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[33mhf_hub/0.30.2; python/3.11.12; torch/2.5.0+cu121;\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[33mtensorflow/2.18.'\u001b[0m+\u001b[94m99\u001b[0m,                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   │   \u001b[0m\u001b[33m'authorization'\u001b[0m: \u001b[33m'Bearer \u001b[0m                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[33mhf_UaOxqAkeBUGymOIFYsDAuwvhjZtLznQhTz'\u001b[0m            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[1m}\u001b[0m,                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[2m│   \u001b[0m\u001b[33m'token'\u001b[0m: \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[1m}\u001b[0m                                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            signature = \u001b[1m<\u001b[0m\u001b[1;95mSignature\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39murl: str, token: Union\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mbool, str, \u001b[0m    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mNoneType\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, proxies: Optional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mDict\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m,\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mtimeout: Optional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mfloat\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94m10\u001b[0m\u001b[39m, library_name: \u001b[0m     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mOptional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, library_version: \u001b[0m           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mOptional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, user_agent: Union\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mDict, \u001b[0m    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mstr, NoneType\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[39m, headers: \u001b[0m                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mOptional\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mDict\u001b[0m\u001b[1;39m[\u001b[0m\u001b[39mstr, str\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m = \u001b[0m\u001b[94mNone\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m -> \u001b[0m              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        \u001b[39mhuggingface_hub.file_download.HfFileMetadata\u001b[0m\u001b[1m>\u001b[0m     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/\u001b[0m\u001b[1;33mfile_download.py\u001b[0m:\u001b[94m140\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[94m1\u001b[0m in \u001b[92mget_hf_file_metadata\u001b[0m                                                    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1398 \u001b[0m\u001b[2m│   \u001b[0mhf_headers[\u001b[33m\"\u001b[0m\u001b[33mAccept-Encoding\u001b[0m\u001b[33m\"\u001b[0m] = \u001b[33m\"\u001b[0m\u001b[33midentity\u001b[0m\u001b[33m\"\u001b[0m  \u001b[2m# prevent any compres\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1399 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1400 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Retrieve metadata\u001b[0m                                               \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1401 \u001b[2m│   \u001b[0mr = _request_wrapper(                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1402 \u001b[0m\u001b[2m│   │   \u001b[0mmethod=\u001b[33m\"\u001b[0m\u001b[33mHEAD\u001b[0m\u001b[33m\"\u001b[0m,                                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1403 \u001b[0m\u001b[2m│   │   \u001b[0murl=url,                                                      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1404 \u001b[0m\u001b[2m│   │   \u001b[0mheaders=hf_headers,                                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m         headers = \u001b[1m{\u001b[0m                                                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m'user-agent'\u001b[0m: \u001b[33m'unknown/None; hf_hub/0.30.2; \u001b[0m       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[33mpython/3.11.12; torch/2.5.0+cu121; \u001b[0m                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[33mtensorflow/2.18.'\u001b[0m+\u001b[94m99\u001b[0m,                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m'authorization'\u001b[0m: \u001b[33m'Bearer \u001b[0m                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[33mhf_UaOxqAkeBUGymOIFYsDAuwvhjZtLznQhTz'\u001b[0m                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[1m}\u001b[0m                                                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m      hf_headers = \u001b[1m{\u001b[0m                                                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m'user-agent'\u001b[0m: \u001b[33m'unknown/None; hf_hub/0.30.2; \u001b[0m       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[33mpython/3.11.12; torch/2.5.0+cu121; \u001b[0m                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[33mtensorflow/2.18.'\u001b[0m+\u001b[94m99\u001b[0m,                                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m'authorization'\u001b[0m: \u001b[33m'Bearer \u001b[0m                          \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[33mhf_UaOxqAkeBUGymOIFYsDAuwvhjZtLznQhTz'\u001b[0m,                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m'Accept-Encoding'\u001b[0m: \u001b[33m'identity'\u001b[0m                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[1m}\u001b[0m                                                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m    library_name = \u001b[94mNone\u001b[0m                                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m library_version = \u001b[94mNone\u001b[0m                                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m         proxies = \u001b[94mNone\u001b[0m                                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m         timeout = \u001b[94m10\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           token = \u001b[94mNone\u001b[0m                                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             url = \u001b[33m'https://huggingface.co/hf-weights/blt-1b/resolve/mai…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m      user_agent = \u001b[94mNone\u001b[0m                                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/\u001b[0m\u001b[1;33mfile_download.py\u001b[0m:\u001b[94m285\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m in \u001b[92m_request_wrapper\u001b[0m                                                          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 282 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 283 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Recursively follow relative redirects\u001b[0m                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 284 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m follow_relative_redirects:                                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 285 \u001b[2m│   │   \u001b[0mresponse = _request_wrapper(                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 286 \u001b[0m\u001b[2m│   │   │   \u001b[0mmethod=method,                                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 287 \u001b[0m\u001b[2m│   │   │   \u001b[0murl=url,                                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 288 \u001b[0m\u001b[2m│   │   │   \u001b[0mfollow_relative_redirects=\u001b[94mFalse\u001b[0m,                          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m follow_relative_redirects = \u001b[94mTrue\u001b[0m                                         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    method = \u001b[33m'HEAD'\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    params = \u001b[1m{\u001b[0m                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             \u001b[2m│   \u001b[0m\u001b[33m'headers'\u001b[0m: \u001b[1m{\u001b[0m                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             \u001b[2m│   │   \u001b[0m\u001b[33m'user-agent'\u001b[0m: \u001b[33m'unknown/None; \u001b[0m        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             \u001b[33mhf_hub/0.30.2; python/3.11.12; \u001b[0m              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             \u001b[33mtorch/2.5.0+cu121; tensorflow/2.18.'\u001b[0m+\u001b[94m99\u001b[0m,     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             \u001b[2m│   │   \u001b[0m\u001b[33m'authorization'\u001b[0m: \u001b[33m'Bearer \u001b[0m            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             \u001b[33mhf_UaOxqAkeBUGymOIFYsDAuwvhjZtLznQhTz'\u001b[0m,      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             \u001b[2m│   │   \u001b[0m\u001b[33m'Accept-Encoding'\u001b[0m: \u001b[33m'identity'\u001b[0m        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             \u001b[2m│   \u001b[0m\u001b[1m}\u001b[0m,                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             \u001b[2m│   \u001b[0m\u001b[33m'allow_redirects'\u001b[0m: \u001b[94mFalse\u001b[0m,                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             \u001b[2m│   \u001b[0m\u001b[33m'proxies'\u001b[0m: \u001b[94mNone\u001b[0m,                         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             \u001b[2m│   \u001b[0m\u001b[33m'timeout'\u001b[0m: \u001b[94m10\u001b[0m                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             \u001b[1m}\u001b[0m                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                       url = \u001b[33m'https://huggingface.co/hf-weights/blt-1b/r…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/\u001b[0m\u001b[1;33mfile_download.py\u001b[0m:\u001b[94m309\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m in \u001b[92m_request_wrapper\u001b[0m                                                          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 306 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 307 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Perform request and return if status_code is not in the retry l\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 308 \u001b[0m\u001b[2m│   \u001b[0mresponse = get_session().request(method=method, url=url, **params \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 309 \u001b[2m│   \u001b[0m\u001b[1;4mhf_raise_for_status(response)\u001b[0m                                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 310 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m response                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 311 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 312 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m follow_relative_redirects = \u001b[94mFalse\u001b[0m                                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    method = \u001b[33m'HEAD'\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    params = \u001b[1m{\u001b[0m                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             \u001b[2m│   \u001b[0m\u001b[33m'headers'\u001b[0m: \u001b[1m{\u001b[0m                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             \u001b[2m│   │   \u001b[0m\u001b[33m'user-agent'\u001b[0m: \u001b[33m'unknown/None; \u001b[0m        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             \u001b[33mhf_hub/0.30.2; python/3.11.12; \u001b[0m              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             \u001b[33mtorch/2.5.0+cu121; tensorflow/2.18.'\u001b[0m+\u001b[94m99\u001b[0m,     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             \u001b[2m│   │   \u001b[0m\u001b[33m'authorization'\u001b[0m: \u001b[33m'Bearer \u001b[0m            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             \u001b[33mhf_UaOxqAkeBUGymOIFYsDAuwvhjZtLznQhTz'\u001b[0m,      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             \u001b[2m│   │   \u001b[0m\u001b[33m'Accept-Encoding'\u001b[0m: \u001b[33m'identity'\u001b[0m        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             \u001b[2m│   \u001b[0m\u001b[1m}\u001b[0m,                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             \u001b[2m│   \u001b[0m\u001b[33m'allow_redirects'\u001b[0m: \u001b[94mFalse\u001b[0m,                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             \u001b[2m│   \u001b[0m\u001b[33m'proxies'\u001b[0m: \u001b[94mNone\u001b[0m,                         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             \u001b[2m│   \u001b[0m\u001b[33m'timeout'\u001b[0m: \u001b[94m10\u001b[0m                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             \u001b[1m}\u001b[0m                                            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  response = \u001b[1m<\u001b[0m\u001b[1;95mResponse\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m[\u001b[0m\u001b[94m404\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1m>\u001b[0m                             \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                       url = \u001b[33m'https://huggingface.co/hf-weights/blt-1b/r…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/\u001b[0m\u001b[1;33m_http.py\u001b[0m:\u001b[94m459\u001b[0m   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m in \u001b[92mhf_raise_for_status\u001b[0m                                                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m456 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m make sure you are authenticated. For more details, s\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m457 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m https://huggingface.co/docs/huggingface_hub/authenti\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m458 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m459 \u001b[2m│   │   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m _format(RepositoryNotFoundError, message, response) \u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m460 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m461 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m response.status_code == \u001b[94m400\u001b[0m:                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m462 \u001b[0m\u001b[2m│   │   │   \u001b[0mmessage = (                                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m endpoint_name = \u001b[94mNone\u001b[0m                                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m    error_code = \u001b[33m'RepoNotFound'\u001b[0m                                           \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m error_message = \u001b[33m'Repository not found'\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       message = \u001b[33m'404 Client Error.\\n\\nRepository Not Found for url: \u001b[0m     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[33mhttps://huggingface.co/hf-weigh'\u001b[0m+\u001b[94m282\u001b[0m                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m      response = \u001b[1m<\u001b[0m\u001b[1;95mResponse\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m[\u001b[0m\u001b[94m404\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1m>\u001b[0m                                         \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "[rank0]: \u001b[1;91mRepositoryNotFoundError: \u001b[0m\u001b[1;36m404\u001b[0m Client Error. \u001b[1m(\u001b[0mRequest ID: \n",
            "[rank0]: \u001b[33mRoot\u001b[0m=\u001b[1;36m1\u001b[0m-6809646b-0d865f943c96cf56196aff1f;\u001b[93m7d30d2a6-3a2b-4182-8948-2eb5f53c9759\u001b[0m\u001b[1m)\u001b[0m\n",
            "\n",
            "[rank0]: Repository Not Found for url: \n",
            "[rank0]: \u001b[4;94mhttps://huggingface.co/hf-weights/blt-1b/resolve/main/tokenizer_config.json.\u001b[0m\n",
            "[rank0]: Please make sure you specified the correct `repo_id` and `repo_type`.\n",
            "[rank0]: If you are trying to access a private or gated repo, make sure you are \n",
            "[rank0]: authenticated. For more details, see \n",
            "[rank0]: \u001b[4;94mhttps://huggingface.co/docs/huggingface_hub/authentication\u001b[0m\n",
            "\n",
            "[rank0]: \u001b[3mThe above exception was the direct cause of the following exception:\u001b[0m\n",
            "\n",
            "[rank0]: \u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/content/blt/\u001b[0m\u001b[1;33mdemo.py\u001b[0m:\u001b[94m20\u001b[0m in \u001b[92mmain\u001b[0m                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   │   \u001b[0msetup_torch_distributed(distributed_args)                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m18 \u001b[0m\u001b[2m│   \u001b[0mcheckpoint_path = os.path.join(\u001b[33m\"\u001b[0m\u001b[33mhf-weights\u001b[0m\u001b[33m\"\u001b[0m, model_name)            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mLoading BLT model: \u001b[0m\u001b[33m{\u001b[0mmodel_name\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m20 \u001b[2m│   \u001b[0mmodel, tokenizer, train_cfg = \u001b[1;4mload_consolidated_model_and_tokenizer\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mcheckpoint_path,\u001b[0m                                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[1;2;4m│   \u001b[0m\u001b[1;4m)\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m23 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94massert\u001b[0m \u001b[96misinstance\u001b[0m(model, ByteLatentTransformer)                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m─────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m──────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  checkpoint_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m                              \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m distributed_args = \u001b[1;35mDistributedArgs\u001b[0m\u001b[1m(\u001b[0m                                 \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdp_shard\u001b[0m=\u001b[94m1\u001b[0m,                                  \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdp_replicate\u001b[0m=\u001b[94m1\u001b[0m,                              \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mtp_size\u001b[0m=\u001b[94m1\u001b[0m,                                   \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mselective_activation_checkpointing\u001b[0m=\u001b[94mFalse\u001b[0m,    \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mcompile\u001b[0m=\u001b[94mFalse\u001b[0m,                               \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfsdp_type\u001b[0m=\u001b[33m'no_shard'\u001b[0m,                        \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mmodel_dtype\u001b[0m=\u001b[33m'bf16'\u001b[0m,                          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfloat8_recipe\u001b[0m=\u001b[94mNone\u001b[0m,                          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfloat8_filter\u001b[0m=\u001b[33m'layers\\\\.\u001b[0m\u001b[1;33m[\u001b[0m\u001b[33m0-9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[33m+\\\\.'\u001b[0m,          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mmatmul_allow_tf32\u001b[0m=\u001b[94mFalse\u001b[0m,                     \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mallow_bf16_reduced_precision_reduction\u001b[0m=\u001b[94mTrue\u001b[0m, \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdetect_anomaly\u001b[0m=\u001b[94mFalse\u001b[0m,                        \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mcompile_cache_size_limit\u001b[0m=\u001b[94m8\u001b[0m,                  \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mspawn_method\u001b[0m=\u001b[33m'forkserver'\u001b[0m                    \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[1m)\u001b[0m                                                \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       model_name = \u001b[33m'blt-1b'\u001b[0m                                         \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           prompt = \u001b[33m'hi'\u001b[0m                                             \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰─────────────────────────────────────────────────────────────────────╯\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/content/blt/bytelatent/\u001b[0m\u001b[1;33mgenerate.py\u001b[0m:\u001b[94m18\u001b[0m in                                    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[92mload_consolidated_model_and_tokenizer\u001b[0m                                        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33mتحميل النموذج والمعالج من Hugging Face بطريقة مرنة تدعم التوزيع وال\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m\\U0001F4E6\u001b[0m\u001b[33m تحميل المعالج من: \u001b[0m\u001b[33m{\u001b[0mcheckpoint_path\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m18 \u001b[2m│   \u001b[0mtokenizer = \u001b[1;4mAutoTokenizer.from_pretrained(\u001b[0m                          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mcheckpoint_path,\u001b[0m                                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mtrust_remote_code=trust_remote_code,\u001b[0m                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[1;2;4m│   \u001b[0m\u001b[1;4m)\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m───────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────\u001b[0m\u001b[33m─╮\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   checkpoint_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        device_map = \u001b[94mNone\u001b[0m                \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  init_distributed = \u001b[94mFalse\u001b[0m               \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                  \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m low_cpu_mem_usage = \u001b[94mFalse\u001b[0m               \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       torch_dtype = \u001b[94mNone\u001b[0m                \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m trust_remote_code = \u001b[94mTrue\u001b[0m                \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   use_safetensors = \u001b[94mTrue\u001b[0m                \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰─────────────────────────────────────────╯\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/\u001b[0m\u001b[1;33mtokenizatio\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[1;33mn_auto.py\u001b[0m:\u001b[94m946\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 943 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m tokenizer_class.from_pretrained(pretrained_model_n \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 944 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 945 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Next, let's try to use the tokenizer_config file to get the\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 946 \u001b[2m│   │   \u001b[0mtokenizer_config = \u001b[1;4mget_tokenizer_config(pretrained_model_name\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 947 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[33m\"\u001b[0m\u001b[33m_commit_hash\u001b[0m\u001b[33m\"\u001b[0m \u001b[95min\u001b[0m tokenizer_config:                        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 948 \u001b[0m\u001b[2m│   │   │   \u001b[0mkwargs[\u001b[33m\"\u001b[0m\u001b[33m_commit_hash\u001b[0m\u001b[33m\"\u001b[0m] = tokenizer_config[\u001b[33m\"\u001b[0m\u001b[33m_commit_hash\u001b[0m\u001b[33m\"\u001b[0m] \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 949 \u001b[0m\u001b[2m│   │   \u001b[0mconfig_tokenizer_class = tokenizer_config.get(\u001b[33m\"\u001b[0m\u001b[33mtokenizer_clas\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m──────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m──────────────────────\u001b[0m\u001b[33m─╮\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        config = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     gguf_file = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        inputs = \u001b[1m(\u001b[0m\u001b[1m)\u001b[0m                   \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        kwargs = \u001b[1m{\u001b[0m\u001b[33m'_from_auto'\u001b[0m: \u001b[94mTrue\u001b[0m\u001b[1m}\u001b[0m \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m pretrained_model_name_or_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m  \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                tokenizer_type = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             trust_remote_code = \u001b[94mTrue\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                use_auth_token = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      use_fast = \u001b[94mTrue\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────╯\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/\u001b[0m\u001b[1;33mtokenizatio\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[1;33mn_auto.py\u001b[0m:\u001b[94m778\u001b[0m in \u001b[92mget_tokenizer_config\u001b[0m                                        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 775 \u001b[0m\u001b[2m│   │   \u001b[0mtoken = use_auth_token                                        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 776 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 777 \u001b[0m\u001b[2m│   \u001b[0mcommit_hash = kwargs.get(\u001b[33m\"\u001b[0m\u001b[33m_commit_hash\u001b[0m\u001b[33m\"\u001b[0m, \u001b[94mNone\u001b[0m)                    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 778 \u001b[2m│   \u001b[0mresolved_config_file = cached_file(                               \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 779 \u001b[0m\u001b[2m│   │   \u001b[0mpretrained_model_name_or_path,                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 780 \u001b[0m\u001b[2m│   │   \u001b[0mTOKENIZER_CONFIG_FILE,                                        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 781 \u001b[0m\u001b[2m│   │   \u001b[0mcache_dir=cache_dir,                                          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m──────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m──────────────────────\u001b[0m\u001b[33m─╮\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     cache_dir = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   commit_hash = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                force_download = \u001b[94mFalse\u001b[0m                \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        kwargs = \u001b[1m{\u001b[0m\u001b[33m'_from_auto'\u001b[0m: \u001b[94mTrue\u001b[0m\u001b[1m}\u001b[0m \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              local_files_only = \u001b[94mFalse\u001b[0m                \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m pretrained_model_name_or_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m  \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                       proxies = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               resume_download = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      revision = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     subfolder = \u001b[33m''\u001b[0m                   \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         token = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                use_auth_token = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────╯\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/utils/\u001b[0m\u001b[1;33mhub.py\u001b[0m:\u001b[94m266\u001b[0m in     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[92mcached_file\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 263 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33mmodel_weights_file = cached_file(\"google-bert/bert-base-uncased\",\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 264 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m```\u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 265 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 266 \u001b[2m│   \u001b[0mfile = \u001b[1;4mcached_files(path_or_repo_id=path_or_repo_id, filenames=[f\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 267 \u001b[0m\u001b[2m│   \u001b[0mfile = file[\u001b[94m0\u001b[0m] \u001b[94mif\u001b[0m file \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m file                      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 268 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m file                                                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 269 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m──────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m───────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        filename = \u001b[33m'tokenizer_config.json'\u001b[0m                             \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          kwargs = \u001b[1m{\u001b[0m                                                   \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m'cache_dir'\u001b[0m: \u001b[94mNone\u001b[0m,                              \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m'force_download'\u001b[0m: \u001b[94mFalse\u001b[0m,                        \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m'resume_download'\u001b[0m: \u001b[94mNone\u001b[0m,                        \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m'proxies'\u001b[0m: \u001b[94mNone\u001b[0m,                                \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m'token'\u001b[0m: \u001b[94mNone\u001b[0m,                                  \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m'revision'\u001b[0m: \u001b[94mNone\u001b[0m,                               \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m'local_files_only'\u001b[0m: \u001b[94mFalse\u001b[0m,                      \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m'subfolder'\u001b[0m: \u001b[33m''\u001b[0m,                                \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m'_raise_exceptions_for_gated_repo'\u001b[0m: \u001b[94mFalse\u001b[0m,      \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m'_raise_exceptions_for_missing_entries'\u001b[0m: \u001b[94mFalse\u001b[0m, \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[2m│   \u001b[0m\u001b[33m...\u001b[0m +\u001b[94m2\u001b[0m                                          \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   \u001b[1m}\u001b[0m                                                   \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m path_or_repo_id = \u001b[33m'hf-weights/blt-1b'\u001b[0m                                 \u001b[33m│\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰───────────────────────────────────────────────────────────────────────╯\u001b[0m    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/utils/\u001b[0m\u001b[1;33mhub.py\u001b[0m:\u001b[94m456\u001b[0m in     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[92mcached_files\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 453 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mException\u001b[0m \u001b[94mas\u001b[0m e:                                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 454 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# We cannot recover from them\u001b[0m                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 455 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(e, RepositoryNotFoundError) \u001b[95mand\u001b[0m \u001b[95mnot\u001b[0m \u001b[96misinstance\u001b[0m( \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 456 \u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mOSError\u001b[0m(                                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 457 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0mpath_or_repo_id\u001b[33m}\u001b[0m\u001b[33m is not a local folder and is not \u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 458 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mlisted on \u001b[0m\u001b[33m'\u001b[0m\u001b[33mhttps://huggingface.co/models\u001b[0m\u001b[33m'\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33mIf this i\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 459 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mhaving permission to this repo either by logging in \u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      _commit_hash = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m _raise_exceptions_for_connection_e… \u001b[94mFalse\u001b[0m                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                   =                                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  _raise_exceptions_for_gated_repo = \u001b[94mFalse\u001b[0m                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m _raise_exceptions_for_missing_entr… \u001b[94mFalse\u001b[0m                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                   =                                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         cache_dir = \u001b[33m'/root/.cache/huggingface/hub'\u001b[0m       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 deprecated_kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    existing_files = \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      file_counter = \u001b[94m0\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                          filename = \u001b[33m'tokenizer_config.json'\u001b[0m              \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         filenames = \u001b[1m[\u001b[0m\u001b[33m'tokenizer_config.json'\u001b[0m\u001b[1m]\u001b[0m            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    force_download = \u001b[94mFalse\u001b[0m                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    full_filenames = \u001b[1m[\u001b[0m\u001b[33m'tokenizer_config.json'\u001b[0m\u001b[1m]\u001b[0m            \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                  local_files_only = \u001b[94mFalse\u001b[0m                                \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   path_or_repo_id = \u001b[33m'hf-weights/blt-1b'\u001b[0m                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                           proxies = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         repo_type = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   resume_download = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                          revision = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                         subfolder = \u001b[33m''\u001b[0m                                   \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                             token = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    use_auth_token = \u001b[94mNone\u001b[0m                                 \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        user_agent = \u001b[33m'transformers/4.51.3; \u001b[0m               \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                     \u001b[33mpython/3.11.12; \u001b[0m                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                     \u001b[33msession_id/b7e9f295b70349878560c80c…\u001b[0m \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "[rank0]: \u001b[1;91mOSError: \u001b[0mhf-weights/blt-1b is not a local folder and is not a valid model \n",
            "[rank0]: identifier listed on \u001b[32m'https://huggingface.co/models'\u001b[0m\n",
            "[rank0]: If this is a private repository, make sure to pass a token having permission to \n",
            "[rank0]: this repo either by logging in with `huggingface-cli login` or by passing \n",
            "[rank0]: `\u001b[33mtoken\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95myour_token\u001b[0m\u001b[1m>\u001b[0m`\n",
            "[rank0]:[W423 22:06:37.255614424 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%cd /content/blt\n",
        "\n",
        "\n",
        "\n",
        "!python /content/blt/demo.py -m facebook/blt-1b \"hi\""
      ],
      "metadata": {
        "id": "EQ5M7g2yrgZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/blt/demo.py --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QrscCphsbT4",
        "outputId": "455b6bc7-46ef-4171-eac1-195698dfe980"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m                                                                                \u001b[0m\n",
            "\u001b[1m \u001b[0m\u001b[1;33mUsage: \u001b[0m\u001b[1mdemo.py [OPTIONS] PROMPT\u001b[0m\u001b[1m                                               \u001b[0m\u001b[1m \u001b[0m\n",
            "\u001b[1m                                                                                \u001b[0m\n",
            "\u001b[2m╭─\u001b[0m\u001b[2m Arguments \u001b[0m\u001b[2m─────────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[31m*\u001b[0m    prompt      \u001b[1;33mTEXT\u001b[0m  \u001b[2m[default: None]\u001b[0m \u001b[2;31m[required]\u001b[0m                            \u001b[2m│\u001b[0m\n",
            "\u001b[2m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[2m╭─\u001b[0m\u001b[2m Options \u001b[0m\u001b[2m───────────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-model\u001b[0m\u001b[1;36m-name\u001b[0m        \u001b[1;33mTEXT\u001b[0m  \u001b[2m[default: blt-1b]\u001b[0m                                  \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-help\u001b[0m              \u001b[1;33m    \u001b[0m  Show this message and exit.                        \u001b[2m│\u001b[0m\n",
            "\u001b[2m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/blt/demo.py  --model-name blt-1b \"hi\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2s_ZR1Ls8qp",
        "outputId": "dac644b0-44cc-4acb-bcdb-1d275a9c4805"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BLT model: blt-1b\n",
            "📦 تحميل المعالج من: hf-weights/blt-1b\n",
            "[rank0]: \u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/content/blt/\u001b[0m\u001b[1;33mdemo.py\u001b[0m:\u001b[94m20\u001b[0m in \u001b[92mmain\u001b[0m                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   │   \u001b[0msetup_torch_distributed(distributed_args)                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m18 \u001b[0m\u001b[2m│   \u001b[0mcheckpoint_path = os.path.join(\u001b[33m\"\u001b[0m\u001b[33mhf-weights\u001b[0m\u001b[33m\"\u001b[0m, model_name)            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mLoading BLT model: \u001b[0m\u001b[33m{\u001b[0mmodel_name\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m20 \u001b[2m│   \u001b[0mmodel, tokenizer, train_cfg = \u001b[1;4mload_consolidated_model_and_tokenizer\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mcheckpoint_path,\u001b[0m                                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[1;2;4m│   \u001b[0m\u001b[1;4m)\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m23 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94massert\u001b[0m \u001b[96misinstance\u001b[0m(model, ByteLatentTransformer)                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m─────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m──────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  checkpoint_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m                              \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m distributed_args = \u001b[1;35mDistributedArgs\u001b[0m\u001b[1m(\u001b[0m                                 \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdp_shard\u001b[0m=\u001b[94m1\u001b[0m,                                  \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdp_replicate\u001b[0m=\u001b[94m1\u001b[0m,                              \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mtp_size\u001b[0m=\u001b[94m1\u001b[0m,                                   \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mselective_activation_checkpointing\u001b[0m=\u001b[94mFalse\u001b[0m,    \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mcompile\u001b[0m=\u001b[94mFalse\u001b[0m,                               \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfsdp_type\u001b[0m=\u001b[33m'no_shard'\u001b[0m,                        \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mmodel_dtype\u001b[0m=\u001b[33m'bf16'\u001b[0m,                          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfloat8_recipe\u001b[0m=\u001b[94mNone\u001b[0m,                          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfloat8_filter\u001b[0m=\u001b[33m'layers\\\\.\u001b[0m\u001b[1;33m[\u001b[0m\u001b[33m0-9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[33m+\\\\.'\u001b[0m,          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mmatmul_allow_tf32\u001b[0m=\u001b[94mFalse\u001b[0m,                     \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mallow_bf16_reduced_precision_reduction\u001b[0m=\u001b[94mTrue\u001b[0m, \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdetect_anomaly\u001b[0m=\u001b[94mFalse\u001b[0m,                        \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mcompile_cache_size_limit\u001b[0m=\u001b[94m8\u001b[0m,                  \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mspawn_method\u001b[0m=\u001b[33m'forkserver'\u001b[0m                    \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[1m)\u001b[0m                                                \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       model_name = \u001b[33m'blt-1b'\u001b[0m                                         \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           prompt = \u001b[33m'hi'\u001b[0m                                             \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰─────────────────────────────────────────────────────────────────────╯\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/content/blt/bytelatent/\u001b[0m\u001b[1;33mgenerate.py\u001b[0m:\u001b[94m18\u001b[0m in                                    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[92mload_consolidated_model_and_tokenizer\u001b[0m                                        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33mتحميل النموذج والمعالج من Hugging Face بطريقة مرنة تدعم التوزيع وال\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m\\U0001F4E6\u001b[0m\u001b[33m تحميل المعالج من: \u001b[0m\u001b[33m{\u001b[0mcheckpoint_path\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m18 \u001b[2m│   \u001b[0mtokenizer = \u001b[1;4mAutoTokenizer.from_pretrained(\u001b[0m                          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mcheckpoint_path,\u001b[0m                                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mtrust_remote_code=trust_remote_code,\u001b[0m                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[1;2;4m│   \u001b[0m\u001b[1;4m)\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m───────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────\u001b[0m\u001b[33m─╮\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   checkpoint_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        device_map = \u001b[94mNone\u001b[0m                \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  init_distributed = \u001b[94mFalse\u001b[0m               \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                  \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m low_cpu_mem_usage = \u001b[94mFalse\u001b[0m               \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       torch_dtype = \u001b[94mNone\u001b[0m                \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m trust_remote_code = \u001b[94mTrue\u001b[0m                \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   use_safetensors = \u001b[94mTrue\u001b[0m                \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰─────────────────────────────────────────╯\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/\u001b[0m\u001b[1;33mtokenizatio\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[1;33mn_auto.py\u001b[0m:\u001b[94m966\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 963 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mconfig_dict = load_gguf_checkpoint(gguf_path, ret \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 964 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mconfig = AutoConfig.for_model(**config_dict)      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 965 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 966 \u001b[2m│   │   │   │   │   \u001b[0mconfig = \u001b[1;4mAutoConfig.from_pretrained(\u001b[0m              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 967 \u001b[0m\u001b[1;2;4m│   │   │   │   │   │   \u001b[0m\u001b[1;4mpretrained_model_name_or_path, trust_remote_c\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 968 \u001b[0m\u001b[1;2;4m│   │   │   │   │   \u001b[0m\u001b[1;4m)\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 969 \u001b[0m\u001b[2m│   │   │   \u001b[0mconfig_tokenizer_class = config.tokenizer_class           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m──────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m──────────────────────\u001b[0m\u001b[33m─╮\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        config = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        config_tokenizer_class = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     gguf_file = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        inputs = \u001b[1m(\u001b[0m\u001b[1m)\u001b[0m                   \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        kwargs = \u001b[1m{\u001b[0m\u001b[33m'_from_auto'\u001b[0m: \u001b[94mTrue\u001b[0m\u001b[1m}\u001b[0m \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m pretrained_model_name_or_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m  \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            tokenizer_auto_map = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              tokenizer_config = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                   \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                tokenizer_type = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             trust_remote_code = \u001b[94mTrue\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                use_auth_token = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      use_fast = \u001b[94mTrue\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────╯\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/\u001b[0m\u001b[1;33mconfigurati\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[1;33mon_auto.py\u001b[0m:\u001b[94m1151\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m                                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1148 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m pattern \u001b[95min\u001b[0m \u001b[96mstr\u001b[0m(pretrained_model_name_or_path):     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1149 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m CONFIG_MAPPING[pattern].from_dict(config_d \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1150 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1151 \u001b[2m│   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;96mValueError\u001b[0m\u001b[1;4m(\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1152 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;33mf\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mUnrecognized model in \u001b[0m\u001b[1;4;33m{\u001b[0m\u001b[1;4mpretrained_model_name_or_path\u001b[0m\u001b[1;4;33m}\u001b[0m\u001b[1;4;33m. \u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1153 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;33mf\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mShould have a `model_type` key in its \u001b[0m\u001b[1;4;33m{\u001b[0m\u001b[1;4mCONFIG_NAME\u001b[0m\u001b[1;4;33m}\u001b[0m\u001b[1;4;33m, or\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1154 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;33mf\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33min its name: \u001b[0m\u001b[1;4;33m{\u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4;33m, \u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4m.join(CONFIG_MAPPING.keys())\u001b[0m\u001b[1;4;33m}\u001b[0m\u001b[1;4;33m\"\u001b[0m        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 code_revision = \u001b[94mNone\u001b[0m                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   config_dict = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                has_local_code = \u001b[94mFalse\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               has_remote_code = \u001b[94mFalse\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        kwargs = \u001b[1m{\u001b[0m                                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                 \u001b[2m│   \u001b[0m\u001b[33m'_from_auto'\u001b[0m: \u001b[94mTrue\u001b[0m,                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                 \u001b[2m│   \u001b[0m\u001b[33m'name_or_path'\u001b[0m: \u001b[33m'hf-weights/blt-1b'\u001b[0m  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                 \u001b[1m}\u001b[0m                                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                       pattern = \u001b[33m't5'\u001b[0m                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m pretrained_model_name_or_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             trust_remote_code = \u001b[94mTrue\u001b[0m                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 unused_kwargs = \u001b[1m{\u001b[0m\u001b[33m'name_or_path'\u001b[0m: \u001b[33m'hf-weights/blt-1b'\u001b[0m\u001b[1m}\u001b[0m    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                use_auth_token = \u001b[94mNone\u001b[0m                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "[rank0]: \u001b[1;91mValueError: \u001b[0mUnrecognized model in hf-weights/blt-1b. Should have a `model_type` \n",
            "[rank0]: key in its config.json, or contain one of the following strings in its name: \n",
            "[rank0]: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, \n",
            "[rank0]: autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, \n",
            "[rank0]: big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, \n",
            "[rank0]: blip-\u001b[1;36m2\u001b[0m, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, \n",
            "[rank0]: chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, \n",
            "[rank0]: clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, \n",
            "[rank0]: convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, \n",
            "[rank0]: data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, \n",
            "[rank0]: decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, \n",
            "[rank0]: depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, \n",
            "[rank0]: distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, \n",
            "[rank0]: encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, \n",
            "[rank0]: fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, \n",
            "[rank0]: gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, \n",
            "[rank0]: gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, \n",
            "[rank0]: granite, granitemoe, granitemoeshared, granitevision, graphormer, \n",
            "[rank0]: grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, \n",
            "[rank0]: idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, \n",
            "[rank0]: instructblipvideo, jamba, jetmoe, jukebox, kosmos-\u001b[1;36m2\u001b[0m, layoutlm, layoutlmv2, \n",
            "[rank0]: layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, \n",
            "[rank0]: llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, \n",
            "[rank0]: mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, \n",
            "[rank0]: mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, \n",
            "[rank0]: mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, \n",
            "[rank0]: modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, \n",
            "[rank0]: mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, \n",
            "[rank0]: omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, \n",
            "[rank0]: patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, \n",
            "[rank0]: phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, \n",
            "[rank0]: prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, \n",
            "[rank0]: qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, \n",
            "[rank0]: realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, \n",
            "[rank0]: roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, \n",
            "[rank0]: rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, \n",
            "[rank0]: sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, \n",
            "[rank0]: smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, \n",
            "[rank0]: speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, \n",
            "[rank0]: swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, \n",
            "[rank0]: tapas, textnet, time_series_transformer, timesformer, timm_backbone, \n",
            "[rank0]: timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, \n",
            "[rank0]: unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, \n",
            "[rank0]: vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, \n",
            "[rank0]: vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits,\n",
            "[rank0]: vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm,\n",
            "[rank0]: xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, \n",
            "[rank0]: zamba, zamba2, zoedepth\n",
            "[rank0]:[W423 22:15:10.682414664 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgUIkWyVtQj4",
        "outputId": "796196f1-eaf2-4e71-b949-b85f3cb3c59b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: read).\n",
            "The token `read` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `read`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PretrainedConfig\n",
        "\n",
        "class CustomSeq2SeqConfig(PretrainedConfig):\n",
        "    model_type = \"custom_seq2seq\"\n",
        "    # Add other necessary configuration parameters here\n"
      ],
      "metadata": {
        "id": "g8vC8ph7wdmz"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"/content/blt/hf-weights/blt-1b\", config=CustomSeq2SeqConfig.from_pretrained(\"/content/blt/hf-weights/blt-1b\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "yb7T00TotdgY",
        "outputId": "9733e4b4-e456-4b78-bebf-f689080461e7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unrecognized configuration class <class '__main__.CustomSeq2SeqConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, Qwen2AudioConfig, SeamlessM4TConfig, SeamlessM4Tv2Config, SwitchTransformersConfig, T5Config, UMT5Config, XLMProphetNetConfig.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-cdd225317f74>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/blt/hf-weights/blt-1b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCustomSeq2SeqConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/blt/hf-weights/blt-1b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             )\n\u001b[0;32m--> 574\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    575\u001b[0m             \u001b[0;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0;34mf\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class '__main__.CustomSeq2SeqConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, Qwen2AudioConfig, SeamlessM4TConfig, SeamlessM4Tv2Config, SwitchTransformersConfig, T5Config, UMT5Config, XLMProphetNetConfig."
          ]
        }
      ]
    },
    {
      "source": [
        "from transformers import AutoConfig, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Load the configuration from the pretrained model, not a custom one\n",
        "config = AutoConfig.from_pretrained(\"/content/blt/hf-weights/blt-1b\")\n",
        "\n",
        "# Update the model type to 'blt' if it's not automatically set\n",
        "if config.model_type != \"blt\":  # Adjust if needed based on the actual model type\n",
        "    config.model_type = \"blt\"\n",
        "\n",
        "# Now load the model using the correct configuration\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    \"/content/blt/hf-weights/blt-1b\", config=config\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "vOvziUaiwzH8",
        "outputId": "0598f15c-0154-4e54-fe31-97c2f57eb510"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unrecognized model in /content/blt/hf-weights/blt-1b. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-bf256968b3b1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the configuration from the pretrained model, not a custom one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/blt/hf-weights/blt-1b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Update the model type to 'blt' if it's not automatically set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1149\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1152\u001b[0m             \u001b[0;34mf\"Unrecognized model in {pretrained_model_name_or_path}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0;34mf\"Should have a `model_type` key in its {CONFIG_NAME}, or contain one of the following strings \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unrecognized model in /content/blt/hf-weights/blt-1b. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer,..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python demo.py \"A BLT has\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_YxKejgwYS6",
        "outputId": "b593e277-1c78-40e4-b198-e573afb14e3b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BLT model: blt-1b\n",
            "📦 تحميل المعالج من: hf-weights/blt-1b\n",
            "[rank0]: \u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/content/blt/\u001b[0m\u001b[1;33mdemo.py\u001b[0m:\u001b[94m20\u001b[0m in \u001b[92mmain\u001b[0m                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   │   \u001b[0msetup_torch_distributed(distributed_args)                       \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m18 \u001b[0m\u001b[2m│   \u001b[0mcheckpoint_path = os.path.join(\u001b[33m\"\u001b[0m\u001b[33mhf-weights\u001b[0m\u001b[33m\"\u001b[0m, model_name)            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mLoading BLT model: \u001b[0m\u001b[33m{\u001b[0mmodel_name\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m20 \u001b[2m│   \u001b[0mmodel, tokenizer, train_cfg = \u001b[1;4mload_consolidated_model_and_tokenizer\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mcheckpoint_path,\u001b[0m                                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[1;2;4m│   \u001b[0m\u001b[1;4m)\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m23 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94massert\u001b[0m \u001b[96misinstance\u001b[0m(model, ByteLatentTransformer)                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m─────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m──────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  checkpoint_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m                              \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m distributed_args = \u001b[1;35mDistributedArgs\u001b[0m\u001b[1m(\u001b[0m                                 \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdp_shard\u001b[0m=\u001b[94m1\u001b[0m,                                  \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdp_replicate\u001b[0m=\u001b[94m1\u001b[0m,                              \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mtp_size\u001b[0m=\u001b[94m1\u001b[0m,                                   \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mselective_activation_checkpointing\u001b[0m=\u001b[94mFalse\u001b[0m,    \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mcompile\u001b[0m=\u001b[94mFalse\u001b[0m,                               \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfsdp_type\u001b[0m=\u001b[33m'no_shard'\u001b[0m,                        \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mmodel_dtype\u001b[0m=\u001b[33m'bf16'\u001b[0m,                          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfloat8_recipe\u001b[0m=\u001b[94mNone\u001b[0m,                          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mfloat8_filter\u001b[0m=\u001b[33m'layers\\\\.\u001b[0m\u001b[1;33m[\u001b[0m\u001b[33m0-9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[33m+\\\\.'\u001b[0m,          \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mmatmul_allow_tf32\u001b[0m=\u001b[94mFalse\u001b[0m,                     \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mallow_bf16_reduced_precision_reduction\u001b[0m=\u001b[94mTrue\u001b[0m, \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mdetect_anomaly\u001b[0m=\u001b[94mFalse\u001b[0m,                        \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mcompile_cache_size_limit\u001b[0m=\u001b[94m8\u001b[0m,                  \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[2m│   \u001b[0m\u001b[33mspawn_method\u001b[0m=\u001b[33m'forkserver'\u001b[0m                    \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                    \u001b[1m)\u001b[0m                                                \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       model_name = \u001b[33m'blt-1b'\u001b[0m                                         \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           prompt = \u001b[33m'A BLT has'\u001b[0m                                      \u001b[33m│\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰─────────────────────────────────────────────────────────────────────╯\u001b[0m      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/content/blt/bytelatent/\u001b[0m\u001b[1;33mgenerate.py\u001b[0m:\u001b[94m18\u001b[0m in                                    \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[92mload_consolidated_model_and_tokenizer\u001b[0m                                        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33mتحميل النموذج والمعالج من Hugging Face بطريقة مرنة تدعم التوزيع وال\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m\\U0001F4E6\u001b[0m\u001b[33m تحميل المعالج من: \u001b[0m\u001b[33m{\u001b[0mcheckpoint_path\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m18 \u001b[2m│   \u001b[0mtokenizer = \u001b[1;4mAutoTokenizer.from_pretrained(\u001b[0m                          \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mcheckpoint_path,\u001b[0m                                                \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mtrust_remote_code=trust_remote_code,\u001b[0m                            \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[1;2;4m│   \u001b[0m\u001b[1;4m)\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m───────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────\u001b[0m\u001b[33m─╮\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   checkpoint_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        device_map = \u001b[94mNone\u001b[0m                \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m  init_distributed = \u001b[94mFalse\u001b[0m               \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            kwargs = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                  \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m low_cpu_mem_usage = \u001b[94mFalse\u001b[0m               \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       torch_dtype = \u001b[94mNone\u001b[0m                \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m trust_remote_code = \u001b[94mTrue\u001b[0m                \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   use_safetensors = \u001b[94mTrue\u001b[0m                \u001b[33m│\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰─────────────────────────────────────────╯\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/\u001b[0m\u001b[1;33mtokenizatio\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[1;33mn_auto.py\u001b[0m:\u001b[94m966\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 963 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mconfig_dict = load_gguf_checkpoint(gguf_path, ret \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 964 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mconfig = AutoConfig.for_model(**config_dict)      \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 965 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 966 \u001b[2m│   │   │   │   │   \u001b[0mconfig = \u001b[1;4mAutoConfig.from_pretrained(\u001b[0m              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 967 \u001b[0m\u001b[1;2;4m│   │   │   │   │   │   \u001b[0m\u001b[1;4mpretrained_model_name_or_path, trust_remote_c\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 968 \u001b[0m\u001b[1;2;4m│   │   │   │   │   \u001b[0m\u001b[1;4m)\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m 969 \u001b[0m\u001b[2m│   │   │   \u001b[0mconfig_tokenizer_class = config.tokenizer_class           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m──────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m──────────────────────\u001b[0m\u001b[33m─╮\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        config = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        config_tokenizer_class = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                     gguf_file = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        inputs = \u001b[1m(\u001b[0m\u001b[1m)\u001b[0m                   \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        kwargs = \u001b[1m{\u001b[0m\u001b[33m'_from_auto'\u001b[0m: \u001b[94mTrue\u001b[0m\u001b[1m}\u001b[0m \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m pretrained_model_name_or_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m  \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            tokenizer_auto_map = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m              tokenizer_config = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                   \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                tokenizer_type = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             trust_remote_code = \u001b[94mTrue\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                use_auth_token = \u001b[94mNone\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                      use_fast = \u001b[94mTrue\u001b[0m                 \u001b[33m│\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────╯\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/\u001b[0m\u001b[1;33mconfigurati\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[1;33mon_auto.py\u001b[0m:\u001b[94m1151\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m                                           \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1148 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m pattern \u001b[95min\u001b[0m \u001b[96mstr\u001b[0m(pretrained_model_name_or_path):     \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1149 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m CONFIG_MAPPING[pattern].from_dict(config_d \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1150 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1151 \u001b[2m│   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;96mValueError\u001b[0m\u001b[1;4m(\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1152 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;33mf\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mUnrecognized model in \u001b[0m\u001b[1;4;33m{\u001b[0m\u001b[1;4mpretrained_model_name_or_path\u001b[0m\u001b[1;4;33m}\u001b[0m\u001b[1;4;33m. \u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1153 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;33mf\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mShould have a `model_type` key in its \u001b[0m\u001b[1;4;33m{\u001b[0m\u001b[1;4mCONFIG_NAME\u001b[0m\u001b[1;4;33m}\u001b[0m\u001b[1;4;33m, or\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m   \u001b[2m1154 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;33mf\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33min its name: \u001b[0m\u001b[1;4;33m{\u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4;33m, \u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4m.join(CONFIG_MAPPING.keys())\u001b[0m\u001b[1;4;33m}\u001b[0m\u001b[1;4;33m\"\u001b[0m        \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 code_revision = \u001b[94mNone\u001b[0m                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                   config_dict = \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m                                       \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                has_local_code = \u001b[94mFalse\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m               has_remote_code = \u001b[94mFalse\u001b[0m                                    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                        kwargs = \u001b[1m{\u001b[0m                                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                 \u001b[2m│   \u001b[0m\u001b[33m'_from_auto'\u001b[0m: \u001b[94mTrue\u001b[0m,                  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                 \u001b[2m│   \u001b[0m\u001b[33m'name_or_path'\u001b[0m: \u001b[33m'hf-weights/blt-1b'\u001b[0m  \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                                 \u001b[1m}\u001b[0m                                        \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                       pattern = \u001b[33m't5'\u001b[0m                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m pretrained_model_name_or_path = \u001b[33m'hf-weights/blt-1b'\u001b[0m                      \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m             trust_remote_code = \u001b[94mTrue\u001b[0m                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 unused_kwargs = \u001b[1m{\u001b[0m\u001b[33m'name_or_path'\u001b[0m: \u001b[33m'hf-weights/blt-1b'\u001b[0m\u001b[1m}\u001b[0m    \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                use_auth_token = \u001b[94mNone\u001b[0m                                     \u001b[33m│\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────────────────────────────╯\u001b[0m \u001b[31m│\u001b[0m\n",
            "[rank0]: \u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "[rank0]: \u001b[1;91mValueError: \u001b[0mUnrecognized model in hf-weights/blt-1b. Should have a `model_type` \n",
            "[rank0]: key in its config.json, or contain one of the following strings in its name: \n",
            "[rank0]: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, \n",
            "[rank0]: autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, \n",
            "[rank0]: big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, \n",
            "[rank0]: blip-\u001b[1;36m2\u001b[0m, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, \n",
            "[rank0]: chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, \n",
            "[rank0]: clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, \n",
            "[rank0]: convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, \n",
            "[rank0]: data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, \n",
            "[rank0]: decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, \n",
            "[rank0]: depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, \n",
            "[rank0]: distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, \n",
            "[rank0]: encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, \n",
            "[rank0]: fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, \n",
            "[rank0]: gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, \n",
            "[rank0]: gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, \n",
            "[rank0]: granite, granitemoe, granitemoeshared, granitevision, graphormer, \n",
            "[rank0]: grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, \n",
            "[rank0]: idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, \n",
            "[rank0]: instructblipvideo, jamba, jetmoe, jukebox, kosmos-\u001b[1;36m2\u001b[0m, layoutlm, layoutlmv2, \n",
            "[rank0]: layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, \n",
            "[rank0]: llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, \n",
            "[rank0]: mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, \n",
            "[rank0]: mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, \n",
            "[rank0]: mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, \n",
            "[rank0]: modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, \n",
            "[rank0]: mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, \n",
            "[rank0]: omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, \n",
            "[rank0]: patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, \n",
            "[rank0]: phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, \n",
            "[rank0]: prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, \n",
            "[rank0]: qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, \n",
            "[rank0]: realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, \n",
            "[rank0]: roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, \n",
            "[rank0]: rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, \n",
            "[rank0]: sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, \n",
            "[rank0]: smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, \n",
            "[rank0]: speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, \n",
            "[rank0]: swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, \n",
            "[rank0]: tapas, textnet, time_series_transformer, timesformer, timm_backbone, \n",
            "[rank0]: timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, \n",
            "[rank0]: unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, \n",
            "[rank0]: vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, \n",
            "[rank0]: vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits,\n",
            "[rank0]: vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm,\n",
            "[rank0]: xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, \n",
            "[rank0]: zamba, zamba2, zoedepth\n",
            "[rank0]:[W423 22:27:56.276155418 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python demo.py --device=\"cuda\" \"A BLT has\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5tMCqzFxalB",
        "outputId": "7c3a23ed-64a7-4da4-89f3-7f3f54864043"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mUsage: \u001b[0mdemo.py [OPTIONS] PROMPT\n",
            "\u001b[2mTry \u001b[0m\u001b[2;34m'demo.py \u001b[0m\u001b[1;2;34m-\u001b[0m\u001b[1;2;34m-help\u001b[0m\u001b[2;34m'\u001b[0m\u001b[2m for help.\u001b[0m\n",
            "\u001b[31m╭─\u001b[0m\u001b[31m Error \u001b[0m\u001b[31m─────────────────────────────────────────────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m No such option: \u001b[1;36m-\u001b[0m\u001b[1;36m-device\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/blt/bytelatent/test_blt.py --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9u-ksAZw4FG",
        "outputId": "ae3cd6b9-35d9-4320-c52b-9d0de6ad9995"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1967, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/integrations/ggml.py\", line 24, in <module>\n",
            "    from tokenizers import Tokenizer, decoders, normalizers, pre_tokenizers, processors\n",
            "ImportError: cannot import name 'Tokenizer' from 'tokenizers' (/content/blt/bytelatent/tokenizers/__init__.py)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/blt/bytelatent/test_blt.py\", line 30, in <module>\n",
            "    from bytelatent.train import compute_loss\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/bytelatent/train.py\", line 53, in <module>\n",
            "    from bytelatent.eval import EVAL_FOLDER_NAME, launch_eval\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/bytelatent/eval.py\", line 11, in <module>\n",
            "    from lm_eval import simple_evaluate\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/__init__.py\", line 4, in <module>\n",
            "    from .evaluator import evaluate, simple_evaluate\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/evaluator.py\", line 12, in <module>\n",
            "    import lm_eval.api.metrics\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/api/metrics.py\", line 12, in <module>\n",
            "    from lm_eval.api.registry import register_aggregation, register_metric\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/api/registry.py\", line 4, in <module>\n",
            "    import evaluate as hf_evaluate\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/evaluate/__init__.py\", line 29, in <module>\n",
            "    from .evaluation_suite import EvaluationSuite\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/evaluate/evaluation_suite/__init__.py\", line 10, in <module>\n",
            "    from ..evaluator import evaluator\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/evaluate/evaluator/__init__.py\", line 17, in <module>\n",
            "    from transformers.pipelines import SUPPORTED_TASKS as SUPPORTED_PIPELINE_TASKS\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/__init__.py\", line 23, in <module>\n",
            "    from ..configuration_utils import PretrainedConfig\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\", line 27, in <module>\n",
            "    from .modeling_gguf_pytorch_utils import load_gguf_checkpoint\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_gguf_pytorch_utils.py\", line 22, in <module>\n",
            "    from .integrations import (\n",
            "  File \"<frozen importlib._bootstrap>\", line 1229, in _handle_fromlist\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1955, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1969, in _get_module\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Failed to import transformers.integrations.ggml because of the following error (look up to see its traceback):\n",
            "cannot import name 'Tokenizer' from 'tokenizers' (/content/blt/bytelatent/tokenizers/__init__.py)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/blt/bytelatent/tokenizers/build_tokenizer.py"
      ],
      "metadata": {
        "id": "TPz50oK_0OrN"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# تحميل النموذج\n",
        "model = torch.load('path_to_model.pt', map_location='cuda')\n",
        "model.to('cuda')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "D7DoXlHX0RXL",
        "outputId": "78590b80-eb4e-47e1-a251-2b5060376091"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-7d0e68c31877>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load('path_to_model.pt', map_location='cuda')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'path_to_model.pt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-7d0e68c31877>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# تحميل النموذج\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'path_to_model.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_model.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/blt/bytelatent/model/local_models.py --help"
      ],
      "metadata": {
        "id": "djtfQwRv09SD"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/blt/bytelatent/e.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9W-SGr4r1Ebz",
        "outputId": "3369a340-6d13-49b2-df76-434acd592dd7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1967, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/integrations/ggml.py\", line 24, in <module>\n",
            "    from tokenizers import Tokenizer, decoders, normalizers, pre_tokenizers, processors\n",
            "ImportError: cannot import name 'Tokenizer' from 'tokenizers' (/content/blt/bytelatent/tokenizers/__init__.py)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1967, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\", line 24, in <module>\n",
            "    from ...configuration_utils import PretrainedConfig\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\", line 27, in <module>\n",
            "    from .modeling_gguf_pytorch_utils import load_gguf_checkpoint\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_gguf_pytorch_utils.py\", line 22, in <module>\n",
            "    from .integrations import (\n",
            "  File \"<frozen importlib._bootstrap>\", line 1229, in _handle_fromlist\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1955, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1969, in _get_module\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Failed to import transformers.integrations.ggml because of the following error (look up to see its traceback):\n",
            "cannot import name 'Tokenizer' from 'tokenizers' (/content/blt/bytelatent/tokenizers/__init__.py)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/blt/bytelatent/e.py\", line 1, in <module>\n",
            "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
            "  File \"<frozen importlib._bootstrap>\", line 1229, in _handle_fromlist\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1956, in __getattr__\n",
            "    value = getattr(module, name)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1955, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\", line 1969, in _get_module\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Failed to import transformers.models.auto.tokenization_auto because of the following error (look up to see its traceback):\n",
            "Failed to import transformers.integrations.ggml because of the following error (look up to see its traceback):\n",
            "cannot import name 'Tokenizer' from 'tokenizers' (/content/blt/bytelatent/tokenizers/__init__.py)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/blt/bytelatent/eval.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkui5zaJ14eM",
        "outputId": "a580efb5-1090-4bf8-ee4d-baaa152d1e19"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/blt/bytelatent/eval.py\", line 11, in <module>\n",
            "    from lm_eval import simple_evaluate\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/__init__.py\", line 4, in <module>\n",
            "    from .evaluator import evaluate, simple_evaluate\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/evaluator.py\", line 12, in <module>\n",
            "    import lm_eval.api.metrics\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/api/metrics.py\", line 12, in <module>\n",
            "    from lm_eval.api.registry import register_aggregation, register_metric\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/api/registry.py\", line 4, in <module>\n",
            "    import evaluate as hf_evaluate\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/evaluate/__init__.py\", line 29, in <module>\n",
            "    from .evaluation_suite import EvaluationSuite\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/evaluate/evaluation_suite/__init__.py\", line 10, in <module>\n",
            "    from ..evaluator import evaluator\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/evaluate/evaluator/__init__.py\", line 17, in <module>\n",
            "    from transformers.pipelines import SUPPORTED_TASKS as SUPPORTED_PIPELINE_TASKS\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/__init__.py\", line 26, in <module>\n",
            "    from ..image_processing_utils import BaseImageProcessor\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py\", line 22, in <module>\n",
            "    from .image_transforms import center_crop, normalize, rescale\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/image_transforms.py\", line 21, in <module>\n",
            "    from .image_utils import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/image_utils.py\", line 64, in <module>\n",
            "    from torchvision import io as torchvision_io\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\", line 10, in <module>\n",
            "    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/_meta_registrations.py\", line 163, in <module>\n",
            "    @torch.library.register_fake(\"torchvision::nms\")\n",
            "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/library.py\", line 795, in register\n",
            "    use_lib._register_fake(op_name, func, _stacklevel=stacklevel + 1)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/library.py\", line 184, in _register_fake\n",
            "    handle = entry.fake_impl.register(func_to_register, source)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_library/fake_impl.py\", line 31, in register\n",
            "    if torch._C._dispatch_has_kernel_for_dispatch_key(self.qualname, \"Meta\"):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: operator torchvision::nms does not exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer"
      ],
      "metadata": {
        "id": "imtgIqBp1_9c"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer"
      ],
      "metadata": {
        "id": "SwOqHBC62dQk"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/blt\n",
        "!python demo.py \"A BLT has\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTVqjU9S5JLf",
        "outputId": "53633d8c-6edb-4cec-e0ac-0db6dd48ea78"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/blt\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/blt/demo.py\", line 7, in <module>\n",
            "    from bytelatent.generate import load_consolidated_model_and_tokenizer\n",
            "  File \"/content/blt/bytelatent/generate.py\", line 13, in <module>\n",
            "    from bytelatent.args import EvalArgs, PackedCausalTransformerGeneratorArgs, TrainArgs\n",
            "  File \"/content/blt/bytelatent/args.py\", line 14, in <module>\n",
            "    from bytelatent.data.iterators.arrow_iterator import ArrowFileIterator\n",
            "  File \"/content/blt/bytelatent/data/iterators/arrow_iterator.py\", line 22, in <module>\n",
            "    from bytelatent.preprocess.preprocess_entropies import get_id_key, get_text\n",
            "  File \"/content/blt/bytelatent/preprocess/preprocess_entropies.py\", line 13, in <module>\n",
            "    from bytelatent.data.patcher import calculate_entropies\n",
            "  File \"/content/blt/bytelatent/data/patcher.py\", line 14, in <module>\n",
            "    from bytelatent.entropy_model import load_entropy_model\n",
            "  File \"/content/blt/bytelatent/entropy_model.py\", line 8, in <module>\n",
            "    from bytelatent.transformer import LMTransformer, LMTransformerArgs\n",
            "  File \"/content/blt/bytelatent/transformer.py\", line 19, in <module>\n",
            "    from bytelatent.base_transformer import (\n",
            "  File \"/content/blt/bytelatent/base_transformer.py\", line 19, in <module>\n",
            "    from bytelatent.tokenizers.constants import EOS_ID\n",
            "ModuleNotFoundError: No module named 'bytelatent.tokenizers'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rhZqXtbX5N9d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}